{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook to code up method of moments framework \n",
    "\n",
    "Here, I will use the method of moments framework used to track the trajectory of the site frequency spectrum through time given drift and selection from [Jouganous et. al. 2017](https://www.genetics.org/content/206/3/1549). Mostly, I will just use equations to start from a certain generation $t$ back in time and then iterate until generation 0. Then I will store this SFS as a data entry for allele age $t$ and certain selection coefficient $s$. This process will be repeated for each value of $\\{1,\\ldots,gen,\\ldots,12000\\}$ generations. \n",
    "\n",
    "These vectors need to be summed to marginalize over *all* generations $a$, to give $P(X, a | s)$.\n",
    "\n",
    "First, need to get a handle on what $\\Phi_n^k(i)$ really is - can just be represented as *np.array*\n",
    "\n",
    "$\\phi_n^k(i)$ is the expected number of sites where the alternate allele is observed exactly $i$ times in a sample of size $n$ at generation $k$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# numerics + rv stuff\n",
    "from numba import jit\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "from scipy.stats.distributions import chi2\n",
    "from scipy.sparse import coo_matrix\n",
    "from scipy.sparse import linalg\n",
    "from numpy.random import default_rng\n",
    "import moments\n",
    "# import dadi \n",
    "# import Selection\n",
    "# plotting + misc tools\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import itertools as it\n",
    "from copy import deepcopy\n",
    "import matplotlib.colors as colors\n",
    "import seaborn\n",
    "from matplotlib import cm \n",
    "from mom_functions import *\n",
    "\n",
    "# rng setup\n",
    "rng = default_rng(100496)\n",
    "\n",
    "# change matplotlib fonts\n",
    "plt.rcParams[\"font.family\"] = \"Arial\"\n",
    "plt.rcParams[\"font.sans-serif\"] = \"Arial\"\n",
    "plt.rcParams[\"figure.figsize\"] = [5, 3.5]\n",
    "plt.rcParams[\"figure.dpi\"] = 110\n",
    "plt.rcParams[\"axes.axisbelow\"] = True\n",
    "plt.rcParams.update({\"figure.facecolor\": \"white\"})\n",
    "\n",
    "# set numpy print option to a more readable format for floats\n",
    "np.set_printoptions(formatter={'float': lambda x: \"{0:0.3f}\".format(x)})\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 10000\n",
    "s = -10/N # 25/N -> gamma2 = 50 - strong selection\n",
    "mu = 1.25e-8 # human mutation rate\n",
    "n = 2000 # 2 * # of inds sampled, diploid\n",
    "\n",
    "# start in generation 10 so generation 11 has all zeros (going back in time)\n",
    "tot_gen = 10000\n",
    "time_steps = np.linspace(0, tot_gen-1, 100, dtype=int)\n",
    "\n",
    "mom = np.zeros((tot_gen+1,n+1))\n",
    "momnp1 = np.zeros(n+1)\n",
    "momkp1 = np.zeros((tot_gen+1,n+1))\n",
    "\n",
    "# double precaution - creating a mask\n",
    "mk = [False] + [True]*(n-1) + [False]\n",
    "\n",
    "iter = np.arange(1,n)\n",
    "iterm1p1 = np.arange(2,n-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## borrowed directly from https://bitbucket.org/simongravel/moments/src/main/moments/Jackknife.pyx\n",
    "def python2round(f):\n",
    "    if round(f + 1) - round(f) != 1:\n",
    "        return f + abs(f) / f * 0.5\n",
    "    return round(f)\n",
    "\n",
    "# The choice i' in n samples that best approximates the frequency of i/(n + 1) is i*n / (n + 1)\n",
    "def index_bis(i, n):\n",
    "    return int(min(max(python2round(i * n / float(n+1)), 2), n-2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code borrowed from https://bitbucket.org/simongravel/moments/src/main/moments/Jackknife.pyx  \n",
    "def calcJK13(n):\n",
    "    J = np.zeros((n,n-1))\n",
    "    for i in range(n):\n",
    "        ibis = index_bis(i + 1, n) - 1\n",
    "        J[i, ibis] = -(1.+n) * ((2.+i)*(2.+n)*(-6.-n+(i+1.)*(3.+n))-2.*(4.+n)*(-1.+(i+1.)*(2.+n))*(ibis+1.)+(12.+7.*n+n**2)*(ibis+1.)**2) / (2.+n) / (3.+n) / (4.+n)\n",
    "        J[i, ibis - 1] = (1.+n) * (4.+(1.+i)**2*(6.+5.*n+n**2)-(i+1.)*(14.+9.*n+n**2)-(4.+n)*(-5.-n+2.*(i+1.)*(2.+n))*(ibis+1.)+(12.+7.*n+n**2)*(ibis+1.)**2) / (2.+n) / (3.+n) / (4.+n) / 2.\n",
    "        J[i, ibis + 1] = (1.+n) * ((2.+i)*(2.+n)*(-2.+(i+1.)*(3.+n))-(4.+n)*(1.+n+2.*(i+1.)*(2.+n))*(ibis+1.)+(12.+7.*n+n**2)*(ibis+1.)**2) / (2.+n) / (3.+n) / (4.+n) / 2.\n",
    "    return J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## testbed for a single realization of gen = t\n",
    "mom[tot_gen,1] = n*mu # singleton input\n",
    "\n",
    "J = calcJK13(n)\n",
    "\n",
    "# going from generation 9 to 0\n",
    "for gen in np.arange(tot_gen)[::-1]:\n",
    "    momkp1[gen,iterm1p1] = 0.25/N * (mom[gen+1,iterm1p1-1] * (iterm1p1-1)*(n-iterm1p1+1) + mom[gen+1,iterm1p1+1] * (iterm1p1+1)*(n-iterm1p1-1) - mom[gen+1,iterm1p1] * 2*iterm1p1*(n-iterm1p1))\n",
    "\n",
    "    momkp1[gen,1] = 0.25/N * ((n-2) * 2 * mom[gen+1,2] - 2 * (n-1) * mom[gen+1,1])\n",
    "    momkp1[gen,n-1] = 0.25/N * ((n-2) * 2 * mom[gen+1,n-2] - 2 * (n-1) * mom[gen+1,n-1])\n",
    "\n",
    "    # notice the difference in indexing for LHS\n",
    "    # momnp1[np.arange(1,n+1)] = (jk13[:,0] * mom[gen+1,np.array(ibis)-1] - jk13[:,1] * mom[gen+1,np.array(ibis)] + jk13[:,2] * mom[gen+1,np.array(ibis)+1])\n",
    "    momnp1[np.arange(1,n+1)] = (J @ mom[gen+1,iter])\n",
    "\n",
    "    momkp1[gen,iter] += mom[gen+1,iter] + 0.5 * s/(n+1) * (iter * (n+1-iter) * momnp1[iter] - (n-iter) * (iter+1) * momnp1[iter+1])\n",
    "\n",
    "    mom[gen,] = deepcopy(momkp1[gen,])\n",
    "\n",
    "mom2 = deepcopy(mom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## code to run APR's moments and compare with output from above\n",
    "# initialize the spectrum, with 1 in singleton bin\n",
    "fs = moments.Spectrum(np.zeros(2*n + 1))\n",
    "fs[1] = n*1\n",
    "# simulate a generations\n",
    "T = tot_gen / 2 / N\n",
    "# set relative size to 1, theta to 0 to forbid new mutations\n",
    "fs.integrate([1], T, gamma=s*2*N, h=0.5, theta=0, adapt_dt=True, dt_fac=0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## testbed for a single realization of gen = t & the Crank-Nicolson method\n",
    "mom[100,1] = n*1 # singleton input\n",
    "\n",
    "dt = 1\n",
    "\n",
    "D = 0.25/N * calcD(n+1)\n",
    "J = calcJK13(n+1)\n",
    "S = 0.5 * s * calcS(n+1, J)\n",
    "\n",
    "# if N is same across all gens then only have to do this once\n",
    "slv = linalg.factorized(sp.sparse.identity(S.shape[0], dtype=\"float\", format=\"csc\") - dt / 2.0 * (D + S))\n",
    "Q = sp.sparse.identity(S.shape[0], dtype=\"float\", format=\"csc\") + dt / 2.0 * (D + S)\n",
    "\n",
    "# going from generation 9 to 0\n",
    "for gen in np.arange(100)[::-1]:\n",
    "\n",
    "    # momkp1[gen,iter] = mom[gen+1,iter] + ((D[iter,] + S[iter,]) @ mom[gen+1,])\n",
    "    momkp1[gen,iter] = mom[gen+1,iter] + slv(Q.dot(mom[gen+1,iter]))\n",
    "    momkp1[gen,0] = momkp1[gen,n] = 0.0\n",
    "\n",
    "    mom[gen,] = deepcopy(momkp1[gen,])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcD(d):\n",
    "    data = []\n",
    "    row = []\n",
    "    col = []\n",
    "    # loop over the fs elements:\n",
    "    for i in range(d):\n",
    "        if i > 1:\n",
    "            data.append((i-1) * (d-i))\n",
    "            row.append(i)\n",
    "            col.append(i - 1)\n",
    "        if i < d - 2:\n",
    "            data.append((i+1) * (d-i-2))\n",
    "            col.append(i + 1)\n",
    "            row.append(i)\n",
    "        if i > 0 and i < d - 1:\n",
    "            data.append(-2 * i * (d-i-1))\n",
    "            row.append(i)\n",
    "            col.append(i)\n",
    "\n",
    "    return coo_matrix((data, (row, col)), shape=(d, d), dtype='float').tocsc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcS(d, ljk):\n",
    "    # Computes the jackknife-transformed selection matrix 1\n",
    "    # for the addition of a single sample\n",
    "    # arrays for the creation of the sparse (coo) matrix\n",
    "    # data will have matrix entry, row + column have coordinates\n",
    "    data = []\n",
    "    row = []\n",
    "    col = []\n",
    "    # loop over the fs elements:\n",
    "    for i in range(d):\n",
    "        i_bis = index_bis(i, d - 1) # This picks the second jackknife index \n",
    "        i_ter = index_bis(i + 1, d - 1) # This picks the third jackknife index\n",
    "        # coefficients of the selection matrix\n",
    "        g1 = i * (d-i) / np.float64(d)\n",
    "        g2 = -(i+1) * (d-1-i) / np.float64(d)\n",
    "\n",
    "        if i < d - 1 and i > 0: # First deal with non-fixed variants\n",
    "            data += [g1 * ljk[i - 1, i_bis - 1], g1 * ljk[i - 1, i_bis - 2],\n",
    "                    g1 * ljk[i - 1, i_bis], g2 * ljk[i, i_ter - 1],\n",
    "                    g2 * ljk[i, i_ter - 2], g2 * ljk[i, i_ter]]\n",
    "            row += 6 * [i]\n",
    "            col += [i_bis, i_bis - 1, i_bis + 1,\n",
    "                    i_ter, i_ter - 1, i_ter + 1]\n",
    "        \n",
    "        elif i == 0: # g1=0\n",
    "            data += [g2 * ljk[i, i_ter - 1],\n",
    "                     g2 * ljk[i, i_ter - 2], g2 * ljk[i, i_ter]]\n",
    "            row += 3 * [i]\n",
    "            col += [i_ter, i_ter - 1, i_ter + 1]\n",
    "        \n",
    "        elif i == d - 1: # g2=0\n",
    "            data += [g1 * ljk[i - 1, i_bis - 1], g1 * ljk[i - 1, i_bis - 2],\n",
    "                     g1 * ljk[i - 1, i_bis]]\n",
    "            row += 3 * [i]\n",
    "            col += [i_bis, i_bis - 1, i_bis + 1]\n",
    "\n",
    "    return coo_matrix((data, (row, col)), shape=(d, d), dtype='float').tocsc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## packaging into a function for easy manipulation - iteration implementation \n",
    "# input: a (number of gens), n (number of samples), s, N (pop size)\n",
    "# output: mom (number of sites)\n",
    "def run_mom_iterate(a, n, s, N, mu, misc):\n",
    "    mom = np.zeros((a+1,n+1))\n",
    "    # momnp1 = np.zeros(n+1)\n",
    "    momkp1 = np.zeros(n+1)\n",
    "\n",
    "    D = 0.25/N * calcD(n+1)\n",
    "    J = calcJK13(n)\n",
    "    S = 0.5 * s * calcS(n+1, J)\n",
    "\n",
    "    # if N is same across all gens then only have to do this once\n",
    "    slv = linalg.factorized(sp.sparse.identity(S.shape[0], dtype=\"float\", format=\"csc\") - 0.5 * (D + S))\n",
    "    Q = sp.sparse.identity(S.shape[0], dtype=\"float\", format=\"csc\") + 0.5 * (D + S)\n",
    "\n",
    "    mom[a,1] = 1 # singleton input\n",
    "\n",
    "    # going from generation 9 to 0\n",
    "    for gen in np.arange(a)[::-1]:\n",
    "        momkp1 = slv(Q.dot(mom[gen+1,]))\n",
    "        momkp1[0] = momkp1[n] = 0.0\n",
    "\n",
    "        mom[gen,] = deepcopy(momkp1)\n",
    "\n",
    "    return mom[:-1,:]           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit(nopython=True)\n",
    "def run_mom_iterate_changing(n, s, Nc, mu, misc):\n",
    "    mom = np.zeros((len(Nc)+1,n+1))\n",
    "    # momnp1 = np.zeros(n+1)\n",
    "    momkp1 = np.zeros(n+1)\n",
    "\n",
    "    changepoints = len(Nc) - np.concatenate((np.array([0]),np.where(Nc[:-1] != Nc[1:])[0]+1),axis=0)\n",
    "    changepoints = np.append(changepoints, 0)\n",
    "\n",
    "    mom[len(Nc),1] = 1 # singleton input\n",
    "    \n",
    "    # only need to do this once - no dependence on N\n",
    "    J = calcJK13(n)\n",
    "    S = 0.5 * s * calcS(n+1, J)\n",
    "\n",
    "    for i in range(len(changepoints)-1):\n",
    "        D = 0.25/Nc[len(Nc)-changepoints[i]] * calcD(n+1)\n",
    "\n",
    "        slv = linalg.factorized(sp.sparse.identity(S.shape[0], dtype=\"float\", format=\"csc\") - 0.5 * (D + S))\n",
    "        Q = sp.sparse.identity(S.shape[0], dtype=\"float\", format=\"csc\") + 0.5 * (D + S)\n",
    "\n",
    "        for gen in np.arange(changepoints[i+1],changepoints[i])[::-1]:\n",
    "            momkp1 = slv(Q.dot(mom[gen+1,]))\n",
    "            momkp1[0] = momkp1[n] = 0.0\n",
    "\n",
    "            mom[gen,] = deepcopy(momkp1)\n",
    "\n",
    "    return mom[:-1,:]           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## starting from the past to the present (Nc[-1] is current generation)\n",
    "# Nc = np.concatenate((np.repeat(20000,1000),np.repeat(2000,2000)))\n",
    "# changepoints = len(Nc) - np.concatenate((np.array([0]),np.where(Nc[:-1] != Nc[1:])[0]+1),axis=0)\n",
    "# changepoints = np.append(changepoints, 0)\n",
    "# mom_outc = run_mom_iterate_changing(1000, -0.000125, Nc/2, 1.25e-8, None)\n",
    "# plt.imshow(mom_outc[000:2100,:]/np.sum(mom_outc[000:2100]),aspect='auto',norm=colors.LogNorm()); plt.colorbar(); plt.show()\n",
    "# plt.imshow(up_xa_s[gamma2[-20]],aspect='auto',norm=colors.LogNorm(vmax=1e-3,vmin=1e-10)); plt.colorbar(); plt.show()\n",
    "plt.imshow(up_xa_s[-100.],aspect='auto',norm=colors.LogNorm(vmax=1e-3,vmin=1e-10)); plt.colorbar(); plt.show()\n",
    "# plt.plot(mom_outc[-100,:]); plt.plot(mom_outc[-2000,:]); plt.loglog(); plt.ylim((1e-10,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fs = run_mom_integrate(4000, 1000, 0.5*gamma[0]/N, N, mu, misc = {'dt_fac':0.02, 'adapt_dt':False})\n",
    "# plt.imshow(fs[:,:],aspect='auto',norm=colors.LogNorm(vmax=10e-7,vmin=10e-30)); plt.colorbar(); plt.show()\n",
    "# fs2 = run_mom_integrate2(4000, 200, 0.5*gamma[0]/N, N, mu, misc = {'dt_fac':0.02, 'adapt_dt':False})\n",
    "# plt.imshow(fs2[:,:],aspect='auto',norm=colors.LogNorm(vmax=10e-7,vmin=10e-30)); plt.colorbar(); plt.show()\n",
    "# mom = run_mom_iterate(8000, 2000, 0.5*gamma[10]/N, N, mu, np.nan)\n",
    "# plt.imshow(mom[:,:],aspect='auto',norm=colors.LogNorm(vmax=10e-7,vmin=10e-30)); plt.colorbar(); plt.show()\n",
    "# fs.shape, mom.shape\n",
    "# plt.scatter(np.ravel(mom[:,1:-1]), np.ravel(fs[:,1:-1]), color='grey', alpha=0.75, s=5); plt.loglog()\n",
    "# plt.axline((0,0),(1,1),color='coral',ls='--', linewidth=0.6); plt.xlabel('iterative framework'); plt.ylabel('integrative framework'); plt.title('γ = {}'.format(-100.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## function where each generation was integrated to separately\n",
    "def run_mom_integrate(a, n, s, N, mu, misc):\n",
    "    fsmat = np.zeros((a,n+1))\n",
    "    for idt, dt in enumerate(np.linspace(0.5/N,0.5*a/N,a)[::-1]):\n",
    "        fs = moments.Spectrum(np.zeros(n + 1))\n",
    "        fs[1] = 1\n",
    "        fs.integrate([1], dt, gamma=2*s*N, h=0.5, theta=0, dt_fac=misc['dt_fac'], adapt_dt=misc['adapt_dt'])\n",
    "        fsmat[idt,:] = n*mu*fs\n",
    "    return fsmat\n",
    "\n",
    "## function where each generation is only integrated from previous generation\n",
    "def run_mom_integrate2(a, n, s, N, mu, misc):\n",
    "    fsmat = np.zeros((a,n+1))\n",
    "    dt = 0.5/N\n",
    "    fs = moments.Spectrum(fsmat[-1,:])\n",
    "    fs[1] = 1\n",
    "    fs.integrate([1], dt, gamma=2*s*N, h=0.5, theta=0)\n",
    "    fsmat[-1,:] = fs\n",
    "    for idt in np.arange(0,a-1)[::-1]:\n",
    "        # fs = moments.Spectrum(fsmat[idt+1,:])\n",
    "        fs.integrate([1], dt, gamma=2*s*N, h=0.5, theta=0, dt_fac=misc['dt_fac'], adapt_dt=misc['adapt_dt'])\n",
    "        fsmat[idt,:] = fs\n",
    "    return n*mu*fsmat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fs.integrate([1],2,gamma=-10,theta=1)\n",
    "# fs1 = moments.Spectrum(np.zeros(n+1))\n",
    "# fs1[1]=1\n",
    "# fs1.integrate([1],2,gamma=-1,theta=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gamma2 = np.hstack((np.logspace(1,-2,10),0.0,-np.logspace(-2,1,10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## creating a database of P(X, a | s) with dicts for different s values\n",
    "# import io\n",
    "# up_xa_s = h5py.File(io.BytesIO(), 'w')\n",
    "# del up_xa_s\n",
    "gamma2 = -np.logspace(-.5,2.5,25,) \n",
    "# up_xa_s = dict.fromkeys(gamma2)\n",
    "## use different lengths of time for each gamma (high gamma - small limits)\n",
    "## go up to 5 orders of magnitude below start (i.e., E[# seg sites] < 1e-11)\n",
    "# limal = np.concatenate((np.linspace(7500,85000,20,dtype=int),np.repeat(85000,30)))\n",
    "# limal = np.linspace(140000,10000,25,dtype='int')\n",
    "# for ig, g in enumerate(gamma2):\n",
    "    # unscaled probability - almost likelihood\n",
    "    # up_xa_s[g] = run_mom_iterate(limal[ig], 2000, 0.5*g/N, N, 1.25e-8, misc = {'dt_fac':0.02, 'adapt_dt':True})\n",
    "    # can project down to any sample size using moments.Spectrum(p_xa_s[g][-1,:]).project([20])*120/20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# up_xa_s_neut = run_mom_iterate(100000, 2000, 0, N, 1.25e-8, misc = {'dt_fac':0.02, 'adapt_dt':True})\n",
    "cutoff = 2\n",
    "up_xa_s_neut[:,np.arange(cutoff,n-cutoff+1)] = up_xa_s_neut[:,np.arange(cutoff,n-cutoff+1)]/np.sum(up_xa_s_neut[:,np.arange(cutoff,n-cutoff+1)])\n",
    "p_xa_s_neut = np.sum(up_xa_s_neut, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testup_xa_s = {}\n",
    "testup_xa_s[gamma[6]] = run_mom_integrate(80000, 2000, 0.5*g/N, N, mu, misc = {'dt_fac':0.02, 'adapt_dt':True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testup_xa_s[gamma[6]][:,np.arange(cutoff,n-cutoff+1)] = testup_xa_s[gamma[6]][:,np.arange(cutoff,n-cutoff+1)]/np.sum(testup_xa_s[gamma[6]][:,np.arange(cutoff,n-cutoff+1)])\n",
    "fs = moments.Spectrum(np.zeros(2000 + 1))\n",
    "fs[1] = 1\n",
    "fs.integrate([1], 120000/2/N, gamma=gamma[-1], theta=0, adapt_dt=True, dt_fac=0.0001)\n",
    "testp_xa_s = fs\n",
    "testp_xa_s[cutoff:(2000-cutoff+1)] = fs[cutoff:(2000-cutoff+1)]/np.sum(fs[cutoff:(2000-cutoff+1)])\n",
    "plt.plot(np.arange(2,1001),testp_xa_s[2:1001],marker='o'); plt.loglog()\n",
    "# testp_xa_s = np.sum(testup_xa_s[gamma[6]], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use these plots to figure out when to stop (for gamma<1, ok could stop at 10k for high gamma)\n",
    "plt.scatter(np.arange(100000)[::-1],(np.sum(up_xa_s[-1.],axis=1)),alpha=0.7,color='k')\n",
    "plt.scatter(np.arange(100000)[::-1],(np.sum(up_xa_s[-.1],axis=1)),alpha=0.7,color='grey')\n",
    "plt.loglog(); plt.xlabel('gens'); plt.ylabel('exp # of seg sites')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## scaling the matrix from above to get pdf (all rows sum to 1)\n",
    "cutoff = 2 # 2 x # of inds\n",
    "for g in gamma2:\n",
    "    # normalizing by rows (summing across gens)\n",
    "    # p_xa_s[g] = up_xa_s[g]/up_xa_s[g].sum(axis=0,keepdims=1)\n",
    "    # up_xa_s[g][:,np.arange(cutoff,n-cutoff+1)] = up_xa_s[g][:,np.arange(cutoff,n-cutoff+1)]/up_xa_s[g][:,np.arange(cutoff,n-cutoff+1)].sum(axis=1,keepdims=True)\n",
    "    # normalizing by rows and cols (summing across gens and # of derived alleles)\n",
    "    # p_xa_s[g] = up_xa_s[g]/np.sum(up_xa_s[g])\n",
    "    # normalizing by rows and cols and number of derived alleles (based on if detectable in sample or not)\n",
    "    up_xa_s[g][:,np.arange(cutoff,2000-cutoff+1)] = up_xa_s[g][:,np.arange(cutoff,2000-cutoff+1)]/np.sum(up_xa_s[g][:,np.arange(cutoff,2000-cutoff+1)]) \n",
    "    # normalizing by rows (summing across allele freqs)\n",
    "    # up_xa_s[g][:,np.arange(cutoff,n-cutoff+1)] = up_xa_s[g][:,np.arange(cutoff,n-cutoff+1)]/up_xa_s[g][:,np.arange(cutoff,n-cutoff+1)].sum(axis=1,keepdims=True)\n",
    "\n",
    "# import pickle as pkl\n",
    "# with open('testdata/fsintegrate.pkl', 'wb') as f:\n",
    "#     pkl.dump(up_xa_s, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## summing across rows and then normalizing to get P(X|\\gamma) \n",
    "# del p_xa_s\n",
    "p_xa_s = {}\n",
    "for g in gamma2:\n",
    "    fs = moments.Spectrum(np.zeros(2000+1))\n",
    "    fs[1] = 1\n",
    "    fs.integrate([0.5], 10, gamma=g)\n",
    "    p_xa_s[g] = fs/np.sum(fs[np.arange(cutoff,2000-cutoff+1)])\n",
    "    # p_xa_s[g] = np.sum(up_xa_s[g], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# moments.Plotting.plot_1d_fs(p_xa_s[gamma2[-15]])\n",
    "gamma2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(dpi=120)\n",
    "fs = moments.Spectrum(np.zeros(50)); fs[1] = 1\n",
    "fs.integrate([0.5],4,gamma=-5); fs = fs.project([11])*50/11\n",
    "plt.bar(x=np.arange(1,11),height=np.array(fs[1:-1]/np.sum(fs)),color='pink',alpha=0.8,); plt.xlabel('# of allele copies'); plt.ylabel('E[i|γ]'); plt.ylim((0,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(p_xa_s[gamma2[-10]],np.sum(up_xa_s[gamma2[-10]],axis=0),alpha=0.8); plt.loglog(); plt.axline((0,0),(1,1),ls='--')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(up_xa_s[gamma[15]],aspect='auto',norm=colors.LogNorm(vmax=1e-7,vmin=1e-20)); plt.colorbar()\n",
    "# np.min(p_xa_s[gamma[4]][10,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading in simulation data for comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"/Users/vivaswatshastry/selCoefEst/PReFerSims\")\n",
    "# df2 = pd.read_csv(\"../traindata/trip-2022-03-29.csv\")\n",
    "# df2['gamma'] = df2['gamma'].round(decimals=2)\n",
    "# df2['empty'] = ''\n",
    "# df2['empty2'] = ''\n",
    "# df2['sXl'] = rng.binomial(n=2000, p=df2['Xl'])\n",
    "# gamma = np.unique(df2['gamma'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df1['sXl'] = np.around(df1['Xl']*n).astype(int)\n",
    "nsites = 1500\n",
    "idx2keep = np.empty(len(gamma)*nsites,dtype='int')\n",
    "for ig, _ in enumerate(gamma):\n",
    "    idx2keep[(ig*nsites):(ig+1)*nsites] = rng.choice(np.where(df2['sXl'][(ig*5000):(ig+1)*5000]>1)[0], nsites, False) + ig*5000\n",
    "df2 = df2.iloc[idx2keep,:]\n",
    "dat2 = df2.to_numpy()\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(df2['gamma'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.distributions.empirical_distribution import ECDF\n",
    "## PReFerSim selection coefficient is HALF of backward WF sims (need to test this further—simple to rerun cell, but long runtime)\n",
    "def plot_ecdf_al_sxl(idx):\n",
    "    indices = np.arange((19*1000),(19+1)*1000)\n",
    "    indices2 = np.arange((16*1000),(16+1)*1000)\n",
    "    age1_ecdf = ECDF(df1['al'][::-1].iloc[indices])\n",
    "    age2_ecdf = ECDF(df2['al'].iloc[indices2])\n",
    "    x = np.linspace(1,np.max(df2['al'].iloc[indices]))\n",
    "    y1 = age1_ecdf(x)\n",
    "    y2 = age2_ecdf(x)\n",
    "    plt.plot(x, y1, color='green', alpha=0.7, label='PReFerSim vals'); plt.xlabel('age (gens)'); plt.semilogx()\n",
    "    plt.plot(x, y2, color='purple', alpha=0.7, label='backward approx WF vals'); plt.title('γ={}'.format(gamma[0])); plt.legend()\n",
    "\n",
    "plot_ecdf_al_sxl(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df2['gamma'].iloc[np.arange((16*1000),(16+1)*1000)], df1['s'][::-1].iloc[np.arange((19*1000),(19+1)*1000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# interpolate gamma (in other words, I need a denser grid of gamma values around the MLE for better quadratic estimation)\n",
    "interp_gamma = np.zeros((len(gamma),5))\n",
    "interp_gamma[0,] = -np.exp(np.linspace(np.log(-gamma[1]),np.log(125),5))\n",
    "for ig, g in enumerate(gamma[1:-1]):\n",
    "    interp_gamma[ig+1,] = -np.exp(np.linspace(np.log(-gamma[ig+2]),np.log(-gamma[ig]),5))\n",
    "interp_gamma[-1,] = -np.exp(np.linspace(np.log(0.7),np.log(-gamma[-2]),5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## going through and deleting entries that are very close to each other (tol=1)\n",
    "interp_gamma = interp_gamma.round(2)\n",
    "print(interp_gamma)\n",
    "it_gamma = np.unique(np.hstack((gamma,np.ravel(interp_gamma))))\n",
    "it_gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## function to obtain the log P(X,|gamma)\n",
    "def get_lp_xl(g, sXlred, n=2000, cutoff=20):\n",
    "    \"\"\"function to compute L(gamma|Xl), where gamma is a range of values and Xl is a given set of freqs\"\"\"\n",
    "    res = np.empty(np.sum((sXlred>cutoff) & (sXlred<n-cutoff+1))) #np.empty(len(Xlred))\n",
    "\n",
    "    # just performing a search in a look-up table\n",
    "    for idx, i in enumerate(np.where((sXlred>cutoff) & (sXlred<n-cutoff+1))[0]):\n",
    "        res[idx] = p_xa_s[g][sXlred[i]]\n",
    "    \n",
    "    return np.log(res)\n",
    "\n",
    "def get_lp_xl2(g, sXlred, n=2000, cutoff=20):\n",
    "    \"\"\"function to compute L(gamma|Xl), where gamma is a range of values and Xl is a given set of freqs\"\"\"\n",
    "    res = np.empty(np.sum((sXlred>cutoff) & (sXlred<n-cutoff+1))) #np.empty(len(Xlred))\n",
    "\n",
    "    # ub = np.exp(2.*g)*scipy.special.expi(-2.*g*0.25/N) - scipy.special.expi(2.*g*(1-0.25/N)) - np.exp(2.*g)*(np.log(0.25/N) - np.log(1-0.25/N))\n",
    "    # lb = np.exp(2.*g)*scipy.special.expi(2.*g*(0.25/N-1)) - scipy.special.expi(2.*g*0.25/N) - np.exp(2.*g)*(np.log(1-0.25/N) - np.log(0.25/N))\n",
    "    ub = np.exp(2.*g)*sp.special.expi(-2.*g*0.5*cutoff/n) - sp.special.expi(2.*g*(1-0.5*cutoff/n)) - np.exp(2.*g)*(np.log(0.5*cutoff/n) - np.log(1-0.5*cutoff/n))\n",
    "    lb = np.exp(2.*g)*sp.special.expi(2.*g*(0.5*cutoff/n-1)) - sp.special.expi(2.*g*0.5*cutoff/n) - np.exp(2.*g)*(np.log(1-0.25/n) - np.log(0.5*cutoff/n))\n",
    "    scalfact = (ub - lb)/np.expm1(2.*g)\n",
    "\n",
    "    # return a vector...\n",
    "    for isx, sx in enumerate(np.where((sXlred>cutoff) & (sXlred<n-cutoff+1))[0]):\n",
    "        res[isx] = (1-np.exp(-2*g*(1-sXlred[sx]/n)))/(sXlred[sx]/n*(1-sXlred[sx]/n)*(1-np.exp(-2*g)))\n",
    "\n",
    "    return np.log(res/scalfact)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## don't need a function since the dict already exists\n",
    "Xsamp = np.arange(1,n+1)/n\n",
    "plt.hist([np.argmin(np.abs(dat[i,0]-Xsamp))+1 for i in range(len(dat))],bins=200);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.hist(newdat[:,2],bins=100)\n",
    "# plt.hist2d(np.log(newdat[:,0]),newdat[:,2][::-1],(25,25)); plt.colorbar()\n",
    "# print(newdat[-5:,])\n",
    "np.where(np.isinf(get_lp_alxl(gamma[-2], newdat[:,0], newdat[:,2], 100)))\n",
    "# print(newdat[429,])\n",
    "# get_lp_alxl(gamma[-2], newdat[:,0], newdat[:,2], 100)\n",
    "# sXlred = np.around(newdat[:,0]*100).astype(int)\n",
    "# np.sum((sXlred>0) & (sXlred<100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('error')\n",
    "## just doing a lookup of sorts for the right probability\n",
    "def get_lp_alxl(g, sXlred, alred, n=2000, cutoff=2):\n",
    "    # Xsamp = np.arange(1,n)/n\n",
    "    # sXlred = np.around(Xlred*n).astype(int) # rng.binomial(n, Xlred, len(Xlred))\n",
    "    res = np.empty(np.sum((sXlred>cutoff) & (sXlred<n-cutoff+1)))\n",
    "    for idx, i in enumerate(np.where((sXlred>cutoff) & (sXlred<n-cutoff+1))[0]):\n",
    "        # if too many gens, then pass in a very low number (like -400.)\n",
    "        # res[i] = np.log(p_xa_s[g][-int(alred[i]),np.argmin(np.abs(Xlred[i]-Xsamp))+1]) if (int(alred[i]<p_xa_s[g].shape[0])) else -400. \n",
    "        try:\n",
    "            res[idx] = np.log(up_xa_s[g][-int(alred[i]),sXlred[i]]) if (int(alred[i])<up_xa_s[g].shape[0]) else np.median(np.log(up_xa_s[g][0,cutoff:(n-cutoff+1)]))\n",
    "        except RuntimeWarning:\n",
    "            print(g, sXlred[i], alred[i])\n",
    "        # if np.isinf(res[idx]):\n",
    "        #     print(i, Xlred[i], alred[i])\n",
    "\n",
    "    return res\n",
    "\n",
    "def get_lp_alxl_nocheck(g, sXlred, alred, n=2000, cutoff=2):\n",
    "    ## version of function with no check for cutoff\n",
    "    res = np.log(up_xa_s[g][-int(alred),sXlred]) if (int(alred)<up_xa_s[g].shape[0]) else np.median(np.log(up_xa_s[g][0,cutoff:(n-cutoff+1)]))\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## finding how many alleles have ages beyond computed values in the lookup matrix/table (like 4)\n",
    "for ig, g in enumerate(gamma):\n",
    "    alred = df1['al'].iloc[ig*1000:(ig+1)*1000]\n",
    "    print(g, np.sum(alred>up_xa_s[g].shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code to run continuous optimization of framework instead of dicrete grid\n",
    "\n",
    "Here, I will construct a method to compute the likelihood of a given dataset for any $\\gamma$ value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ll_freq(g, sXlred, n=2000, cutoff=2):\n",
    "    fs = moments.Spectrum(np.zeros(n+1))\n",
    "    fs[1] = 1\n",
    "    fs.integrate([0.5], 10, gamma=g)\n",
    "    pxs = fs/np.sum(fs[np.arange(cutoff,n-cutoff+1)])\n",
    "\n",
    "    res = np.empty(np.sum((sXlred>cutoff) & (sXlred<n-cutoff+1))) #np.empty(len(Xlred))\n",
    "\n",
    "    # just performing a search in a look-up table\n",
    "    for idx, i in enumerate(np.where((sXlred>cutoff) & (sXlred<n-cutoff+1))[0]):\n",
    "        res[idx] = pxs[sXlred[i]]\n",
    "    \n",
    "    return -np.sum(np.log(res))\n",
    "@jit(forceobj=True)\n",
    "def get_ll_freqage(g, sXlred, alred, n=2000, cutoff=2):\n",
    "    pxas = run_mom_iterate(int(100000+900*g), n, 0.5*g/N, N, 1.25e-8, misc = {'dt_fac':0.02, 'adapt_dt':True})\n",
    "    \n",
    "    pxas[:,np.arange(cutoff,n-cutoff+1)] = pxas[:,np.arange(cutoff,n-cutoff+1)]/np.sum(pxas[:,np.arange(cutoff,n-cutoff+1)]) \n",
    "\n",
    "    res = np.empty(np.sum((sXlred>cutoff) & (sXlred<n-cutoff+1)),dtype='float32')\n",
    "    for idx, i in enumerate(np.where((sXlred>cutoff) & (sXlred<n-cutoff+1))[0]):\n",
    "        res[idx] = np.log(pxas[-int(alred[i]),sXlred[i]]) if (int(alred[i])<pxas.shape[0]) else np.median(np.log(pxas[0,cutoff:(n-cutoff+1)]))\n",
    "\n",
    "    return -np.sum(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_sims = 16\n",
    "parestfreq = np.zeros((4,num_sims))\n",
    "parestage = np.zeros((4,num_sims))\n",
    "for ig, g in enumerate([1.,10.,100.]):\n",
    "    for n in range(num_sims):\n",
    "        with open('simfiles/ParameterFilesConstant.txt',\"r\") as file:\n",
    "            data = file.readlines()\n",
    "\n",
    "        data[0] = 'MutationRate: {:f}\\n'.format(2)\n",
    "        data[2] = 'DFEPointSelectionCoefficient: {:.8f}\\n'.format(-0.5*g/10000)\n",
    "        data[7] = 'FilePrefix: outfiles/ConstantSize{}_n{}_sim{}_t2\\n'.format(-g,400,n)\n",
    "\n",
    "        with open('simfiles/ParameterFilesConstant.txt', 'w') as file:\n",
    "            file.writelines(data)\n",
    "        \n",
    "        os.system(\"GSL_RNG_SEED={} GSL_RNG_TYPE=mrg ../../PReFerSim/PReFerSim simfiles/ParameterFilesConstant.txt 3 > /dev/null 2>&1 \".format(rng.integers(100496)))\n",
    "\n",
    "        dft = pd.read_csv('outfiles/ConstantSize{}_n{}_sim{}_t2.3.full_out.txt'.format(-g,400,n),sep='\\t',header=None,names=['','Xl','s','al','id'])\n",
    "        dft['empty'] = ''\n",
    "        dft['sXl'] = (dft['Xl']*400).astype('int')\n",
    "        dft['al'] = 80000+1 - dft['al']\n",
    "        dft['al'] = dft['al'].astype('int')\n",
    "        dft = dft.iloc[:,1:]\n",
    "        datt = dft.to_numpy()\n",
    "\n",
    "        SMS = np.zeros((80000,400+1),dtype='int16')\n",
    "        mask = np.zeros_like(SMS); mask[0,:] = 1; mask[:,0] = 1; mask[:,-1] = 1;\n",
    "        for i in range(len(datt)):\n",
    "            SMS[datt[i,2],datt[i,5]] += 1\n",
    "        SMSmask = np.ma.array(SMS,mask=mask)\n",
    "\n",
    "        sfs = moments.Spectrum(np.histogram(datt[:,5],bins=range(0,402))[0])\n",
    "\n",
    "        # parestfreq[ig,n] = sp.optimize.minimize_scalar(get_ll_freq, args=(datt[:,5]), options={'xtol': .05,}).x\n",
    "        # parestage[ig,n] = sp.optimize.minimize_scalar(get_ll_freqage, args=(datt[:,5], datt[:,2]), options={'xtol': .2}).x\n",
    "        parestfreq[ig,n] = sp.optimize.minimize_scalar(get_ll_freqconstant_notfm,args=({'sfs':sfs,'theta':2,'p_misid':0},400)).x\n",
    "        parestage[ig,n] = sp.optimize.minimize_scalar(get_ll_freqageconstant_notfm,args=({'sms':SMSmask,'theta':2,'N':10000,'p_misid':0,'gens':80000},400)).x\n",
    "\n",
    "        print(parestfreq[ig,n],parestage[ig,n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp.optimize.minimize_scalar(get_ll_freqconstant_notfm,args=({'sfs':sfs,'theta':200,'p_misid':0},400)).x, sp.optimize.minimize_scalar(get_ll_freqageconstant_notfm,args=({'sms':SMSmask,'theta':200,'N':10000,'p_misid':0,'gens':80000},400)).x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parestfreq, parestage/2\n",
    "# 10**sp.optimize.minimize_scalar(get_ll_freqageconstant,args=({'sms':SMSmask,'theta':400,'N':10000,'p_misid':0,'gens':80000},400),).x\n",
    "# 10**sp.optimize.minimize_scalar(get_ll_freqageconstant,args=({'sms':SMSmask,'theta':4,'N':10000,'p_misid':0,'gens':80000},400),).x\n",
    "# run_mom_iterate_constant??\n",
    "# -10**sp.optimize.minimize_scalar(get_ll_freqconstant,args=({'sfs':sfs,'theta':400,'p_misid':0},200)).x\n",
    "# parestfreq.std(axis=1)\n",
    "# dft = pd.read_csv('PReFerSims/outfiles/ConstantSize0.0.1.full_out.txt',sep='\\t',header=None,names=['','Xl','s','al','id'])\n",
    "# dft['empty'] = ''\n",
    "# dft['sXl'] = (dft['Xl']*400).astype('int')\n",
    "# dft['al'] = 80000+1 - dft['al']\n",
    "# dft['al'] = dft['al'].astype('int')\n",
    "# dft = dft.iloc[:,1:]\n",
    "# datt = dft.to_numpy()\n",
    "\n",
    "# sfs = moments.Spectrum(np.histogram(datt[:,5],bins=range(0,402))[0])\n",
    "# print(sp.optimize.minimize_scalar(get_ll_freqconstant_notfm,args=({'sfs':sfs,'theta':400,'p_misid':0},400)))\n",
    "\n",
    "# SMS = np.zeros((80000,400+1),dtype='int16')\n",
    "# mask = np.zeros_like(SMS); mask[0,:] = 1; mask[:,0] = 1; mask[:,-1] = 1;\n",
    "# for i in range(len(datt)):\n",
    "#     SMS[datt[i,2],datt[i,5]] += 1\n",
    "# SMSmask = np.ma.array(SMS,mask=mask)\n",
    "# plt.plot(np.linspace(-125,125,20),[get_ll_freqageconstant_notfm(g,{'sms':SMSmask,'theta':4,'N':10000,'p_misid':0,'gens':80000},400) for g in np.linspace(-125,125,20)],'ko')\n",
    "# sp.optimize.minimize_scalar(get_ll_freqageconstant_notfm,args=({'sms':SMSmask,'theta':400,'N':10000,'p_misid':0,'gens':80000},400))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parestfreq = -np.array([[100.756, 104.793, 104.931, 99.876, 98.784, 90.711, 97.843, 101.897, 100.753, 99.271],\n",
    "#         [9.616, 9.769, 10.315, 9.405, 10.001, 9.913, 9.895, 9.425, 9.713, 10.653],\n",
    "#         [1.113, 1.151, 1.204, 0.938, 1.036, 0.911, 1.122, 0.915, 1.026, 1.046],\n",
    "#         [-4.35832402069063e-45, -4.35832402069063e-45, -4.35832402069063e-45, -4.35832402069063e-45, -4.35832402069063e-45, -1.971976596466917e-15, -1.971976596466917e-15, -1.971975942072855e-15, -4.35832402069063e-45, -4.35832402069063e-45, -4.35832402069063e-45]])\n",
    "# parestage = -np.array([[107.958, 103.559, 111.178, 105.733, 98.938, 95.403, 104.087, 104.810, 103.923, 106.285],\n",
    "#         [10.354, 9.969, 10.613, 9.571, 10.331, 10.372, 10.616, 9.793, 10.120, 11.210],\n",
    "#         [1.005, 1.193, 1.148, 0.99, 0.846, 1.126, 0.93, 1.089, 1.044, 1.02]])\n",
    "\n",
    "# parestage = np.array([[-72.638, -72.416, -72.477, -72.360, -72.582, -72.191, -72.398,\n",
    "#          -72.534, -72.418, -72.591, -72.341, -72.598, -72.206, -72.451,\n",
    "#          -72.207, -72.625, -72.508, -72.380, -72.447, -72.224],\n",
    "#         [-25.950, -26.648, -25.815, -27.308, -26.011, -26.545, -25.472,\n",
    "#          -26.114, -25.423, -26.061, -25.936, -25.875, -25.913, -26.334,\n",
    "#          -26.247, -26.006, -27.079, -26.789, -26.381, -26.240],\n",
    "#         [-11.182, -10.847, -10.509, -10.994, -10.700, -10.616, -10.484,\n",
    "#          -10.843, -10.313, -11.236, -10.623, -10.742, -10.775, -10.517,\n",
    "#          -10.198, -10.811, -10.679, -10.517, -11.417, -10.289],\n",
    "#         [-2.967, -3.017, -3.077, -2.870, -2.618, -3.203, -3.312, -2.521,\n",
    "#          -2.955, -2.772, -2.618, -2.797, -2.593, -2.700, -3.156, -3.265, -3.135, -2.559, -2.755, -2.576],\n",
    "#         [-0.995, -0.618, -0.618, -0.618, -0.618, -0.908, -0.618, -0.618,\n",
    "#          -0.618, -0.618, -0.618, -0.618, -1.181, -0.618, -0.742, -0.618, -0.818, -0.742, -0.618, -0.742]],dtype=float)\n",
    "\n",
    "# parestfreq = np.array(([[-76.064, -70.595, -72.271, -70.274, -76.273, -66.556, -69.697, -74.476, -70.613, -73.723, -71.448, -74.220, -67.808, -71.549, -66.524, -77.530, -74.721, -71.273, -71.199, -66.284],[-23.513, -25.072, -24.235, -24.239, -23.385, -26.478, -23.393, -25.434, -22.406, -23.127, -24.769, -24.344, -24.670, -24.671, -23.045, -22.772, -27.346, -26.433, -24.705, -23.759],[-9.703, -7.282, -8.675, -7.885, -8.933, -9.850, -6.890, -7.391, -10.592, -7.572, -16.959, -7.963, -6.882, -17.281, -10.580, -19.203, -7.826, -18.361, -10.170, -9.423],[-1.969, -1.924, -1.915, -2.075, -2.034, -2.074, -2.038, -1.928, -2.039, -2.079, -2.634, -2.056, -1.916, -2.645, -2.654, -2.971, -2.752, -1.876,\n",
    "#        -2.518, -2.279],[-1.346, -0.618, -0.960, -0.618, -0.916, -1.148, -0.618, -0.618,-1.016, -0.824, -0.618, -0.814, -1.439, -0.943, -0.930, -1.000, -1.083, -0.885, -0.950, -0.964]]),dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setViolColors(bp):\n",
    "    bp['bodies'][0].set_facecolor('deepskyblue')\n",
    "    bp['bodies'][0].set_alpha(0.8)\n",
    "    bp['bodies'][0].set_linewidth(1)\n",
    "    bp['cbars'].set_colors('grey')\n",
    "    bp['cbars'].set_alpha(0.6)\n",
    "    bp['cmins'].set_color('grey')\n",
    "    bp['cmaxes'].set_color('grey')\n",
    "    # plt.setp(bp['cbars'][0], color='deepskyblue')\n",
    "    # plt.setp(bp['caps'][1], color='deepskyblue')\n",
    "    # plt.setp(bp['whiskers'][0], color='deepskyblue')\n",
    "    # plt.setp(bp['whiskers'][1], color='deepskyblue')\n",
    "    # plt.setp(bp['fliers'][0], color='deepskyblue')\n",
    "    # plt.setp(bp['medians'][0], color='deepskyblue')\n",
    "\n",
    "    plt.setp(bp['bodies'][1], color='coral', alpha=0.8)\n",
    "    # plt.setp(bp['cbars'][1], color='coral')\n",
    "    # plt.setp(bp['caps'][3], color='coral')\n",
    "    # plt.setp(bp['whiskers'][2], color='coral')\n",
    "    # plt.setp(bp['whiskers'][3], color='coral')\n",
    "    # plt.setp(bp['fliers'][1], color='coral')\n",
    "    # plt.setp(bp['medians'][1], color='coral')\n",
    "\n",
    "fig = plt.figure(dpi=130)\n",
    "ax = plt.axes()\n",
    "s = np.array([-100.,-10.,-1.,0])\n",
    "# hold(True)\n",
    "\n",
    "for i in range(len(s)):\n",
    "    # bp = plt.boxplot([parestfreq[i,~np.isnan(parestfreq[i,:])],parestage[i,~np.isnan(parestage[i,:])]],positions=[3*i+1,3*i+2], widths=0.6,flierprops=dict(marker='x',markersize=5,alpha=0.5),)\n",
    "    # setBoxColors(bp)\n",
    "    vp = plt.violinplot([parestfreq[i,:],parestage[i,:]],positions=[3*i+1,3*i+2], widths=0.6,)\n",
    "    setViolColors(vp)\n",
    "    plt.axhline(s[i],color='grey',ls='--',alpha=0.6)\n",
    "\n",
    "# bp = plt.boxplot([parestfreq0,parestage0],positions=[16,17],widths=0.6,flierprops=dict(marker='x',markersize=5,alpha=0.5))\n",
    "# setBoxColors(bp) \n",
    "# vp = plt.violinplot([-parestfreq[-1,:],-parestage[-1,:]],positions=[9,10],widths=0.6,)\n",
    "# setViolColors(vp)\n",
    "# plt.axhline(0.,color='grey',ls='--',alpha=0.6);\n",
    "\n",
    "# ax.set_xticks([1.5,4.5,7.5,10.5,13.5,16.5]); ax.set_xticklabels(np.ravel(s.tolist()+[0.])); ax.set_yscale('symlog');\n",
    "ax.set_xticks([1.5,4.5,7.5,10.5]); ax.set_xticklabels(np.ravel(s.tolist())); ax.set_yscale('symlog'); ax.set_yticks([-100.,-10.,-1.,0,]); plt.grid()\n",
    "\n",
    "hB, = plt.plot([1,0],'-',color='deepskyblue'); hR, = plt.plot([1,0],'-',color='coral'); hB.set_visible(False); hR.set_visible(False)\n",
    "plt.xlabel('true γ value'); plt.ylabel('MLE γ value (20 replicates)')\n",
    "plt.legend((hB, hR),('freq only, RMSLE: {:.2f}'.format(np.sqrt(np.mean((np.log10(-s+1)-np.log10(np.nanmean(parestfreq,axis=1)))**2))), 'freq & age, RMSLE: {:.2f}'.format(np.sqrt(np.mean((np.log10(-s+1)-np.log10(np.nanmean(parestage,axis=1)))**2)))),loc='upper left')\n",
    "# plt.savefig(\"../figs/MLEfreqage2.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot([-100,-10,-1,0],np.std(parestfreq,axis=1)/np.std(parestage,axis=1),'k-o', alpha=0.7); plt.xscale('symlog'); plt.ylabel('ratio of SD'); plt.xlabel(r'$\\gamma$'); plt.axhline(1,color='grey')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dft = pd.read_csv('outfiles/ConstantSize100.0.3.full_out.txt',sep='\\t',header=None,names=['','Xl','s','al','id'])\n",
    "dft['empty'] = ''\n",
    "dft['sXl'] = (dft['Xl']*400).astype('int')\n",
    "dft['al'] = 80000+1 - dft['al']\n",
    "dft['al'] = dft['al'].astype('int')\n",
    "dft = dft.iloc[:,1:]\n",
    "datt = dft.to_numpy()\n",
    "\n",
    "SMS = np.zeros((80000,400+1),dtype='int16')\n",
    "mask = np.zeros_like(SMS); mask[0,:] = 1; mask[:,0] = 1; mask[:,-1] = 1;\n",
    "for i in range(len(datt)):\n",
    "    if (datt[i,2]<80000) & (datt[i,5]<400):\n",
    "        SMS[datt[i,2],datt[i,5]] += 1\n",
    "SMSmask = np.ma.array(SMS,mask=mask)\n",
    "\n",
    "# thetas = np.logspace(0,2,12)\n",
    "# gammas = np.logspace(-1,2,12,base=10)\n",
    "# ll2d = np.zeros((len(thetas),len(gammas)))\n",
    "# for it, t in enumerate(thetas):\n",
    "#     for ig, g in enumerate(gammas):\n",
    "#         ll2d[it,ig] = get_ll_freqageconstant(np.log10(g),{'sms':SMSmask,'theta':t,'N':10000,'p_misid':0,'gens':50000},400)\n",
    "\n",
    "# plt.imshow(-ll2d+np.min(ll2d),vmin=-1000); plt.colorbar(); plt.ylabel(r'$\\theta$'); plt.xlabel(r'$\\gamma$'); plt.yticks(np.arange(0,12,3),labels=np.round(thetas[::3],1)); plt.xticks(np.arange(0,12,3),labels=np.round(gammas[::3],1)); plt.plot(8,4,'kx'); plt.axvline(8.3,color='red')\n",
    "# plt.plot(np.linspace(1,10,5),[get_ll_freqageconstant(np.log10(10),{'sms':SMSmask,'theta':t,'N':10000,'p_misid':0,'gens':20000},200) for t in np.linspace(1,10,5)])\n",
    "# get_ll_freqageconstant(np.log10(1),{'sms':SMSmask,'theta':400,'N':10000,'p_misid':0,'gens':100000},400), get_ll_freqageconstant(np.log10(1),{'sms':SMSmask,'theta':4,'N':10000,'p_misid':0,'gens':100000},400), get_ll_freqageconstant(np.log10(1e-7),{'sms':SMSmask,'theta':4,'N':10000,'p_misid':0,'gens':100000},400)\n",
    "# sp.optimize.minimize_scalar(get_ll_freqageconstant,args=({'sms':SMSmask,'theta':7.2,'N':10000,'p_misid':0,'gens':50000},400))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SMSmask.sum(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Relevent error distribution for allele ages\n",
    "Initially, using a simplification of estimates from Figure 2 in Albers & McVean, 2018 (piecewise-linear fit) with Poisson distributed error. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_age_err(simal, rng, errvar):\n",
    "    erral = np.zeros_like(simal,dtype='int')\n",
    "    mask1 = (simal>0) & (simal<21)\n",
    "    mask2 = (simal>20) & (simal<1001)\n",
    "    mask3 = (simal>1000) & (simal<20001)\n",
    "    mask4 = simal>20000\n",
    "    \n",
    "    erral[mask1] = rng.normal(rng.poisson(10+simal[mask1].astype('int')*1.5, ),errvar[0])\n",
    "    erral[mask2] = rng.normal(rng.poisson(15+simal[mask2].astype('int')*95/98, ),errvar[1])\n",
    "    erral[mask3] = rng.normal(rng.poisson(700+simal[mask3].astype('int')*6/19, ),errvar[2])\n",
    "    erral[mask4] = rng.normal(rng.poisson(2000+simal[mask4].astype('int')*80/98, ),errvar[3])\n",
    "\n",
    "    # erral[mask1] = rng.lognormal(simal[mask1].astype('int')*1.5, 0.02)\n",
    "    # erral[mask2] = rng.lognormal(15+simal[mask2].astype('int')*95/98, 0.02)\n",
    "    # erral[mask3] = rng.lognormal(400+simal[mask3].astype('int')*11/19, 0.02)\n",
    "    # erral[mask4] = rng.lognormal(2000+simal[mask4].astype('int')*80/98, 0.02)\n",
    "\n",
    "    return erral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rng = np.random.RandomState(234)\n",
    "# errvar = [20,100,200,1500]\n",
    "errvar = np.repeat(0,4)\n",
    "erral = get_age_err(np.repeat(np.arange(10000)+1,20),rng,errvar)\n",
    "# plt.hist(erral[mask1],50)\n",
    "# plt.hist(simal[mask1],50)\n",
    "plt.figure(dpi=120); plt.scatter(np.repeat(np.arange(10000)+1,20),erral); plt.loglog(); plt.axline((1,1),(100,100),color='grey',ls='--'); plt.grid(); plt.xlabel('true ages', fontsize=9); plt.ylabel('ages with Normal distributed error rates', fontsize=9); plt.xticks(fontsize=10); plt.yticks(fontsize=10)\n",
    "#plt.plot([1,20],[10,35],color='silver',linewidth=1.5); plt.plot([20,1000],[35,1035],color='silver',linewidth=1.5)\n",
    "# mask1 = (simal>0) & (simal<21)\n",
    "# mask2 = (simal>20) & (simal<1001)\n",
    "# mask3 = (simal>1000) & (simal<20001)\n",
    "# mask4 = simal>20000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.repeat(np.arange(5000)+1,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trueg1 = -0.01\n",
    "trueg2 = gamma[-10]\n",
    "# newdf1 = df1.iloc[np.ravel(np.where(dat[:,1]==trueg1)),0:3].append(df1.iloc[np.ravel(np.where(dat[:,1]==trueg2)),0:3])\n",
    "newdf1 = df1.iloc[np.ravel(np.where(dat[:,1]==trueg2)),:]\n",
    "newdat = newdf1.to_numpy()\n",
    "\n",
    "print(gamma2[np.argmax([np.sum(get_lp_xl(g1, newdat[:,5], cutoff=2)) for g1 in gamma2])])\n",
    "print(gamma2[np.argmax([np.sum(get_lp_alxl_nocheck(g1, newdat[:,5], newdat[:,2], cutoff=2)) for g1 in gamma2])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[np.sum(get_lp_xl(g1, newdat[:,5], cutoff=2)) for g1 in gamma2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llfreq, llage = np.zeros(len(gamma2)), np.zeros(len(gamma2))\n",
    "for ig, g in enumerate(gamma2):\n",
    "    llfreq[ig] = get_lp_xl(g, newdat[:,5], cutoff=2)\n",
    "    llfreq[ig] = get_lp_alxl(g, newdat[:,5], newdat[:,2], cutoff=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## getting parametric bootstrap, basically compute MLE s for each data point and then find the 95% CI/2 std to get info measure\n",
    "mlegfreq = np.zeros((len(gamma),len(newdat)))\n",
    "mlegage = np.zeros((len(gamma),len(newdat)))\n",
    "for it, trueg2 in enumerate(gamma): \n",
    "    newdf1 = df1.iloc[np.ravel(np.where(dat[:,1]==trueg2)),:]\n",
    "    newdat = newdf1.to_numpy()\n",
    "    for i in range(len(newdat)):\n",
    "        mlegfreq[it,i] = gamma2[np.argmax([np.log(p_xa_s[g1][newdat[i,5]]) for g1 in gamma2])]\n",
    "        # mlegfreq[it,i] = gamma2[np.argmax([sp.stats.poisson.logpmf(newdat[i,5],mu=p_xa_s[g1][newdat[i,5]]) for g1 in gamma2])]\n",
    "        mlegage[it,i] = gamma2[np.argmax([get_lp_alxl_nocheck(g1, newdat[i,5], newdat[i,2], cutoff=2) for g1 in gamma2])]\n",
    "        # mlegage[it,i] = gamma2[np.argmax([sp.stats.poisson.logpmf(newdat[i,5],mu=np.exp(get_lp_alxl_nocheck(g1, newdat[i,5], newdat[i,2], cutoff=2))) for g1 in gamma2])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(mlegfreq,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.scatter(newdat[:,5],mleg,alpha=0.4); plt.xlabel('# of derived alleles'); plt.ylabel('MLE γ'); plt.axhline(trueg2, ls='--', color='red')\n",
    "plt.boxplot(np.array([mlegfreq[5,],mlegage[5,]]).T,labels=['freq only', 'freq & age']); plt.axhline(gamma[5],color='red',ls='--'); "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(gamma,mlegfreq.std(axis=1/mlegage.std(axis=1),color='k',alpha=0.7); plt.axhline(1,color='grey',ls='--'); plt.grid(); \n",
    "plt.xlabel('γ'); plt.ylabel('SD of freq/SD of freq & age')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(get_lp_xl(g, newdat[:,5], cutoff=10), get_lp_xl2(g, newdat[:,5], cutoff=10), alpha=0.7, color='k', s=2.5); \n",
    "plt.axline((-5.75,1.75),slope=1.,color='grey',ls='--',linewidth=0.5);\n",
    "plt.xlabel('p(X|γ) using moments framework'); plt.ylabel('using PRF approx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sin_onlyfreq = np.empty(len(gamma2))\n",
    "sin_onlyage = np.empty(len(gamma2))\n",
    "# num_sims = 3\n",
    "# num_samps = [35,350,3500]\n",
    "# info_onlyfreq, info_onlyage = np.zeros((len(num_samps),num_sims)), np.zeros((len(num_samps),num_sims))\n",
    "# nboot = 10\n",
    "# for i in range(nboot):\n",
    "#     newdat = newdat[np.random.choice(len(newdat),1000,replace=True)]\n",
    "    # for ig, g in enumerate(gamma):\n",
    "        # sum log prob for each locus\n",
    "        # sin_onlyfreq[ig] = np.sum(get_lp_xl(g, newdat[:,5], cutoff=2))\n",
    "        # sin_onlyage[ig] = np.sum(get_lp_alxl(g, newdat[:,5], newdat[:,2], cutoff=2))\n",
    "        # mle = get_boot_ci(newdat,nboot=2)\n",
    "    # print(gamma[np.argmax(sin_onlyfreq)], gamma[np.argmax(sin_onlyage)])\n",
    "\n",
    "for ig, g in enumerate(gamma2):\n",
    "    sin_onlyfreq[ig] = np.sum(get_lp_xl(g, newdat[:,5], cutoff=2))\n",
    "    sin_onlyage[ig] = np.sum(get_lp_alxl(g, newdat[:,5], newdat[:,2], cutoff=2))\n",
    "\n",
    "# mle = get_boot_ci(newdat,nboot=20)\n",
    "# mle.mean(axis=0), mle.std(axis=0)\n",
    "\n",
    "# for ins, ns in enumerate(num_samps):\n",
    "#     info_onlyfreq[ins,:], info_onlyage[ins,:] = get_info_content(newdat, num_samps=ns, num_sims=num_sims, cutoff=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_mean_sd_werr(newdat, nreps=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## compute gain in information for the same number of sites but over a range of gamma\n",
    "gain_info = np.empty((3,num_sims))\n",
    "info_onlyfreq, info_onlyage = np.zeros_like(gain_info), np.zeros_like(gain_info)\n",
    "# for ig, g in enumerate(gamma):\n",
    "#     newdf1 = df1.iloc[np.ravel(np.where(dat[:,1]==g)),:]\n",
    "#     newdat = newdf1.to_numpy()\n",
    "\n",
    "#     info_onlyfreq[2,:], info_onlyage[2,:] = get_info_content(newdat, num_samps=5000, num_sims=num_sims, cutoff=2)\n",
    "#     gain_info[ig,:] = np.divide(info_onlyage[2,:],info_onlyfreq[2,:])\n",
    "\n",
    "## but need to resimulate a bunch of new sites for each gamma (do it only for 1, 10, 100 first)\n",
    "for ig, g in enumerate([-1., -10., -100.]):\n",
    "    for n in range(num_sims):\n",
    "        with open('simfiles/ParameterFilesConstant.txt',\"r\") as file:\n",
    "            data = file.readlines()\n",
    "\n",
    "        data[0] = 'MutationRate: {:f}\\n'.format(250)\n",
    "        data[2] = 'DFEPointSelectionCoefficient: {:.8f}\\n'.format(-0.25*g/10000)\n",
    "        data[7] = 'FilePrefix: outfiles/ConstantSize{}\\n'.format(-g)\n",
    "\n",
    "        with open('simfiles/ParameterFilesConstant.txt', 'w') as file:\n",
    "            file.writelines(data)\n",
    "        \n",
    "        os.system(\"GSL_RNG_SEED={} GSL_RNG_TYPE=mrg ../../PReFerSim/PReFerSim simfiles/ParameterFilesConstant.txt 3 > /dev/null 2>&1 \".format(rng.integers(100496)))\n",
    "\n",
    "        dft = pd.read_csv('outfiles/ConstantSize{}.3.full_out.txt'.format(-g),sep='\\t',header=None,names=['','Xl','s','al','id'])\n",
    "        dft['empty'] = ''\n",
    "        dft['sXl'] = (dft['Xl']*2000).astype('int')\n",
    "        dft['al'] = 80000+1 - dft['al']\n",
    "        dft['al'] = dft['al'].astype('int')\n",
    "        dft = dft.iloc[:,1:]\n",
    "        if(len(dft)>1000):\n",
    "            datt = dft.sample(n=1000).to_numpy()\n",
    "        else: \n",
    "            datt = dft.to_numpy()\n",
    "\n",
    "        info_onlyfreq[ig,n], info_onlyage[ig,n] = get_info_content(datt, num_samps=len(datt), num_sims=1, cutoff=2)\n",
    "    \n",
    "    gain_info[ig,:] = np.divide(info_onlyage[ig,:],info_onlyfreq[ig,:])\n",
    "    # print(gain_info[ig,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gain_info[1,:] = [1.412, 1.498, 1.583, 1.728, 1.718, 1.688, 1.489, 1.541, 1.494, 1.526, 1.440, 1.337, 1.306, 1.564, 1.461, 1.488]\n",
    "# gain_info[2,:] = [1.546, 1.551, 1.545, 1.556, 1.542, 1.547, 1.537, 1.538, 1.532, 1.544, 1.546, 1.543, 1.539, 1.551, 1.553, 1.554]\n",
    "# gain_info[0,:] = [np.nan, 1.100, 1.132, 1.092, 1.088, 1.098, 1.104, 1.090, 1.130, 1.099, 1.122, 1.093, 1.094, 1.105, 1.105, 1.118]\n",
    "# (array([1.962, 2.510, 2.103, 1.538, 1.352, 1.590]),\n",
    "#  array([0.267, 0.974, 0.428, 0.224, 0.032, 0.011]))\n",
    "# array([[1.814, 1.361, 1.377, 1.944, 1.836, 1.736, 0.520, 1.882, 1.884,\n",
    "#         1.469, 1.042, 0.974, 1.816, 1.457, 2.198, 2.188],\n",
    "#        [0.074, 0.082, 0.077, 0.176, 0.080, 0.068, 0.077, 0.087, 0.076,\n",
    "#         0.075, 0.070, 0.072, 0.078, 0.073, 0.070, 0.164],\n",
    "#        [1.502, 1.510, 1.494, 1.521, 1.512, 1.539, 1.526, 1.513, 1.521,\n",
    "#         1.517, 1.512, 1.532, 1.483, 1.505, 1.522, 1.516]])\n",
    "# gain_info.mean(axis=1), gain_info.std(axis=1)\n",
    "# mean and sd of -0.1, -1., -10., -100 under 2 different regimes (no error, low GEVA error)\n",
    "gain_info_mean = np.array([[1.048, 0.988, 1.005, 1.019, 1.011, 1.32, 1.484, 1.435, 1.494]])\n",
    "gain_info_sd = np.array([[0.121, 0.06, 0.027, 0.038, 0.017, 0.42, 0.059, 0.135, 0.149]])\n",
    "# gain_info_mean = np.array([[1.048, 1.005, 1.484, 1.494],[0.821,0.838,1.037,1.307]])\n",
    "# gain_info_sd = np.array([[0.121, 0.027, 0.059, 0.149],[0.01,0.037,0.222,0.234]])\n",
    "# no point in computing the gain in information under high error since MLE is not even the same...\n",
    "# gamma[2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(info_onlyage, info_onlyfreq)\n",
    "print(gain_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.divide(infoage, infofreq)\n",
    "np.abs([get_bfq(ll_adam, gamma2)[0], get_bfq(sin_onlyfreq, gamma2)[0], get_bfq(sin_onlyage, gamma2)[0]])\n",
    "# -get_bfq(ll_adam-np.max(ll_adam), gamma2)[1]*0.5/get_bfq(ll_adam-np.max(ll_adam), gamma2)[0], -get_bfq(sin_onlyfreq-np.max(sin_onlyfreq), gamma2)[1]*0.5/get_bfq(sin_onlyfreq-np.max(sin_onlyfreq), gamma2)[0], -get_bfq(sin_onlyage-np.max(sin_onlyage), gamma2)[1]*0.5/get_bfq(sin_onlyage-np.max(sin_onlyage), gamma2)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seaborn.stripplot(data=pd.DataFrame(gain_info.T,columns=gamma),color='salmon',alpha=0.8,size=7); \n",
    "# plt.xlabel('|γ|'); plt.ylabel('gain in information'); plt.title('20 reps of 5000 sites'); plt.grid();\n",
    "plt.figure(dpi=130)\n",
    "plt.scatter(np.array([0.099,0.299,0.99,1.77,3.29,7.29,9.9,29,98]),gain_info_mean[0,:],color='slategrey',alpha=0.9,); \n",
    "# plt.scatter(np.array([0.11,1.01,10.1,102]),gain_info_mean[1,:],color='lightsteeldeepskydeepskydeepskyblue',alpha=0.9,label='GEVA error'); plt.grid()\n",
    "plt.errorbar(np.array([0.099,0.299,0.99,1.77,3.29,7.29,9.9,29,98]),gain_info_mean[0,:],yerr=gain_info_sd[0,:],color='slategrey',alpha=0.9,linewidth=2,ls='none'); plt.xticks(fontsize=10); plt.yticks(fontsize=10)\n",
    "# plt.errorbar(np.array([0.11,1.01,10.1,102]),gain_info_mean[1,:],yerr=gain_info_sd[1,:],color='lightsteelblue',alpha=0.9,linewidth=2,ls='none'); \n",
    "# plt.scatter(-gamma[2:],gain_info[0],alpha=0.8,color='k'); plt.semilogx(); plt.xlabel('|γ|'); plt.title('20 reps of 1000 sites')\n",
    "# plt.errorbar(-gamma[2:],gain_info[0],yerr=2*gain_info[1],color='k',alpha=0.8); plt.grid(); plt.ylabel('gain in info'); \n",
    "plt.axhline(1.,color='grey',ls='--',linewidth=2,alpha=0.7); plt.ylabel('ratio of curvature +/- 1 SD', fontsize=10); plt.xlabel('|γ|', fontsize=16); plt.semilogx(); plt.grid(); #plt.legend(loc=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Working with Adam's log-likelihoods\n",
    "(and data, since we only need a subset of alleles...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ll_adam = np.array([-12779.905577243711,-12779.67560657378,-12779.345559393892,-12778.872363586619,-12778.19492458077,-12777.22714620313,-12775.848879337178,-12773.89494812682,-12771.143604153498,-12767.30870429352,-12762.046666304424,-12755.004172803323,-12745.96462111521,-12735.2182496041,-12724.415368038992,-12718.41220639416,-12729.002025638603,-12781.678920760727,-12925.363284151737,-13238.84902857637]) # using ~ 500 trajectories\n",
    "ll_adam=np.array([-29565.76313316775,-29565.621718394377,-29565.418924835354,-29565.128508755162,-29564.713434007765,-29564.121904008814,-29563.282471330967,-29562.098695611345,-29560.44496421317,-29558.16773914124,-29555.102378376516,-29551.12840744588,-29546.313073918434,-29541.248815564642,-29537.801653373364,-29540.693962031066,-29560.662504959695,-29620.153166439388,-29761.530313756353,-30052.589833320435]) # using ~3000 trajectories \n",
    "# plt.plot(-gamma2,ll_adam-np.max(ll_adam),'-o',alpha=0.5,label='full traj',color='k'); plt.grid(); plt.plot(-gamma2, sin_onlyfreq-np.max(sin_onlyfreq),'-o',alpha=0.5,label=\"only freq\",color='deepskyblue'); plt.plot(-gamma2,sin_onlyage-np.max(sin_onlyage),'-o',alpha=0.5,label=\"freq & age\",color='coral'); plt.ylim((-200,10)); plt.semilogx()\n",
    "plt.plot(-gamma2,ll_adam-np.max(ll_adam),'-o',alpha=0.5,label='full traj',color='k'); plt.grid(); plt.plot(-gamma2, np.sum(mlegfreq,axis=1)-np.max(np.sum(mlegfreq,axis=1)),'-o',alpha=0.5,label=\"only freq\",color='deepskyblue'); plt.plot(-gamma2,np.sum(mlegage,axis=1)-np.max(np.sum(mlegage,axis=1)),'-o',alpha=0.5,label=\"freq & age\",color='coral'); plt.ylim((-200,10)); plt.semilogx()\n",
    "#plt.axvline(-gamma2[np.argmax(sin_onlyage)],color='grey',ls='--'); \n",
    "plt.axvline(100.,color='red',); plt.xlabel('|γ|'); plt.ylabel('log-lik'); plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(dft.loc[dft['id'].isin(alls['IDs'])]['sXl']/20,dft.loc[dft['id'].isin(alls['IDs'])]['al'],color='grey',alpha=0.7); plt.xlabel('allele freq (%)'); plt.ylabel('allele age'); plt.semilogy()\n",
    "# datt = dft.loc[dft['id'].isin(alls['IDs'])].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(-np.repeat(gamma,num_sims),gain_info,s=10,color='salmon',alpha=0.6); plt.semilogx()\n",
    "plt.xlabel('|γ|'); plt.ylabel('gain in information'); plt.title('20 reps of 5000 sites'); plt.grid();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_boot_ci(newdat, nsamps=1000, nboot=20, cutoff=2):\n",
    "    mle = np.zeros((nboot,2))\n",
    "    sin_onlyfreq, sin_onlyage = np.empty(len(gamma2)), np.empty(len(gamma2))\n",
    "    for i in range(nboot):\n",
    "        newnewdat = newdat[np.random.choice(len(newdat),nsamps,replace=True)]\n",
    "        for ig, g in enumerate(gamma2):\n",
    "            sin_onlyfreq[ig] = np.sum(get_lp_xl(g, newnewdat[:,5], cutoff=cutoff))\n",
    "            sin_onlyage[ig] = np.sum(get_lp_alxl(g, newnewdat[:,5], newnewdat[:,2], cutoff=cutoff))\n",
    "        \n",
    "        mle[i,0] = gamma2[np.argmax(sin_onlyfreq)]\n",
    "        mle[i,1] = gamma2[np.argmax(sin_onlyage)]\n",
    "\n",
    "    return mle            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mean_sd_werr(newdat, errvar, nreps=20, cutoff=2):\n",
    "    ests = np.zeros(nreps)\n",
    "    sin_onlyage = np.empty(len(gamma2))\n",
    "    for i in range(nreps):\n",
    "        newage = get_age_err(newdat[:,2],rng,errvar)\n",
    "        sin_onlyage = [np.sum(get_lp_alxl(g, newdat[:,5], newage, cutoff=cutoff)) for g in gamma2]\n",
    "        \n",
    "        ests[i] = gamma2[np.argmax(sin_onlyage)]\n",
    "    \n",
    "    return ests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seaborn.stripplot(data=pd.DataFrame(np.divide(info_onlyage, info_onlyfreq).T,columns=num_samps),color='salmon',alpha=0.8); \n",
    "plt.xlabel('# of sites'); plt.ylabel('gain in information'); plt.title('γ = {:.1f}'.format(trueg2)); plt.grid(); "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seaborn.stripplot(data=pd.DataFrame(info_onlyfreq.T,columns=num_samps), color='deepskyblue', alpha=0.7, label='only freq')\n",
    "seaborn.stripplot(data=pd.DataFrame(info_onlyage.T,columns=num_samps), color='coral', alpha=0.7, label='freq & age')\n",
    "plt.xlabel('# of sites'); plt.ylabel('information measure'); plt.grid(); plt.title('γ = {:.1f}'.format(trueg2)); \n",
    "plt.legend(handles=[mpatches.Patch(color='deepskyblue', label='only freq, total info: {:.2f}'.format(np.sum(info_onlyfreq))), mpatches.Patch(color='coral', label='freq & age, total info: {:.2f}'.format(np.sum(info_onlyage)))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_info_content(newdat, num_samps=800, num_sims=16, cutoff=2):\n",
    "    ci_freq, ci_age = np.zeros(num_sims), np.zeros(num_sims)\n",
    "    for n in range(num_sims):\n",
    "        newnewdat = newdat[rng.choice(newdat.shape[0], num_samps, replace=False),:]\n",
    "        sin_onlyfreq = [np.sum(get_lp_xl(g1, newnewdat[:,5], cutoff=cutoff)) for g1 in gamma2]\n",
    "        sin_onlyage = [np.sum(get_lp_alxl(g1, newnewdat[:,5], newnewdat[:,2], cutoff=cutoff)) for g1 in gamma2]\n",
    "\n",
    "        ci_freq[n] = np.abs(get_bfq(sin_onlyfreq-np.max(sin_onlyfreq), gamma2)[0])\n",
    "        ci_age[n] = np.abs(get_bfq(sin_onlyage-np.max(sin_onlyage), gamma2)[0])\n",
    "\n",
    "    return [ci_freq, ci_age]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_info_content_werr(newdat, errvar, num_samps=800, reps=16, cutoff=2):\n",
    "    ci_freq, ci_age = np.zeros(reps), np.zeros(reps)\n",
    "    for n in range(reps):\n",
    "        newnewdat = newdat[rng.choice(newdat.shape[0], num_samps, replace=False),:]\n",
    "        sin_onlyfreq = [np.sum(get_lp_xl(g1, newnewdat[:,5], cutoff=cutoff)) for g1 in gamma2]\n",
    "        ci_freq[n] = np.abs(get_bfq(sin_onlyfreq-np.max(sin_onlyfreq), gamma2)[0])\n",
    "        for r in range(reps):\n",
    "            newage = get_age_err(newnewdat[:,2],rng,errvar)\n",
    "            sin_onlyage = [np.sum(get_lp_alxl(g1, newnewdat[:,5], newage, cutoff=cutoff)) for g1 in gamma2]\n",
    "            # print(gamma2[np.argmax(sin_onlyage)])\n",
    "            ci_age[n] += np.abs(get_bfq(sin_onlyage-np.max(sin_onlyage), gamma2)[0])\n",
    "\n",
    "    return [ci_freq, ci_age/reps]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_conf_int(loglik, thresh=2):\n",
    "    mle = gamma[np.argmax(loglik)]\n",
    "\n",
    "    if mle==np.min(gamma):\n",
    "        lower_thresh = gamma[1]  \n",
    "        return [(np.max(loglik)-thresh-loglik[np.argmax(loglik)+1])/(np.max(loglik) - loglik[np.argmax(loglik)+1])/(mle - lower_thresh)+lower_thresh, mle]\n",
    "    elif mle==np.max(gamma):\n",
    "        upper_thresh = gamma[-2] \n",
    "        return [mle, -thresh/(loglik[np.argmax(loglik)-1] - np.max(loglik))/(upper_thresh - mle)+mle]\n",
    "    else:\n",
    "        lower_thresh = gamma[np.argmax(loglik)+1] \n",
    "        upper_thresh = gamma[np.argmax(loglik)-1] \n",
    "        return [(np.max(loglik)-thresh-loglik[np.argmax(loglik)+1])*(mle - lower_thresh)/(np.max(loglik) - loglik[np.argmax(loglik)+1])+lower_thresh, -thresh*(upper_thresh - mle)/(loglik[np.argmax(loglik)-1] - np.max(loglik))+mle,]\n",
    "        # return [(np.max(loglik)-thresh-loglik[np.argmax(loglik)+1])/(np.max(loglik) - loglik[np.argmax(loglik)+1])/(mle - lower_thresh)+lower_thresh, -thresh/(loglik[np.argmax(loglik)-1] - np.max(loglik))/(upper_thresh - mle)+mle]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## run the same search on a denser grid\n",
    "win_sin_onlyfreq = np.empty(interp_gamma.shape[1])\n",
    "for ig, g in enumerate(interp_gamma[np.argmax(sin_onlyfreq),]):\n",
    "    win_sin_onlyfreq[ig] = np.sum(get_lp_xl(g, newdat[:,5], cutoff=10))\n",
    "\n",
    "win_sin_onlyage = np.empty(interp_gamma.shape[1])\n",
    "for ig, g in enumerate(interp_gamma[np.argmax(sin_onlyage),]):\n",
    "    win_sin_onlyage[ig] = np.sum(get_lp_alxl(g, newdat[:,5], newdat[:,2], cutoff=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "densegrid = -np.logspace(0,2,base=10,num=100)\n",
    "lxlbfq = get_bfq(sin_onlyfreq-np.max(sin_onlyfreq), gamma) @ np.vstack((densegrid**2, densegrid,np.repeat(1.0,100)))\n",
    "# lxlbfq = get_bfq_win(sin_onlyfreq-np.max(sin_onlyfreq), gamma) @ np.vstack((np.repeat(1.0,len(gamma)), gamma, gamma**2,))\n",
    "# lxlalbfq = np.polynomial.polynomial.Polynomial.fit(gamma[7:10],(sin_onlyage-np.max(sin_onlyage))[7:10],deg=2).coef @ np.vstack((np.repeat(1.0,4), gamma[(np.argmax(sin_onlyage)-1):(np.argmax(sin_onlyage)+3)], gamma[(np.argmax(sin_onlyage)-1):(np.argmax(sin_onlyage)+3)]**2,))\n",
    "lxlalbfq = get_bfq(sin_onlyage-np.max(sin_onlyage), gamma) @ np.vstack((densegrid**2, densegrid,np.repeat(1.0,100)))\n",
    "# lxlalbfq = get_bfq_win(sin_onlyage-np.max(sin_onlyage), gamma) @ np.vstack((np.repeat(1.0,len(gamma)), gamma, gamma**2,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(-gamma2, sin_onlyfreq-np.max(sin_onlyfreq),'grey', label='freq'); plt.ylim((-10,1)); plt.xlabel('est gamma')\n",
    "plt.axvline(-gamma2[np.argmax(sin_onlyfreq)], color='grey', linestyle='--'); \n",
    "# plt.plot(-densegrid, lxlbfq-np.max(lxlbfq), color='deepskyblue', alpha=0.6,label='best-fit quadratic Xl');#interp_gamma\n",
    "plt.axvline(1., color='red', ls='--'); plt.semilogx()\n",
    "plt.plot(-gamma2, sin_onlyage-np.max(sin_onlyage),'k', label='freq & age'); plt.ylim((-10,1)); \n",
    "# plt.plot(-densegrid, lxlalbfq-np.max(lxlalbfq), color='coral', alpha=0.6,label='best-fit quadratic Xl, al');#interp_gamma[np.argmax(sin_onlyage)]\n",
    "plt.axvline(-gamma2[np.argmax(sin_onlyage)], color='k', linestyle='--'); plt.ylabel('log-lik units'); plt.legend(); plt.grid()\n",
    "plt.axhline(-2.,color='red',alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bfq(loglik, gamma):\n",
    "    ## does not work for some reason—wasted multiple hours on it...\n",
    "    # return np.polynomial.polynomial.Polynomial.fit(-gamma[(ig-3):(ig+3)], loglik[(ig-3):(ig+3)], deg=2)\n",
    "    igamma = gamma[(np.argmax(loglik)-1):(np.argmax(loglik)+2)] if np.argmax(loglik)>0 else gamma[0:3]\n",
    "    loglik = loglik[(np.argmax(loglik)-1):(np.argmax(loglik)+2)] if np.argmax(loglik)>0 else loglik[0:3]\n",
    "\n",
    "    rhs = np.array([np.dot(igamma**2,loglik), np.dot(igamma,loglik), np.sum(loglik)])\n",
    "    lhs = np.array([[np.sum(igamma**4), np.sum(igamma**3), np.sum(igamma**2)],\n",
    "    [np.sum(igamma**3), np.sum(igamma**2), np.sum(igamma)],\n",
    "    [np.sum(igamma**2), np.sum(igamma), len(igamma)]])\n",
    "\n",
    "    return np.linalg.solve(lhs, rhs) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_sin_onlyfreq = np.zeros(len(s))\n",
    "ci_sin_onlyfreq = np.zeros((len(s),2))\n",
    "\n",
    "for ig, g in enumerate(s):\n",
    "    newdf1 = df1.iloc[np.ravel(np.where(dat[:,1]==g)),:]\n",
    "    newdat = newdf1.to_numpy()\n",
    "    # sin_onlyfreq = [np.sum(get_lp_xl(g1, newdat[:,5], cutoff=5)) for g1 in gamma]\n",
    "    # preds_sin_onlyfreq[ig] = gamma[np.argmax(sin_onlyfreq)]\n",
    "    # ci_sin_onlyfreq[ig] = 1/np.abs(get_bfq(sin_onlyfreq - np.argmax(sin_onlyfreq), gamma)[0]) # get_conf_int(sin_onlyfreq)\n",
    "    info_onlyfreq[ig,:], info_onlyage[ig,:] = get_info_content(newdat, num_samps=1000, num_sims=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preds_sin_agefreq = np.zeros(len(gamma))\n",
    "# for ig, g in enumerate(gamma):\n",
    "#     newdf1 = df1.iloc[np.ravel(np.where(dat[:,1]==g)),:]\n",
    "#     newdat = newdf1.to_numpy()\n",
    "#     sin_agefreq = [np.sum(get_lp_alxl(g1, newdat[:,5], newdat[:,2]) + get_lp_xl(g1, newdat[:,5])) for g1 in gamma]\n",
    "#     # sin_agefreq = [np.sum(get_lp_alxl(g1, newdat[:,0], newdat[:,2], n=100)) for g1 in gamma]\n",
    "#     preds_sin_agefreq[ig] = gamma[np.nanargmax(sin_agefreq)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_sin_onlyage = np.zeros(len(gamma))\n",
    "ci_sin_onlyage = np.zeros((len(gamma),2))\n",
    "for ig, g in enumerate(gamma):\n",
    "    newdf1 = df1.iloc[np.ravel(np.where(dat[:,1]==g)),:]\n",
    "    newdat = newdf1.to_numpy()\n",
    "    # newdat = newdf1.loc[np.logical_and(newdf1['al']>1, newdf1['al']<60),:].to_numpy()\n",
    "    sin_onlyage = [np.sum(get_lp_alxl(g1, newdat[:,5], newdat[:,2], cutoff=5)) for g1 in gamma]\n",
    "    preds_sin_onlyage[ig] = gamma[np.argmax(sin_onlyage)]\n",
    "    ci_sin_onlyage[ig] = 1/np.abs(get_bfq(sin_onlyage-np.argmax(sin_onlyage), gamma)[0]) # get_conf_int(sin_onlyage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## get bootstrap std estimates but mle only calculated once\n",
    "preds_sin_onlyfreq, preds_sin_onlyage = np.zeros(len(gamma)), np.zeros(len(gamma))\n",
    "ci_sin_onlyfreq, ci_sin_onlyage = np.zeros((len(gamma),)), np.zeros((len(gamma),))\n",
    "\n",
    "for ig, g in enumerate(gamma):\n",
    "    newdf1 = df1.iloc[np.ravel(np.where(dat[:,1]==g)),:]\n",
    "    newdat = newdf1.to_numpy()\n",
    "    sin_onlyfreq = [np.sum(get_lp_xl(g1, newdat[:,5], cutoff=2)) for g1 in gamma2]\n",
    "    sin_onlyage = [np.sum(get_lp_alxl(g1, newdat[:,5], newdat[:,2], cutoff=2)) for g1 in gamma2]\n",
    "    preds_sin_onlyfreq[ig] = gamma2[np.argmax(sin_onlyfreq)]\n",
    "    preds_sin_onlyage[ig] = gamma2[np.argmax(sin_onlyage)]\n",
    "    mle = get_boot_ci(newdat, nsamps=len(newdat), nboot=5)\n",
    "    ci_sin_onlyfreq[ig,], ci_sin_onlyage[ig,] = mle.std(axis=0) #np.percentile(mle[:,0],[97.5,2.5]), np.percentile(mle[:,1],[97.5,2.5]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## get age estimates but with error introduced into the mix \n",
    "preds_sin_onlyfreq, preds_sin_onlyage = np.zeros(len(gamma)), np.zeros(len(gamma))\n",
    "ci_sin_onlyfreq, ci_sin_onlyage = np.zeros((len(gamma),)), np.zeros((len(gamma),))\n",
    "\n",
    "for ig, g in enumerate(gamma):\n",
    "    newdf1 = df1.iloc[np.ravel(np.where(dat[:,1]==g)),:]\n",
    "    newdat = newdf1.to_numpy()\n",
    "    sin_onlyfreq = [np.sum(get_lp_xl(g1, newdat[:,5], cutoff=2)) for g1 in gamma2]\n",
    "    # sin_onlyage = [np.sum(get_lp_alxl(g1, newdat[:,5], newdat[:,2], cutoff=2)) for g1 in gamma2]\n",
    "    preds_sin_onlyfreq[ig] = gamma2[np.argmax(sin_onlyfreq)]\n",
    "    ests_age = get_mean_sd_werr(newdat, errvar = np.repeat(0,4), nreps=5)\n",
    "    # print(ests_age)\n",
    "    preds_sin_onlyage[ig] = np.mean(ests_age)\n",
    "    ci_sin_onlyage[ig] = np.std(ests_age)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# newdf1 = df1.iloc[np.ravel(np.where(dat[:,1]==gamma[-10])),:]\n",
    "# newdat = newdf1.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## predictions with data from neutral case only \n",
    "dfneut = pd.read_csv(\"outfiles/ConstantSize0.0.1.full_out.txt\",sep='\\t',header=None,)\n",
    "dfneut.columns = ['','Xl','s','al','id']\n",
    "dfneut['empty'] = ''\n",
    "dfneut['sXl'] = (dfneut['Xl']*2000).astype('int')\n",
    "dfneut['al'] = 80000+1 - dfneut['al']\n",
    "dfneut['al'] = dfneut['al'].astype('int')\n",
    "dfneut = dfneut.iloc[:,1:]\n",
    "datneut = dfneut.to_numpy()\n",
    "dfneut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shufidx = np.arange(len(datneut))\n",
    "rng.shuffle(shufidx)\n",
    "parestfreq0 = np.zeros(11)\n",
    "parestage0 = np.zeros(11)\n",
    "for i in range(11):    \n",
    "    parestfreq0[i] = sp.optimize.minimize_scalar(get_ll_freq, args=(datneut[shufidx[(i*962):((i+1)*962)],5]), options={'xtol': .1,}).x\n",
    "    parestage0[i] = sp.optimize.minimize_scalar(get_ll_freqage, args=(datneut[shufidx[(i*962):((i+1)*962)],5], datneut[shufidx[(i*962):((i+1)*962)],2]), options={'xtol': .1}).x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parestfreq0 = np.array([-0.239, 0.230, -0.841, 0.284, 0.088, 0.249, 0.083, -0.115, -0.004, 0.031, -0.003])\n",
    "parestage0 = np.array([0.027, 0.538, -0.618, 0.680, 0.420, 0.547, 0.420, 0.187, 0.317, 0.382, 0.277])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dfhund = pd.read_csv(\"outfiles/ConstantSize100.0.1.full_out.txt\",sep='\\t',header=None,).sample(n=2000)\n",
    "# dfhund.columns = ['','Xl','s','al','id']\n",
    "# dfhund['empty'] = ''\n",
    "# dfhund['sXl'] = (dfhund['Xl']*2000).astype('int')\n",
    "# dfhund['al'] = 100000+1 - dfhund['al']\n",
    "# dfhund['al'] = dfhund['al'].astype('int')\n",
    "# dfhund = dfhund.iloc[:,1:]\n",
    "# dathund = dfhund.to_numpy()\n",
    "# dfhund"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.hist(dfhund['al'],alpha=0.8); plt.hist(dfneut['al'],alpha=0.4); plt.semilogx()\n",
    "plt.plot(p_xa_s[gamma2[-5]],'o'); plt.plot(p_xa_s[0.],'o',alpha=0.8); plt.loglog()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sin_onlyfreq = [np.sum(get_lp_xl(g1, datneut[:,5], cutoff=2)) for g1 in gamma2]\n",
    "# sin_onlyage = [np.sum(get_lp_alxl(g1, datneut[:,5], datneut[:,2], cutoff=2)) for g1 in gamma2]\n",
    "# print(gamma2[np.argmax(sin_onlyfreq)], gamma2[np.argmax(sin_onlyage)])\n",
    "mle = get_boot_ci(datneut, nsamps=len(datneut), nboot=100)\n",
    "# mle.std(axis=0)\n",
    "\n",
    "# plt.scatter(np.repeat(0.001,len(mle)),mle[:,0],alpha=0.5,color='deepskyblue',label='only freq'); plt.ylabel('estimated γ')\n",
    "# plt.scatter(np.repeat(-.001,len(mle)),mle[:,1],alpha=0.5,color='coral',label='freq & age'); plt.xlabel('true γ'); #plt.semilogy()\n",
    "# plt.legend(); plt.grid()\n",
    "# np.corrcoef(mle[:,0],mle[:,1])\n",
    "# mle.mean(axis=0), mle.std(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## get bootstrap std estimates but mle only calculated once\n",
    "preds_sin_onlyfreq, preds_sin_onlyage = np.zeros(len(s)), np.zeros(len(s))\n",
    "ci_sin_onlyfreq, ci_sin_onlyage = np.zeros((len(s),)), np.zeros((len(s),))\n",
    "\n",
    "for ig, g in enumerate(s):\n",
    "    newdf1 = df1.iloc[np.ravel(np.where(dat[:,1]==g)),:]\n",
    "    newdat = newdf1.to_numpy()\n",
    "    sin_onlyfreq = [np.sum(get_lp_xl(g1, newdat[:,5], cutoff=2)) for g1 in gamma2]\n",
    "    sin_onlyage = [np.sum(get_lp_alxl(g1, newdat[:,5], newdat[:,2], cutoff=2)) for g1 in gamma2]\n",
    "    preds_sin_onlyfreq[ig] = gamma2[np.argmax(sin_onlyfreq)]\n",
    "    preds_sin_onlyage[ig] = gamma2[np.argmax(sin_onlyage)]\n",
    "    mle = get_boot_ci(newdat, nsamps=len(newdat), nboot=50)\n",
    "    ci_sin_onlyfreq[ig,], ci_sin_onlyage[ig,] = mle.std(axis=0) #np.percentile(mle[:,0],[97.5,2.5]), np.percentile(mle[:,1],[97.5,2.5]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# newdf1 = df1.iloc[np.ravel(np.where(dat[:,1]==s[4])),:]\n",
    "# newdat = newdf1.to_numpy()\n",
    "# sin_onlyfreq = [np.sum(get_lp_xl(g1, newdat[:,5], cutoff=2)) for g1 in gamma2]\n",
    "# sin_onlyage = [np.sum(get_lp_alxl(g1, newdat[:,5], newdat[:,2], cutoff=2)) for g1 in gamma2]\n",
    "# gamma2[np.argmax(sin_onlyfreq)], gamma2[np.argmax(sin_onlyage)]\n",
    "# get_boot_ci(newdat, nsamps=len(newdat), nboot=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## basically, do fitdadi inference for each selection coefficient\n",
    "## cannot do point selection coefficient inference\n",
    "import dadi\n",
    "df = pd.read_csv(\"PReFerSims/outfiles/ConstantSize2.0.1.full_out.txt\", sep=\"\\t\", header=None).sample(n=1000)\n",
    "fs = dadi.Spectrum(np.histogram(fdf.iloc[:, 1] * 2000, bins=2001)[0])\n",
    "def one_epoch_sel(params, ns, pts):\n",
    "    nu, T, gamma = params\n",
    "    xx = dadi.Numerics.default_grid(pts)\n",
    "    phi = dadi.PhiManip.phi_1D(xx, gamma=gamma)\n",
    "    phi = dadi.Integration.one_pop(phi, xx, T, nu, gamma=gamma)\n",
    "    fs = dadi.Spectrum.from_phi(phi, ns, (xx,))\n",
    "    return fs\n",
    "pts_l = [600, 800, 1000]\n",
    "spectra = Selection.spectra(\n",
    "    [1, 2],\n",
    "    np.array([2000]),\n",
    "    one_epoch_sel,\n",
    "    pts_l=pts_l,\n",
    "    int_bounds=[0.1, 100],\n",
    "    Npts=20,\n",
    "    echo=True,\n",
    ")\n",
    "sel_params = [0.1, 200.0]\n",
    "lower_bound = [1e-3, 1e-2]\n",
    "upper_bound = [1, 100.0]\n",
    "p0 = dadi.Misc.perturb_params(sel_params, lower_bound=lower_bound, upper_bound=upper_bound)\n",
    "popt = Selection.optimize_log(\n",
    "    p0,\n",
    "    fs,\n",
    "    spectra.integrate,\n",
    "    Selection.normal_dist,\n",
    "    300,\n",
    "    lower_bound=lower_bound,\n",
    "    upper_bound=upper_bound,\n",
    "    verbose=len(sel_params),\n",
    "    maxiter=30,\n",
    ")\n",
    "spectra.Integrate(popt[1], Selection.normal_dist, 300)\n",
    "spectra.integrate([1.,4.], Selection.normal_dist, 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(-s, -preds_sin_onlyfreq, color='deepskyblue', alpha=0.5, label='only freq, rel. error: {:.1f}'.format(np.sum(np.abs(s-preds_sin_onlyfreq)*100/-s)),)#s=truenumloci*50/1000)\n",
    "plt.errorbar(-s, -preds_sin_onlyfreq, yerr=1.96*ci_sin_onlyfreq[:], color='deepskyblue', alpha=0.5, ls='none')\n",
    "plt.scatter(-s, -preds_sin_onlyage, color='coral', alpha=0.5, label='freq & age, rel. error: {:.1f}'.format(np.sum(np.abs(s-preds_sin_onlyage)*100/-s)),)#s=truenumloci*50/1000)\n",
    "plt.errorbar(-s, -preds_sin_onlyage, yerr=1.96*ci_sin_onlyage[:], color='coral', alpha=0.5, ls='none')\n",
    "plt.xlabel('true -γ value'); plt.ylabel('MLE -γ value'); plt.legend(); plt.loglog()\n",
    "plt.axline((1,1),(100,100),color='grey',ls='--', linewidth=1.); plt.ylim((0.1,1000)); plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(-gamma, -preds_sin_onlyfreq, color='deepskyblue', alpha=0.5, label='only freq, RMSLE: {:.2f}'.format(np.sqrt(np.mean((np.log10(-gamma)-np.log10(-preds_sin_onlyfreq))**2))))#s=truenumloci*50/1000)\n",
    "# plt.scatter(-gamma, -preds_sin_onlyfreq, color='deepskyblue', alpha=0.5, label='only freq, r2: {:.2f}'.format(sp.stats.pearsonr(np.log(-gamma),np.log(-preds_sin_onlyfreq))[0]**2))\n",
    "plt.xticks(fontsize=10); plt.yticks(fontsize=10); \n",
    "# plt.vlines(-gamma, -ci_sin_onlyfreq[:,0], -ci_sin_onlyfreq[:,1], color='deepskyblue', alpha=0.5)\n",
    "plt.errorbar(-gamma, -preds_sin_onlyfreq, yerr=1.96*ci_sin_onlyfreq[:], color='deepskyblue', alpha=0.5, ls='none')\n",
    "plt.errorbar(-gamma, -preds_sin_onlyfreq, yerr=-ci_sin_onlyfreq.T, color='deepskyblue', alpha=0.5, ls='none')\n",
    "plt.scatter(-gamma, -preds_sin_onlyage, color='coral', alpha=0.5, label='freq & age, RMSLE: {:.2f}'.format(np.sqrt(np.mean((np.log10(-gamma)-np.log10(-preds_sin_onlyage))**2))))#s=truenumloci*50/1000)\n",
    "# plt.scatter(-gamma, -preds_sin_onlyage, color='coral', alpha=0.5, label='freq & age, r2: {:.2f}'.format(sp.stats.pearsonr(np.log(-gamma),np.log(-preds_sin_onlyage))[0]**2))\n",
    "# plt.vlines(-gamma, -ci_sin_onlyage[:,0], -ci_sin_onlyage[:,1], color='coral', alpha=0.5)\n",
    "plt.errorbar(-gamma, -preds_sin_onlyage, yerr=1.96*ci_sin_onlyage[:], color='coral', alpha=0.5, ls='none')\n",
    "plt.errorbar(-gamma, -preds_sin_onlyage, yerr=-ci_sin_onlyage.T, color='coral', alpha=0.5, ls='none'); plt.yscale('log')\n",
    "# plt.scatter(-gamma, -preds_sin_agefreq, color='darkgreen', alpha=0.5, label='age & freq, abs. error: {:.1f}'.format(np.sum(np.abs(gamma-preds_sin_agefreq))))\n",
    "plt.xlabel('true |γ| value', fontsize=10); plt.ylabel('MLE |γ| value', fontsize=10); plt.legend(loc='lower right', fontsize=10); plt.loglog()\n",
    "plt.axline((1,1),(100,100),color='grey',ls='--', linewidth=1.); plt.ylim((0.2,200)); plt.grid()\n",
    "# plt.xticks(fontsize=8)\n",
    "# plt.yticks(fontsize=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sp.stats.pearsonr(np.log(-gamma),np.log(-preds_sin_onlyfreq))[0]**2\n",
    "preds_sin_onlyage, preds_sin_onlyfreq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dft = pd.read_csv(\"../traindata/trip-2022-04-05.csv\")\n",
    "# dft['gamma'] = dft['gamma'].round(decimals=2)\n",
    "preds_sin_onlyfreq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## comparing empirical distribution from data of frequency values with likelihood from moments framework\n",
    "# basically take samples from binomial for each rep and then store the values in a vector that keeps getting updated...\n",
    "ir = np.empty((100,1500),dtype=int)\n",
    "for n in range(100): #nreps\n",
    "    ir[n,:] = rng.binomial(n=2000, p=df1[df1['s']==-10.]['Xl'])\n",
    "counts, bins, _ = plt.hist(np.ravel(ir),bins=2000,range=(0,2000),density=True); plt.semilogx(); plt.xlabel('# of allele copies')\n",
    "counts[cutoff:(2000-cutoff+1)] = counts[cutoff:(2000-cutoff+1)]/np.sum(counts[cutoff:(2000-cutoff+1)])\n",
    "counts[0:cutoff] = counts[-cutoff:] = np.repeat(0,cutoff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## numerical integration of likelihood to get analytical expression (from eq 2 in Bustamante et al 2001)\n",
    "def Fanal(gg, i, n):\n",
    "    with np.errstate(divide='ignore'):\n",
    "        return [(1-np.exp(-2*gg*(1-x)))*2/((1-np.exp(-2*gg))*x*(1-x)) * sp.stats.binom.pmf(i,n,x) for x in np.logspace(-8,-0.01,99)]\n",
    "\n",
    "analexp = [sp.integrate.trapezoid(Fanal(-11.625/2,i,2000),np.logspace(-8,-0.01,99)) for i in range(2000)]\n",
    "analexp[cutoff:(2000-cutoff+1)] = analexp[cutoff:(2000-cutoff+1)]/np.sum(analexp[cutoff:(2000-cutoff+1)])\n",
    "analexp[0:cutoff] = analexp[-cutoff:] = np.repeat(0,cutoff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.corrcoef([sp.integrate.trapezoid(Fanal(gamma[19],i,2000),np.linspace(1e-6,1-1e-6,200)) for i in range(20)], [sp.integrate.trapezoid(Fanal(gamma[0],i,2000),np.linspace(1e-6,1-1e-6,200)) for i in range(20)])\n",
    "# Fanal(gamma[19],10,2000), Fanal(gamma[0],10,2000) \n",
    "# np.linspace(1e-6,1-1e-6,20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.scatter(p_xa_s[gamma[15]],p_xa_s[gamma[17]]); plt.loglog(); plt.axline((0.1,0.1),(0,0))\n",
    "# np.allclose(p_xa_s[gamma[19]],p_xa_s[gamma[16]])\n",
    "gamma2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(np.arange(2,100+1),p_xa_s[gamma2[15]][2:101],color='purple',label='moments, error={:.2f}'.format(np.sum(np.abs(p_xa_s[gamma2[15]][2:101]-analexp[2:101]))),alpha=0.5);\n",
    "# plt.scatter(np.arange(2,100+1),testp_xa_s[2:101],color='grey',label='moments2, error={:.2f}'.format(np.sum(np.abs(testp_xa_s[2:101]-analexp[2:101]))),alpha=0.5);\n",
    "# plt.scatter(np.arange(2,100+1),counts[2:101],color='green',label='PReFerSims, error={:.2f}'.format(np.sum(np.abs(counts[2:101]-analexp[2:101]))),alpha=0.5); \n",
    "plt.scatter(np.arange(2,100+1),analexp[2:101],color='lightpink',marker='+',label='analytical expression');  plt.grid(); plt.loglog()\n",
    "plt.ylabel('probability'); plt.title('γ = {}'.format(-10.)); plt.legend(); plt.xlabel('# of allele copies')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## plotting different SFS from moments on the same plot\n",
    "plt.scatter(np.arange(2,100+1),p_xa_s[gamma[0]][2:101],color=cm.get_cmap('Purples',6)(5),alpha=0.6,label='γ={}'.format(gamma[0])); \n",
    "plt.scatter(np.arange(2,100+1),p_xa_s[gamma[10]][2:101],color=cm.get_cmap('Purples',6)(3),label='γ={}'.format(gamma[10])); \n",
    "plt.scatter(np.arange(2,100+1),p_xa_s[gamma[-1]][2:101],color=cm.get_cmap('Purples',6)(1),label='γ={}'.format(gamma[-1])); \n",
    "plt.xlabel('# of allele copies'); plt.ylabel('moments likelihood'); plt.legend(); plt.loglog(); plt.grid()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_sin_onlyfreq, ci_sin_onlyfreq, preds_sin_onlyage, ci_sin_onlyage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(dpi=150)\n",
    "import matplotlib.patches as mpatches\n",
    "seaborn.stripplot(data=pd.DataFrame(info_onlyfreq.T,columns=-gamma[::-1]), color='deepskyblue', alpha=0.7, label='only freq',s=4)\n",
    "seaborn.stripplot(data=pd.DataFrame(info_onlyage.T,columns=-gamma[::-1]), color='coral', alpha=0.7, label='freq & age',s=4)\n",
    "plt.semilogy(); plt.xlabel('gamma'); plt.ylabel('information measure'); plt.grid(); \n",
    "plt.legend(handles=[mpatches.Patch(color='deepskyblue', label='only freq, total info: {:.2f}'.format(np.sum(info_onlyfreq))), mpatches.Patch(color='coral', label='freq & age, total info: {:.2f}'.format(np.sum(info_onlyage)))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## plotting error rates across selection coefficients\n",
    "plt.plot(-gamma, np.abs(gamma-preds_sin_onlyfreq), color='deepskyblue', alpha=0.8, label='only freq'.format(np.sum(np.abs(gamma-preds_sin_onlyfreq))), ls='--')\n",
    "plt.plot(-gamma, np.abs(gamma-preds_sin_onlyage), color='coral', label='freq & age', alpha=0.8, ls='--')\n",
    "plt.legend(); plt.semilogx(); plt.xlabel('-γ'); plt.ylabel('absolute error'); plt.show()\n",
    "\n",
    "plt.plot(-gamma, -np.abs(gamma-preds_sin_onlyfreq)*100/gamma, color='deepskyblue', alpha=0.8, label='only freq, total: {:.2f}'.format(np.sum(-np.abs(gamma-preds_sin_onlyfreq)*100/gamma)), ls='--')\n",
    "plt.plot(-gamma, -np.abs(gamma-preds_sin_onlyage)*100/gamma, color='coral', label='freq & age, total: {:.2f}'.format(np.sum(-np.abs(gamma-preds_sin_onlyage)*100/gamma)), alpha=0.8, ls='--')\n",
    "plt.legend(); plt.semilogx(); plt.xlabel('-γ'); plt.ylabel('relative error'); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## picking a choice of g1 and g2 & creating a new data frame\n",
    "trueg1 = gamma[-4]\n",
    "trueg2 = gamma[-10]\n",
    "# 2000 x 3 (first 1000 is g1, second 1000 is g2)\n",
    "newdf1 = df1.iloc[np.ravel(np.where(dat[:,1]==trueg1)),:].append(df1.iloc[np.ravel(np.where(dat[:,1]==trueg2)),:])\n",
    "# newdf1 = newdf1.sample(frac=1)\n",
    "\n",
    "# only keeping alleles with ages > 1 (cos NN & PRF approx finds really high prob for these alleles to have small gamma...)\n",
    "newdat = newdf1.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## need code to do scipy.optimize for two parameters...\n",
    "def get_ll_freq2(g, sXlred, n=2000, cutoff=2):\n",
    "    g1, g2 = g\n",
    "\n",
    "    fs1 = moments.Spectrum(np.zeros(n+1))\n",
    "    fs1[1] = 1\n",
    "    fs1.integrate([0.5], 10, gamma=g1)\n",
    "    pxs1 = fs1/np.sum(fs1[np.arange(cutoff,n-cutoff+1)])\n",
    "\n",
    "    fs2 = moments.Spectrum(np.zeros(n+1))\n",
    "    fs2[1] = 1\n",
    "    fs2.integrate([0.5], 10, gamma=g2)\n",
    "    pxs2 = fs2/np.sum(fs2[np.arange(cutoff,n-cutoff+1)])\n",
    "\n",
    "    res = np.empty(np.sum((sXlred>cutoff) & (sXlred<n-cutoff+1))) #np.empty(len(Xlred))\n",
    "\n",
    "    # just performing a search in a look-up table\n",
    "    for idx, i in enumerate(np.where((sXlred>cutoff) & (sXlred<n-cutoff+1))[0]):\n",
    "        res[idx] = np.log(0.5*pxs1[sXlred[i]]+0.5*pxs2[sXlred[i]])\n",
    "    \n",
    "    return -np.sum(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ll_freqage2(g, sXlred, alred, n=2000, cutoff=2):\n",
    "    g1, g2 = g\n",
    "\n",
    "    pxas1 = run_mom_iterate(int(100000+900*g1), n, 0.5*g1/N, N, 1.25e-8, misc = {'dt_fac':0.02, 'adapt_dt':True})\n",
    "    pxas1[:,np.arange(cutoff,n-cutoff+1)] /= np.sum(pxas1[:,np.arange(cutoff,n-cutoff+1)]) \n",
    "\n",
    "    pxas2 = run_mom_iterate(int(100000+900*g2), n, 0.5*g2/N, N, 1.25e-8, misc = {'dt_fac':0.02, 'adapt_dt':True})\n",
    "    pxas2[:,np.arange(cutoff,n-cutoff+1)] /= np.sum(pxas2[:,np.arange(cutoff,n-cutoff+1)]) \n",
    "\n",
    "    res = np.empty(np.sum((sXlred>cutoff) & (sXlred<n-cutoff+1)))\n",
    "    \n",
    "    for idx, i in enumerate(np.where((sXlred>cutoff) & (sXlred<n-cutoff+1))[0]):\n",
    "        res[idx] = np.log(0.5*pxas1[-int(alred[i]),sXlred[i]] + 0.5*pxas2[-int(alred[i]),sXlred[i]])\n",
    "\n",
    "    return -np.sum(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.sum((newdat[:,5]>cutoff) & (newdat[:,5]<2000-cutoff+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0.5*pxs1[newdat[10,5]] + 0.5*pxs2[newdat[10,5]]\n",
    "# trueg1, trueg2, N, 41843.793\n",
    "# sp.optimize.minimize(get_ll_freqage2, x0=[trueg1, trueg2], method='L-BFGS-B', args=(newdat[:,5], newdat[:,2]), options={'maxiter':5, 'ftol':1.}, bounds=[(-200,-0.2),(-200,-0.2)])\n",
    "np.sum((trueg1+24.772)**2+(trueg2+11.16)**2)/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sp.optimize.basinhopping(get_ll_freq2, x0=[-10,-10], minimizer_kwargs={'method': 'L-BFGS-B', 'args': (newdat[:,5]), 'tol': (1., 1.)})\n",
    "sp.optimize.minimize(get_ll_freq2, x0=[-1,-10], method='L-BFGS-B', args=(newdat[:,5]), options={'maxiter':50, 'ftol':1e-2,}, bounds=[(-200,-0.2),(-200,-0.2)])\n",
    "# Nelder-Mead: 'xatol':0.25, 'fatol': 1., takes ~1min/iter and progresses slowly (need to initialize close to true for faster convergence)\n",
    "# sp.optimize.minimize(get_ll_freqage2, x0=[trueg1+1, trueg2+1], method='TNC', args=(newdat[:,5], newdat[:,2]), options={'eps':1e-2,'ftol':1,'xtol':1e-2,'gtol':1e-2,},)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## assuming two selection coefficients and only freq info (split 50/50)\n",
    "dub_onlyfreq = np.zeros((len(gamma2),len(gamma2))) # need a 2d search\n",
    "# for ig1, g1 in enumerate(gamma2):\n",
    "#     for ig2, g2 in enumerate(gamma2[0:(ig1+1)]):\n",
    "#         dub_onlyfreq[ig1, ig2] = np.sum(np.log(0.5*np.exp(get_lp_xl(g1, newdat[:,5])) + 0.5*np.exp(get_lp_xl(g2, newdat[:,5]))))\n",
    "\n",
    "mask = np.full(dub_onlyfreq.shape,False)\n",
    "mask[np.triu_indices_from(dub_onlyfreq,k=1)] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seaborn.heatmap(dub_onlyfreq,alpha=0.95,cmap='viridis_r',mask=mask,vmax=np.max(dub_onlyfreq[~mask])-5,vmin=np.max(dub_onlyfreq[~mask]))\n",
    "plt.xticks(np.linspace(0,24,6,dtype='int'),gamma2[np.linspace(0,24,6,dtype='int')].round(2))\n",
    "plt.yticks(np.linspace(0,24,6,dtype='int'),gamma2[np.linspace(0,24,6,dtype='int')].round(2))\n",
    "np.take(gamma, np.unravel_index(np.nanargmax(np.ma.masked_array(dub_onlyfreq, mask)),dub_onlyfreq.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## assuming two selection coefficients and only age info (split 50/50)\n",
    "dub_onlyage = np.zeros((len(gamma2),len(gamma2))) # need a 2d search\n",
    "for ig1, g1 in enumerate(gamma2):\n",
    "    for ig2, g2 in enumerate(gamma2[0:(ig1+1)]):\n",
    "        dub_onlyage[ig1, ig2] = np.sum(np.log(0.5*np.exp(get_lp_alxl(g1, newdat[:,5], newdat[:,2])) + \n",
    "        0.5*np.exp(get_lp_alxl(g2, newdat[:,5], newdat[:,2]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seaborn.heatmap(dub_onlyage,alpha=0.75,xticklabels=gamma,yticklabels=gamma,cmap='Blues',mask=mask,vmin=np.max(dub_onlyage[~mask])-10,vmax=np.max(dub_onlyage[~mask]))\n",
    "plt.xticks(np.linspace(0,24,6,dtype='int'),gamma2[np.linspace(0,24,6,dtype='int')].round(2))\n",
    "plt.yticks(np.linspace(0,24,6,dtype='int'),gamma2[np.linspace(0,24,6,dtype='int')].round(2))\n",
    "np.take(gamma, np.unravel_index(np.argmax(np.ma.masked_array(dub_onlyage, mask)),dub_onlyfreq.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unravel_index(np.argmax(np.ma.masked_array(dub_onlyage, mask)),dub_onlyage.shape)\n",
    "np.unravel_index(np.nanargmax(np.ma.masked_array(dub_onlyfreq, mask)),dub_onlyfreq.shape)\n",
    "# print(np.max(dub_onlyfreq[~mask]))\n",
    "# print(np.max(dub_onlyage[~mask]))\n",
    "# dub_onlyfreq[24,]\n",
    "# dub_onlyage[21,]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What if there was only one $\\gamma$ in the data? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samepredsonlyfreq = np.zeros((len(gamma),4))\n",
    "samepredsagefreq = np.zeros((len(gamma),4))\n",
    "for itg, trueg in enumerate(gamma):\n",
    "    newdf1 = df1.iloc[np.ravel(np.where(dat[:,1]==trueg)),:]\n",
    "\n",
    "    newdat = newdf1.to_numpy()\n",
    "    \n",
    "    sin_onlyfreq = np.zeros(len(gamma))\n",
    "    sin_agefreq = np.zeros(len(gamma))\n",
    "    \n",
    "    dub_onlyfreq = np.zeros((len(gamma),len(gamma))) # need a 2d search\n",
    "    dub_agefreq = np.zeros((len(gamma),len(gamma))) # need a 2d search\n",
    "\n",
    "    for ig1, g1 in enumerate(gamma):\n",
    "        sin_onlyfreq[ig1] = np.sum(get_lp_xl(g1, newdat[:,5]))\n",
    "        # sin_agefreq[ig1] = np.sum(get_lp_alxl(g1, newdat[:,0], newdat[:,2], n=100) + get_lp_xl(g1, newdat[:,0]))\n",
    "        sin_agefreq[ig1] = np.sum(get_lp_alxl(g1, newdat[:,5], newdat[:,2]))\n",
    "\n",
    "        for ig2, g2 in enumerate(gamma[0:(ig1+1)]):        \n",
    "            dub_onlyfreq[ig1, ig2] = np.sum(np.log(0.5*np.exp(get_lp_xl(g1, newdat[:,5])) + 0.5*np.exp(get_lp_xl(g2, newdat[:,5]))))\n",
    "    \n",
    "            # dub_agefreq[ig1, ig2] = np.sum(np.log(0.5*np.exp(get_lp_alxl(g1, newdat[:,0], newdat[:,2], n=100) + get_lp_xl(g1, newdat[:,0])) + 0.5*np.exp(get_lp_alxl(g2, newdat[:,0], newdat[:,2], n=100) + get_lp_xl(g2, newdat[:,0]))))\n",
    "            \n",
    "            dub_agefreq[ig1, ig2] = np.sum(np.log(0.5*np.exp(get_lp_alxl(g1, newdat[:,5], newdat[:,2])) + 0.5*np.exp(get_lp_alxl(g2, newdat[:,5], newdat[:,2]))))\n",
    "\n",
    "    estgonlyfreq = gamma[np.argmax(sin_onlyfreq)]\n",
    "\n",
    "    estg1onlyfreq = gamma[np.unravel_index(dub_onlyfreq.argmax(), dub_onlyfreq.shape)[0]]\n",
    "    estg2onlyfreq = gamma[np.unravel_index(dub_onlyfreq.argmax(), dub_onlyfreq.shape)[1]]\n",
    "\n",
    "    lambfreq = 2.*(np.max(dub_onlyfreq[~mask]) - np.max(sin_onlyfreq))\n",
    "\n",
    "    samepredsonlyfreq[itg,2] = estgonlyfreq\n",
    "\n",
    "    samepredsonlyfreq[itg,:2] = np.take(gamma, np.unravel_index(np.nanargmax(np.ma.masked_array(dub_onlyfreq, mask)),dub_onlyfreq.shape))\n",
    "\n",
    "    if(chi2.sf(lambfreq, 1)<0.05):\n",
    "        samepredsonlyfreq[itg,3] = True\n",
    "    else:\n",
    "        samepredsonlyfreq[itg,3] = False\n",
    "\n",
    "    estgagefreq = gamma[np.nanargmax(sin_agefreq)]        \n",
    "\n",
    "    estg1agefreq = np.take(gamma, np.unravel_index(np.nanargmax(np.ma.masked_array(dub_agefreq, mask)),dub_agefreq.shape))[0]\n",
    "    estg2agefreq = np.take(gamma, np.unravel_index(np.nanargmax(np.ma.masked_array(dub_agefreq, mask)),dub_agefreq.shape))[1]\n",
    "\n",
    "    lambagefreq = 2.*(np.nanmax(dub_agefreq[~mask]) - np.nanmax(sin_agefreq))\n",
    "\n",
    "    samepredsagefreq[itg,2] = estgagefreq\n",
    "\n",
    "    samepredsagefreq[itg,:2] = np.take(gamma, np.unravel_index(np.nanargmax(np.ma.masked_array(dub_agefreq, mask)),dub_agefreq.shape))\n",
    "\n",
    "    if(chi2.sf(lambagefreq, 1)<0.05):\n",
    "        samepredsagefreq[itg,3] = True\n",
    "    else:\n",
    "        samepredsagefreq[itg,3] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.max(dub_onlyfreq) - np.max(sin_onlyfreq)\n",
    "samepredsagefreq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## plotting the above results\n",
    "plt.scatter(-gamma, -samepredsonlyfreq[:,0], color='deepskyblue', marker='+', alpha=0.5)\n",
    "plt.scatter(-gamma, -samepredsonlyfreq[:,1], color='deepskyblue', marker='+', label='estimates using only freq', alpha=0.5)\n",
    "plt.scatter(-gamma, -samepredsagefreq[:,0], color='coral', marker='*', label='estimates using freq & age', alpha=0.5)\n",
    "plt.scatter(-gamma, -samepredsagefreq[:,1], color='coral', marker='*', alpha=0.5)\n",
    "plt.xlabel('true -γ value'); plt.ylabel('predicted -γ value(s)'); plt.loglog()\n",
    "plt.axline((0.01,0.01),(100,100),color='grey',ls='--'); plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Template code to compute difference in $\\gamma$ given set of data\n",
    "\n",
    "Here, I will run the mechanism to compute the probability of a significant difference in selection coefficients is detected given a set of large data i.e., 2000 data points and I will resample 25 times to obtain smaller datasets of 500. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_sims is number of reps to run to calculate prob\n",
    "# num_samps is number of rows to resample the big data from\n",
    "# gamma is np.array of values to calculate over\n",
    "# thresh is threshold to assign significance\n",
    "def resample_calculateprob_freq(newdat, gamma, num_sims=16, num_samps=500, thresh=0.05, cutoff=cutoff):\n",
    "    prob = 0.\n",
    "    sin_onlyfreq = np.empty(len(gamma))\n",
    "    dub_onlyfreq = np.zeros((len(gamma),len(gamma)))\n",
    "    for n in np.arange(num_sims):\n",
    "        newnewdat = newdat[np.random.choice(newdat.shape[0], num_samps, replace=False),:]\n",
    "        for ig, g in enumerate(gamma):\n",
    "            # sum log prob for each locus\n",
    "            sin_onlyfreq[ig] = np.sum(get_lp_xl(g, newnewdat[:,5], cutoff=cutoff))\n",
    "            for ig2, g2 in enumerate(gamma[0:(ig+1)]):\n",
    "                dub_onlyfreq[ig, ig2] = np.sum(np.log(0.5*np.exp(get_lp_xl(g, newnewdat[:,5], cutoff=cutoff)) + 0.5*np.exp(get_lp_xl(g2, newnewdat[:,5], cutoff=cutoff))))\n",
    "\n",
    "\n",
    "        estgonlyfreq = gamma[np.argmax(sin_onlyfreq)]\n",
    "\n",
    "        # estg1onlyfreq = gamma[np.unravel_index(dub_onlyfreq.argmax(), dub_onlyfreq.shape)[0]]\n",
    "        # estg2onlyfreq = gamma[np.unravel_index(dub_onlyfreq.argmax(), dub_onlyfreq.shape)[1]]\n",
    "        estg1onlyfreq, estg2onlyfreq = np.take(gamma, np.unravel_index(np.argmax(np.ma.masked_array(dub_onlyfreq, mask)),dub_onlyfreq.shape))\n",
    "\n",
    "        lambfreq = 2.*(dub_onlyfreq[gamma==estg1onlyfreq,gamma==estg2onlyfreq] - sin_onlyfreq[gamma==estgonlyfreq])\n",
    "\n",
    "        if(chi2.sf(lambfreq, 1)<thresh):\n",
    "            prob += 1.\n",
    "\n",
    "    return [prob/num_sims, estgonlyfreq, np.array([estg1onlyfreq, estg2onlyfreq])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## function call to find MLE using scipy.optimize (and corresponding changes in the next steps for LRT)\n",
    "def resample_calculateprob_freqcont(newdat, gamma, num_sims=16, num_samps=500, thresh=0.05, cutoff=2):\n",
    "    prob = 0.\n",
    "    sin_onlyfreq = np.empty(len(gamma))\n",
    "    dub_onlyfreq = np.zeros((len(gamma),len(gamma)))\n",
    "    for n in np.arange(num_sims):\n",
    "        newnewdat = newdat[np.random.choice(newdat.shape[0], num_samps, replace=False),:]\n",
    "        ressin = sp.optimize.minimize_scalar(get_ll_freq, args=(newnewdat[:,5]), options={'xtol': .05,})\n",
    "        resdub = sp.optimize.minimize(get_ll_freq2, x0=[-1,-10], method='L-BFGS-B', args=(newnewdat[:,5]), options={'maxiter':50, 'ftol':1e-2,}, bounds=[(-200,-0.2),(-200,-0.2)])\n",
    "\n",
    "        estgonlyfreq = ressin.x\n",
    "\n",
    "        estg1onlyfreq, estg2onlyfreq = resdub.x[0], resdub.x[1]\n",
    "\n",
    "        lambfreq = 2.*(resdub.fun - ressin.fun)\n",
    "\n",
    "        if(chi2.sf(lambfreq, 1)<thresh):\n",
    "            prob += 1.\n",
    "\n",
    "    return [prob/num_sims, ressin.x, resdub.x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_samps is number of rows to resample the big data from\n",
    "# gamma is np.array of values to calculate over\n",
    "# thresh is threshold to assign significance\n",
    "def resample_calculateprob_agefreq(newdat, gamma, num_sims=16, num_samps=500, thresh=0.05):\n",
    "    prob = 0.\n",
    "\n",
    "    sin_agefreq = np.empty(len(gamma))\n",
    "    \n",
    "    dub_agefreq = np.zeros((len(gamma),len(gamma)))\n",
    "    for n in np.arange(num_sims):\n",
    "        newnewdat = newdat[np.random.choice(newdat.shape[0], num_samps, replace=False),:]\n",
    "        for ig, g in enumerate(gamma):\n",
    "            # sum log prob for each locus\n",
    "            sin_agefreq[ig] = np.sum(get_lp_alxl(g, newnewdat[:,0], newnewdat[:,2], n=100) + get_lp_xl(g, newnewdat[:,0]))\n",
    "            for ig2, g2 in enumerate(gamma[0:(ig+1)]):\n",
    "                dub_agefreq[ig, ig2] = np.sum(np.log(0.5*np.exp(get_lp_alxl(g, newnewdat[:,5], newnewdat[:,2]) + get_lp_xl(g, newnewdat[:,5])) + 0.5*np.exp(get_lp_alxl(g2, newnewdat[:,5], newnewdat[:,2]) + get_lp_xl(g2, newnewdat[:,5]))))\n",
    "\n",
    "        estgagefreq = gamma[np.nanargmax(sin_agefreq)]        \n",
    "\n",
    "        estg1agefreq = gamma[np.unravel_index(np.nanargmax(dub_agefreq[~mask]), dub_agefreq.shape)[0]]\n",
    "        estg2agefreq = gamma[np.unravel_index(np.nanargmax(dub_agefreq[~mask]), dub_agefreq.shape)[1]]\n",
    "\n",
    "        lambagefreq = 2.*(dub_agefreq[gamma==estg1agefreq,gamma==estg2agefreq] - sin_agefreq[gamma==estgagefreq])\n",
    "\n",
    "        if(chi2.sf(lambagefreq, 1)<thresh):\n",
    "            prob += 1.\n",
    "\n",
    "    return prob/num_sims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_samps is number of rows to resample the big data from\n",
    "# gamma is np.array of values to calculate over\n",
    "# thresh is threshold to assign significance\n",
    "def resample_calculateprob_age(newdat, gamma, num_sims=16, num_samps=500, thresh=0.05, cutoff=cutoff):\n",
    "    prob = 0.\n",
    "\n",
    "    sin_onlyage = np.empty(len(gamma))\n",
    "    \n",
    "    dub_onlyage = np.zeros((len(gamma),len(gamma)))\n",
    "    for n in np.arange(num_sims):\n",
    "        newnewdat = newdat[np.random.choice(newdat.shape[0], num_samps, replace=False),:]\n",
    "        for ig, g in enumerate(gamma):\n",
    "            # sum log prob for each locus\n",
    "            sin_onlyage[ig] = np.sum(get_lp_alxl(g, newnewdat[:,5], newnewdat[:,2], cutoff=cutoff))\n",
    "            for ig2, g2 in enumerate(gamma[0:(ig+1)]):\n",
    "                dub_onlyage[ig, ig2] = np.sum(np.log(0.5*np.exp(get_lp_alxl(g, newnewdat[:,5], newnewdat[:,2], cutoff=cutoff)) + 0.5*np.exp(get_lp_alxl(g2, newnewdat[:,5], newnewdat[:,2], cutoff=cutoff))))\n",
    "\n",
    "        estgonlyage = gamma[np.argmax(sin_onlyage)]        \n",
    "\n",
    "        # estg1onlyage = gamma[np.unravel_index(dub_onlyage.argmax(), dub_onlyage.shape)[0]]\n",
    "        # estg2onlyage = gamma[np.unravel_index(dub_onlyage.argmax(), dub_onlyage.shape)[1]]\n",
    "        estg1onlyage, estg2onlyage = np.take(gamma, np.unravel_index(np.argmax(np.ma.masked_array(dub_onlyage, mask)),dub_onlyage.shape))\n",
    "\n",
    "        lambonlyage = 2.*(dub_onlyage[gamma==estg1onlyage,gamma==estg2onlyage] - sin_onlyage[gamma==estgonlyage])\n",
    "\n",
    "        if(chi2.sf(lambonlyage, 1)<thresh):\n",
    "            prob += 1.\n",
    "\n",
    "    return [prob/num_sims, estgonlyage, np.array([estg1onlyage, estg2onlyage])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diffgamma = np.reshape(list(it.combinations(gamma[::3], 2)), (-1,2)) # np.reshape([(-1., x) for x in gamma[:-1][::2]], (-1,2)) #\n",
    "diffprobs = np.zeros((len(diffgamma),2))\n",
    "sinpreds = np.zeros((len(diffgamma),2))\n",
    "dubpreds = np.zeros((len(diffgamma),4))\n",
    "for ig, g in enumerate(diffgamma):\n",
    "    newdf1 = df2.iloc[np.ravel(np.where(dat2[:,1]==g[0])),:].append(df2.iloc[np.ravel(np.where(dat2[:,1]==g[1])),:])\n",
    "    # newdf1 = newdf1.sample(frac=1)\n",
    "    newdat = newdf1.to_numpy()\n",
    "    ## continuous pred doesn't do great, worse than using grid! \n",
    "    diffprobs[ig,0], sinpreds[ig,0], dubpreds[ig,:2] = resample_calculateprob_freq(newdat, gamma2, num_sims=20, num_samps=1000, cutoff=2)\n",
    "    # diffprobs[ig,1], sinpreds[ig,1], dubpreds[ig,-2:] = resample_calculateprob_age(newdat, gamma2, num_sims=20, num_samps=1000, cutoff=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Need a way to plot what the predictions are too..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dubpredsfsame, dubpredsasame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(range(1,len(diffgamma)+1),diffgamma[:,0],marker='+',color='k',alpha=0.5); plt.scatter(range(1,len(diffgamma)+1),diffgamma[:,1],marker='+',color='k',label='True values'); #plt.yscale('symlog')\n",
    "plt.scatter(range(1,len(diffgamma)+1),sinpreds[:,0],color='deepskyblue',alpha=0.6,label='Predicted value (freq only)'); \n",
    "plt.scatter(range(1,len(diffgamma)+1),sinpreds[:,1],color='coral',alpha=0.6,label='Predicted value (freq & age)'); \n",
    "plt.plot([range(1,len(diffgamma)+1),range(1,len(diffgamma)+1)],[diffgamma[:,0],diffgamma[:,1]],color='k',alpha=0.4,linewidth=3); \n",
    "plt.legend(); plt.xlabel('replicate data set'); plt.ylabel(r'$\\gamma_{1,2}$'); plt.grid(); plt.xticks(ticks=range(1,22))\n",
    "# plt.savefig(\"../figs/twoparam1pred.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.hist(newdf1['al'].iloc[:1500],np.logspace(0,3)); plt.hist(newdf1['al'].iloc[1500:],np.logspace(0,3),alpha=0.5)\n",
    "# np.reshape(list(it.combinations(gamma[1::3], 2)), (-1,2))\n",
    "plt.scatter(diffgamma[:,0], diffgamma[:,1], marker='+', color='k', alpha=0.8, label='True values'); plt.yscale('symlog'); plt.xscale('symlog'); plt.grid()\n",
    "plt.scatter(np.unique(diffgamma), np.unique(diffgamma), marker='+', color='k', alpha=0.8); \n",
    "frmse = np.sqrt(np.mean(np.sum((dubpreds[:,0]-diffgamma[:,0])**2+(dubpreds[:,1]-diffgamma[:,1])**2)))#+np.mean(np.sum((dubpredsfsame[:,0]-np.unique(diffgamma))**2+(dubpredsfsame[:,1]-np.unique(diffgamma))**2)))\n",
    "plt.scatter(dubpreds[:,0], dubpreds[:,1], color='deepskyblue', alpha=0.4, label='Predicted values (freq only), RMSE: {:.0f}'.format(frmse)); \n",
    "# plt.scatter(dubpredsfsame[:,0], dubpredsfsame[:,1], color='deepskyblue', alpha=0.4); plt.scatter(dubpredsasame[:,0], dubpredsasame[:,1], color='coral', alpha=0.4);\n",
    "# armse = np.sqrt(np.mean(np.sum((dubpreds[:,2]-diffgamma[:,0])**2+(dubpreds[:,3]-diffgamma[:,1])**2))+np.mean(np.sum((dubpredsasame[:,0]-np.unique(diffgamma))**2+(dubpredsasame[:,1]-np.unique(diffgamma))**2)))\n",
    "# plt.scatter(dubpreds[:,2], dubpreds[:,3], color='coral', alpha=0.4, label='Predicted values (freq & age), RMSE: {:.0f}'.format(armse))\n",
    "plt.plot([diffgamma[:,0],dubpreds[:,0]],[diffgamma[:,1],dubpreds[:,1]],color='deepskyblue',alpha=0.4); plt.xlabel(r'$\\gamma_1$'); plt.ylabel(r'$\\gamma_2$')\n",
    "# plt.plot([diffgamma[:,0],dubpreds[:,2]],[diffgamma[:,1],dubpreds[:,3]],color='coral',alpha=0.4); plt.legend()\n",
    "# plt.plot([np.unique(diffgamma),dubpredsfsame[:,0]],[np.unique(diffgamma),dubpredsfsame[:,1]],color='deepskyblue',alpha=0.4)\n",
    "# plt.plot([np.unique(diffgamma),dubpredsasame[:,0]],[np.unique(diffgamma),dubpredsasame[:,1]],color='coral',alpha=0.4)\n",
    "plt.savefig(\"../figs/twoparampreds.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# newdf1 = df1.iloc[np.ravel(np.where(dat[:,1]==s[5])),:].append(df1.iloc[np.ravel(np.where(dat[:,1]==s[4])),:])\n",
    "# newdat = newdf1.to_numpy()\n",
    "# print(resample_calculateprob_freq(newdat, gamma2, num_sims=16, num_samps=1200, cutoff=2))\n",
    "# print(resample_calculateprob_age(newdat, gamma2, num_sims=16, num_samps=1200, cutoff=2))\n",
    "newnewdat = newdat[np.random.choice(newdat.shape[0], 1200, replace=False),:]\n",
    "for ig, g in enumerate(gamma2):\n",
    "    # sum log prob for each locus\n",
    "    sin_onlyage[ig] = np.sum(get_lp_alxl(g, newnewdat[:,5], newnewdat[:,2], cutoff=cutoff))\n",
    "    for ig2, g2 in enumerate(gamma2[0:(ig+1)]):\n",
    "        dub_onlyage[ig, ig2] = np.sum(np.log(0.5*np.exp(get_lp_alxl(g, newnewdat[:,5], newnewdat[:,2], cutoff=cutoff)) + 0.5*np.exp(get_lp_alxl(g2, newnewdat[:,5], newnewdat[:,2], cutoff=cutoff))))\n",
    "\n",
    "print(gamma[np.argmax(sin_onlyage)])\n",
    "print(np.take(gamma, np.unravel_index(np.argmax(np.ma.masked_array(dub_onlyage, mask)),dub_onlyage.shape)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diffgamma = np.reshape(list(it.combinations(s, 2)), (-1,2)) # np.reshape([(-1., x) for x in gamma[:-1][::2]], (-1,2)) #\n",
    "diffprobs = np.zeros((len(diffgamma),2))\n",
    "sinpreds = np.zeros((len(diffgamma),2))\n",
    "dubpreds = np.zeros((len(diffgamma),4))\n",
    "for ig, g in enumerate(diffgamma):\n",
    "    newdf1 = df1.iloc[np.ravel(np.where(dat[:,1]==g[0])),:].append(df1.iloc[np.ravel(np.where(dat[:,1]==g[1])),:])\n",
    "    # newdf1 = newdf1.sample(frac=1)\n",
    "    newdat = newdf1.to_numpy()\n",
    "    diffprobs[ig,0], sinpreds[ig,0], dubpreds[ig,:2] = resample_calculateprob_freq(newdat, gamma2, num_sims=20, num_samps=5000, cutoff=2)\n",
    "    diffprobs[ig,1], sinpreds[ig,1], dubpreds[ig,-2:] = resample_calculateprob_age(newdat, gamma2, num_sims=20, num_samps=5000, cutoff=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## alternative plotting mechanism to highlight the power in discriminating between gamma values\n",
    "powermat_freq = np.empty((len(np.unique(diffgamma)),len(np.unique(diffgamma))))\n",
    "powermat_age = np.empty((len(np.unique(diffgamma)),len(np.unique(diffgamma))))\n",
    "\n",
    "mask_pow = np.full(powermat_freq.shape,False)\n",
    "mask_pow[np.tril_indices_from(powermat_freq,k=-1)] = True\n",
    "\n",
    "for ig, g in enumerate(diffgamma[:,]):\n",
    "    powermat_freq[np.argmax(g[0]==np.unique(diffgamma)), np.argmax(g[1]==np.unique(diffgamma))] = diffprobs[ig, 0]\n",
    "    powermat_age[np.argmax(g[0]==np.unique(diffgamma)), np.argmax(g[1]==np.unique(diffgamma))] = diffprobs[ig, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sinpredsfsame = np.zeros(len(np.unique(diffgamma)))\n",
    "sinpredsasame = np.zeros(len(np.unique(diffgamma)))\n",
    "dubpredsfsame = np.zeros((len(np.unique(diffgamma)),2))\n",
    "dubpredsasame = np.zeros((len(np.unique(diffgamma)),2))\n",
    "for ig, g in enumerate(np.unique(diffgamma)):\n",
    "    newdf1 = df2.iloc[np.ravel(np.where(dat2[:,1]==g)),:]\n",
    "    # newdf1 = newdf1.sample(frac=1)\n",
    "    newdat = newdf1.to_numpy()\n",
    "    powermat_freq[ig,ig], sinpredsfsame[ig], dubpredsfsame[ig,:2] = resample_calculateprob_freq(newdat, gamma2, num_sims=20, num_samps=1000, cutoff=2)\n",
    "    powermat_age[ig,ig], sinpredsasame[ig], dubpredsasame[ig,-2:] = resample_calculateprob_age(newdat, gamma2, num_sims=20, num_samps=1000, cutoff=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newdf1 = df1.iloc[np.ravel(np.where(dat[:,1]==np.unique(diffgamma)[2])),:]\n",
    "newdat = newdf1.to_numpy()\n",
    "print(resample_calculateprob_age(newdat, gamma, num_samps=800, cutoff=10))\n",
    "resample_calculateprob_freq(newdat, gamma, num_samps=800, cutoff=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# newdf1 = df1.iloc[np.ravel(np.where(dat[:,1]==gamma[6])),:].append(df1.iloc[np.ravel(np.where(dat[:,1]==gamma[-1])),:])\n",
    "# newdat = newdf1.to_numpy()\n",
    "# print(resample_calculateprob_freq(newdat, gamma, num_sims=10, num_samps=1000, cutoff=50))\n",
    "# print(resample_calculateprob_age(newdat, gamma, num_sims=10, num_samps=1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter([-np.mean(g) for g in diffgamma], -sinpreds[:,0], color='deepskyblue', marker='+', label='only freq')\n",
    "plt.scatter([-np.mean(g) for g in diffgamma], -sinpreds[:,1], color='coral', marker='*', label='freq & age',)\n",
    "plt.xlabel('mean of true γ'); plt.ylabel('predicted γ'); plt.loglog(); plt.legend() \n",
    "plt.axline((1,1),(100,100),color='grey',ls='--')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter([np.abs(g[1]-g[0]) for g in diffgamma], diffprobs[:,0], color='deepskyblue', alpha=0.7, label='only freq') #s=truenumlocifreq[:-1]/(truenumlocifreq[:-1]+truenumlocifreq[-1])*70.)\n",
    "plt.scatter([np.abs(g[1]-g[0]) for g in diffgamma], diffprobs[:,1], color='coral', alpha=0.7, label='freq & age')\n",
    "ginterp = np.logspace(0,4.5,base=np.exp(1),num=25)\n",
    "fit = sp.interpolate.interp1d([np.abs(g[1]-g[0]) for g in diffgamma], diffprobs[:,0], kind='linear')\n",
    "plt.plot(ginterp, fit(ginterp), '--', color='deepskyblue', alpha=0.7)\n",
    "fit = sp.interpolate.interp1d([np.abs(g[1]-g[0]) for g in diffgamma], diffprobs[:,1], kind='linear')\n",
    "plt.plot(ginterp, fit(ginterp), '--', color='coral', alpha=0.7)\n",
    "plt.ylim((-0.1,1.1)); plt.xscale('log'); plt.xlabel('log(|γ1-γ2|)'); plt.ylabel('prob of choosing complex model'); plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.hstack((diffgamma, dubpreds))\n",
    "# np.argmax(diffgamma[0][1]==np.unique(diffgamma))\n",
    "plt.figure(dpi=250)\n",
    "fig, (ax1, ax2) = plt.subplots(1,2)\n",
    "seaborn.heatmap(powermat_freq, mask=mask_pow, xticklabels=[-100,-50,-25,-10,-5,-2,-1], yticklabels=[-100,-50,-25,-10,-5,-2,-1], cmap='GnBu', ax=ax1, cbar=False, linewidths=0.05, square=True,alpha=0.8,vmin=-0,); ax1.ylabel(r'$\\gamma_1$')\n",
    "ax1.set_title('freq only'); ax2.set_title('freq & age'); \n",
    "im = seaborn.heatmap(powermat_age, mask=mask_pow, xticklabels=[-100,-50,-25,-10,-5,-2,-1], yticklabels=[], cmap='GnBu', square=True, ax=ax2, linewidths=0.05, vmin=-0, cbar=False,alpha=0.8)\n",
    "plt.colorbar(im.get_children()[0], ax = [ax1,ax2],orientation = 'horizontal',shrink=0.6,label='prob. of choosing two parameter model',)\n",
    "# plt.savefig('../figs/twoparamdiff.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "truenumloci = np.zeros(len(gamma))\n",
    "for ig, g in enumerate(gamma):\n",
    "    newdf1 = df1.iloc[np.ravel(np.where(dat[:,1]==g)),:]\n",
    "    sXlred = newdf1['sXl']\n",
    "    truenumloci[ig] = np.sum((sXlred>10) & (sXlred<n-10+1))\n",
    "\n",
    "truenumlocifreq = np.zeros(len(gamma))\n",
    "for ig, g in enumerate(gamma):\n",
    "    newdf1 = df1.iloc[np.ravel(np.where(dat[:,1]==g)),0:3]\n",
    "    truenumlocifreq[ig] = newdf1.to_numpy().shape[0]\n",
    "\n",
    "truenumloci"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automating workflow for simulating point DFE from PReFerSim (instead of using the R approach from before)\n",
    "\n",
    "Here, I will write a python function to run the program with appropriate parameters and read the input into a dataframe after."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"/Users/vivaswatshastry/selCoefEst/PReFerSims\")\n",
    "## creating a small set of gamma for simulating data (testing framework) \n",
    "## gamma = 4Ns (if it has to coincide with the moments framework)\n",
    "# gamma = -np.round(np.logspace(0,2,20),2) \n",
    "# s = -np.array([1.,5.,10.,50.,100.,500.])\n",
    "# for ig, g in enumerate(gamma[1:]):\n",
    "#     with open('simfiles/ParameterFilesConstant.txt',\"r\") as file:\n",
    "#         data = file.readlines()\n",
    "\n",
    "#     data[2] = 'DFEPointSelectionCoefficient: {:f}\\n'.format(-0.25*g/10000)\n",
    "#     data[7] = 'FilePrefix: outfiles/ConstantSize{}\\n'.format(-g)\n",
    "\n",
    "#     with open('simfiles/ParameterFilesConstant.txt', 'w') as file:\n",
    "#         file.writelines(data)\n",
    "        \n",
    "#     os.system(\"GSL_RNG_SEED={} GSL_RNG_TYPE=mrg ../../PReFerSim/PReFerSim simfiles/ParameterFilesConstant.txt 2\".format(rng.integers(100496)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## reading in the data and storing in a data frame\n",
    "nsites = 2000\n",
    "# gamma = np.array([0, -0.01, -.1, -1, -10, -100])\n",
    "df1 = pd.DataFrame(index=range(nsites*len(gamma)),columns=['','Xl','s','al','id'])\n",
    "for ig, g in enumerate(gamma):\n",
    "    # if(g==-100.):\n",
    "    #     df1.iloc[(ig*nsites):(ig+1)*nsites] = pd.read_csv('outfiles/ConstantSize50.0.1.full_out.txt'.format(-g),sep='\\t',header=None).sample(n=nsites)\n",
    "    # elif(g==-1.):\n",
    "    #     df1.iloc[(ig*nsites):(ig+1)*nsites] = pd.read_csv('outfiles/ConstantSize2.0.1.full_out.txt'.format(-g),sep='\\t',header=None).sample(n=nsites)\n",
    "    # else:\n",
    "    if g!=0:\n",
    "        df1.iloc[(ig*nsites):(ig+1)*nsites] = pd.read_csv('outfiles/ConstantSize{}.2.full_out.txt'.format(-g),sep='\\t',header=None).sample(n=nsites)\n",
    "    else:\n",
    "        df1.iloc[(ig*nsites):(ig+1)*nsites] = pd.read_csv('outfiles/ConstantSize{}.2.full_out.txt'.format(g),sep='\\t',header=None).sample(n=nsites)\n",
    "    df1['s'].iloc[(ig*nsites):(ig+1)*nsites] = np.repeat(g,nsites)\n",
    "df1['empty'] = ''\n",
    "df1['sXl'] = (df1['Xl']*2000).astype('int')\n",
    "df1['al'] = 80000+1 - df1['al']\n",
    "df1['al'] = df1['al'].astype('int')\n",
    "df1 = df1.iloc[:,1:]\n",
    "dat = df1.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(df1)):\n",
    "    if(df1['al'][i]<0):\n",
    "        df1.iloc[i,2] = df1.iloc[i,2] + 20000\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [0.5,1,2,5,10,20,50,100,200,500]\n",
    "# np.concatenate((-np.arange(1,5),-np.round(np.logspace(-1,10,25,base=2,),1)))\n",
    "# gamma = -np.logspace(-0.2,3,25,base=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MutationRate: 1000\n",
    "# DFEType: point\n",
    "# DFEPointSelectionCoefficient: 0.005\n",
    "# DemographicHistory: simfiles/ConstantSize.txt \n",
    "# n: 2000\n",
    "# PrintSegSiteInfo: 1\n",
    "# LastGenerationAFSamplingValue: 1\n",
    "# FilePrefix: outfiles/ConstantSize"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "37dbdfb015eb2911072604397bc3ab5127f1ef4d866242904832becb363c8f9b"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('cvae': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
