{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Notebook for building and testing the model     \n",
    "\n",
    "In this notebook, I will build and run an initial test over the first 2,048 rows of the data to ensure proper functionality before deploying on midway2 to train on GPUs. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset, TensorDataset, DataLoader\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cuda setup\n",
    "device = torch.device(\"cuda\")\n",
    "kwargs = {'num_workers': 2, 'pin_memory': True} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 831,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyper params\n",
    "batch_size = 128\n",
    "latent_size = 10\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading in the data\n",
    "dat = np.genfromtxt('traindata/trip-2021-07-28.csv', delimiter=',')[1:,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to subset this in a conditional fashion (i.e., each gamma value, here class c, has 1000 data points)\n",
    "gamma = np.unique(dat[:,1])\n",
    "idx = [np.where(dat[:,1] == gamma[i]) for i in np.arange(len(gamma))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an 80/20 split in each data set\n",
    "temp = [train_test_split(dat[idx[i][0],0], dat[idx[i][0],2], test_size=0.2, random_state=42) for i in np.arange(len(gamma))]\n",
    "\n",
    "# for each gamma value...\n",
    "Xltrain = []\n",
    "Xltest = []\n",
    "altrain = []\n",
    "altest = []\n",
    "for t in np.arange(len(temp)):\n",
    "    Xltrain.append(temp[t][0])\n",
    "    Xltest.append(temp[t][1])\n",
    "    altrain.append(temp[t][2])\n",
    "    altest.append(temp[t][3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 893,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a simple matrix of training data (...x2, ...x1) \n",
    "tr_dat = TensorDataset(torch.tensor(np.vstack((np.hstack((Xltrain[0:5])), np.repeat(gamma[0:5],len(Xltrain[0])))).T), torch.tensor(np.hstack((altrain[0:5]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 900,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_dl = DataLoader(tr_dat, batch_size=128, shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 899,
   "metadata": {},
   "outputs": [],
   "source": [
    "te_dat = TensorDataset(torch.tensor(np.vstack((np.hstack((Xltest[0:5])), np.repeat(gamma[0:5],len(Xltest[0])))).T), torch.tensor(np.hstack((altest[0:5]))))\n",
    "te_dl = DataLoader(te_dat, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 898,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaselineNet(nn.Module):\n",
    "    def __init__(self, hidden1):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Linear(2, hidden1),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden1, 10), \n",
    "            nn.Tanh(),\n",
    "            nn.Linear(10, 1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.double()\n",
    "\n",
    "    def forward(self, xg):\n",
    "        a = self.fc1(xg)\n",
    "        return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 901,
   "metadata": {},
   "outputs": [],
   "source": [
    "bnet = BaselineNet(500)\n",
    "bnet.eval()\n",
    "mse_loss = nn.MSELoss()\n",
    "optimizer = optim.Adagrad(bnet.parameters(), lr=2e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 878,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "800"
      ]
     },
     "metadata": {},
     "execution_count": 878
    }
   ],
   "source": [
    "len(tr_dl.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 903,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[ 1.0000e-02, -4.6416e+01],\n        [ 2.0000e-03, -6.8129e+01],\n        [ 2.6000e-03, -2.1544e+01],\n        [ 1.1200e-02, -2.1544e+01],\n        [ 9.8040e-01, -3.1623e+01],\n        [ 1.9880e-01, -6.8129e+01],\n        [ 3.0000e-03, -4.6416e+01],\n        [ 4.2000e-03, -1.0000e+02],\n        [ 8.2000e-03, -6.8129e+01],\n        [ 6.2620e-01, -4.6416e+01],\n        [ 6.3240e-01, -3.1623e+01],\n        [ 6.1800e-02, -3.1623e+01],\n        [ 1.7020e-01, -2.1544e+01],\n        [ 4.0000e-04, -4.6416e+01],\n        [ 9.2660e-01, -3.1623e+01],\n        [ 4.4800e-02, -1.0000e+02],\n        [ 3.3980e-01, -3.1623e+01],\n        [ 1.3580e-01, -6.8129e+01],\n        [ 5.0600e-01, -3.1623e+01],\n        [ 8.6600e-02, -4.6416e+01],\n        [ 6.6460e-01, -4.6416e+01],\n        [ 1.7800e-02, -1.0000e+02],\n        [ 2.6180e-01, -3.1623e+01],\n        [ 1.8000e-03, -2.1544e+01],\n        [ 1.3600e-02, -4.6416e+01],\n        [ 1.9400e-02, -1.0000e+02],\n        [ 5.6420e-01, -4.6416e+01],\n        [ 4.9440e-01, -2.1544e+01],\n        [ 3.1000e-02, -1.0000e+02],\n        [ 6.0000e-04, -4.6416e+01],\n        [ 5.8000e-02, -4.6416e+01],\n        [ 4.4400e-02, -1.0000e+02],\n        [ 6.0000e-03, -1.0000e+02],\n        [ 1.2240e-01, -2.1544e+01],\n        [ 1.0020e-01, -4.6416e+01],\n        [ 2.6000e-03, -3.1623e+01],\n        [ 2.7620e-01, -1.0000e+02],\n        [ 5.4000e-03, -6.8129e+01],\n        [ 9.7600e-01, -4.6416e+01],\n        [ 5.4000e-03, -1.0000e+02],\n        [ 2.9600e-02, -6.8129e+01],\n        [ 3.0000e-03, -4.6416e+01],\n        [ 3.2800e-02, -3.1623e+01],\n        [ 4.2400e-02, -2.1544e+01],\n        [ 1.4600e-02, -4.6416e+01],\n        [ 1.4000e-03, -1.0000e+02],\n        [ 3.4800e-02, -1.0000e+02],\n        [ 2.8600e-02, -1.0000e+02],\n        [ 7.6320e-01, -3.1623e+01],\n        [ 4.0800e-02, -6.8129e+01],\n        [ 1.2000e-03, -1.0000e+02],\n        [ 8.2000e-03, -2.1544e+01],\n        [ 1.2800e-02, -2.1544e+01],\n        [ 8.6000e-03, -2.1544e+01],\n        [ 6.4520e-01, -2.1544e+01],\n        [ 4.7400e-02, -2.1544e+01],\n        [ 5.0400e-02, -4.6416e+01],\n        [ 4.8000e-03, -3.1623e+01],\n        [ 6.2000e-03, -4.6416e+01],\n        [ 1.3000e-01, -1.0000e+02],\n        [ 3.0000e-03, -2.1544e+01],\n        [ 1.6000e-03, -3.1623e+01],\n        [ 6.0000e-04, -2.1544e+01],\n        [ 4.0340e-01, -4.6416e+01],\n        [ 2.6000e-03, -2.1544e+01],\n        [ 7.2000e-03, -1.0000e+02],\n        [ 8.0000e-03, -4.6416e+01],\n        [ 5.2000e-03, -6.8129e+01],\n        [ 1.0000e-03, -4.6416e+01],\n        [ 9.5800e-02, -2.1544e+01],\n        [ 2.3800e-02, -4.6416e+01],\n        [ 7.4000e-03, -4.6416e+01],\n        [ 2.0000e-03, -3.1623e+01],\n        [ 3.7760e-01, -2.1544e+01],\n        [ 8.0000e-04, -1.0000e+02],\n        [ 3.8000e-03, -1.0000e+02],\n        [ 2.4600e-02, -2.1544e+01],\n        [ 4.0000e-04, -2.1544e+01],\n        [ 2.1000e-02, -6.8129e+01],\n        [ 7.2060e-01, -1.0000e+02],\n        [ 3.4000e-03, -1.0000e+02],\n        [ 1.0000e-03, -4.6416e+01],\n        [ 1.0000e-03, -2.1544e+01],\n        [ 3.7400e-02, -6.8129e+01],\n        [ 7.5200e-02, -6.8129e+01],\n        [ 1.2000e-03, -4.6416e+01],\n        [ 1.3920e-01, -6.8129e+01],\n        [ 1.2400e-02, -4.6416e+01],\n        [ 9.2000e-03, -1.0000e+02],\n        [ 5.8000e-03, -4.6416e+01],\n        [ 4.0600e-02, -1.0000e+02],\n        [ 1.8000e-03, -4.6416e+01],\n        [ 8.8000e-03, -3.1623e+01],\n        [ 1.7520e-01, -4.6416e+01],\n        [ 2.2000e-03, -4.6416e+01],\n        [ 4.4000e-03, -4.6416e+01],\n        [ 5.5200e-02, -2.1544e+01],\n        [ 1.8000e-03, -6.8129e+01],\n        [ 6.4000e-03, -6.8129e+01],\n        [ 4.2000e-03, -4.6416e+01],\n        [ 7.5800e-02, -4.6416e+01],\n        [ 2.7000e-02, -1.0000e+02],\n        [ 1.8000e-03, -6.8129e+01],\n        [ 2.7000e-02, -2.1544e+01],\n        [ 2.4200e-02, -1.0000e+02],\n        [ 4.0000e-04, -6.8129e+01],\n        [ 1.8000e-03, -2.1544e+01],\n        [ 8.7840e-01, -6.8129e+01],\n        [ 1.7920e-01, -2.1544e+01],\n        [ 2.2000e-03, -2.1544e+01],\n        [ 1.0000e-03, -4.6416e+01],\n        [ 7.4600e-02, -3.1623e+01],\n        [ 6.1400e-02, -6.8129e+01],\n        [ 4.8000e-03, -4.6416e+01],\n        [ 6.0000e-04, -6.8129e+01],\n        [ 1.8000e-03, -1.0000e+02],\n        [ 4.6800e-02, -3.1623e+01],\n        [ 2.2000e-02, -4.6416e+01],\n        [ 2.0000e-03, -1.0000e+02],\n        [ 1.4000e-03, -1.0000e+02],\n        [ 8.6000e-03, -2.1544e+01],\n        [ 4.0000e-03, -1.0000e+02],\n        [ 2.2000e-03, -4.6416e+01],\n        [ 8.2000e-03, -6.8129e+01],\n        [ 2.3800e-01, -2.1544e+01],\n        [ 1.1200e-02, -2.1544e+01],\n        [ 2.8000e-03, -3.1623e+01],\n        [ 9.2420e-01, -2.1544e+01]], dtype=torch.float64)\ntensor([ 811.,  333.,  815., 1838., 3652., 1079.,  601.,  304.,  588., 1929.,\n        2851., 1831., 3179.,  140., 3442.,  533., 2510.,  987., 2635., 1240.,\n        1981.,  447., 2288., 1084.,  854.,  427., 1841., 3830.,  509.,  267.,\n        1250.,  536.,  340., 2956., 1269.,  740.,  751.,  453., 2559.,  325.,\n         761.,  600., 1606., 2524.,  956.,  208.,  492.,  494., 3039.,  750.,\n         182., 1625., 1786., 1595., 4245., 2590., 1105.,  979.,  707.,  652.,\n        1230.,  639.,  390., 1679.,  995.,  337.,  819.,  493.,  319., 3123.,\n        1002.,  765.,  689., 3446.,  153.,  286., 2182.,  252.,  645.,  944.,\n         283.,  365.,  810.,  787.,  847.,  436.,  949.,  885.,  410.,  667.,\n         537.,  415., 1258., 1522.,  513.,  689., 2727.,  326.,  500.,  655.,\n        1235.,  506.,  316., 2223.,  475.,   79.,  943., 1521., 3040., 1156.,\n         373., 2109.,  882.,  628.,  154.,  228., 1878.,  955.,  229.,  213.,\n        1850.,  289.,  459.,  517., 3295., 1985.,  909., 5188.],\n       dtype=torch.float64)\ntensor([[2.6837],\n        [2.6838],\n        [2.6835],\n        [2.6835],\n        [2.6837],\n        [2.6838],\n        [2.6837],\n        [2.6838],\n        [2.6838],\n        [2.6837],\n        [2.6837],\n        [2.6836],\n        [2.6835],\n        [2.6837],\n        [2.6837],\n        [2.6838],\n        [2.6837],\n        [2.6838],\n        [2.6837],\n        [2.6837],\n        [2.6837],\n        [2.6838],\n        [2.6837],\n        [2.6835],\n        [2.6837],\n        [2.6838],\n        [2.6837],\n        [2.6836],\n        [2.6838],\n        [2.6837],\n        [2.6837],\n        [2.6838],\n        [2.6838],\n        [2.6835],\n        [2.6837],\n        [2.6836],\n        [2.6838],\n        [2.6838],\n        [2.6838],\n        [2.6838],\n        [2.6838],\n        [2.6837],\n        [2.6836],\n        [2.6835],\n        [2.6837],\n        [2.6838],\n        [2.6838],\n        [2.6838],\n        [2.6837],\n        [2.6838],\n        [2.6838],\n        [2.6835],\n        [2.6835],\n        [2.6835],\n        [2.6836],\n        [2.6835],\n        [2.6837],\n        [2.6836],\n        [2.6837],\n        [2.6838],\n        [2.6835],\n        [2.6836],\n        [2.6835],\n        [2.6837],\n        [2.6835],\n        [2.6838],\n        [2.6837],\n        [2.6838],\n        [2.6837],\n        [2.6835],\n        [2.6837],\n        [2.6837],\n        [2.6836],\n        [2.6836],\n        [2.6838],\n        [2.6838],\n        [2.6835],\n        [2.6835],\n        [2.6838],\n        [2.6838],\n        [2.6838],\n        [2.6837],\n        [2.6835],\n        [2.6838],\n        [2.6838],\n        [2.6837],\n        [2.6838],\n        [2.6837],\n        [2.6838],\n        [2.6837],\n        [2.6838],\n        [2.6837],\n        [2.6836],\n        [2.6837],\n        [2.6837],\n        [2.6837],\n        [2.6835],\n        [2.6838],\n        [2.6838],\n        [2.6837],\n        [2.6837],\n        [2.6838],\n        [2.6838],\n        [2.6835],\n        [2.6838],\n        [2.6838],\n        [2.6835],\n        [2.6838],\n        [2.6835],\n        [2.6835],\n        [2.6837],\n        [2.6837],\n        [2.6838],\n        [2.6837],\n        [2.6838],\n        [2.6838],\n        [2.6836],\n        [2.6837],\n        [2.6838],\n        [2.6838],\n        [2.6835],\n        [2.6838],\n        [2.6837],\n        [2.6838],\n        [2.6835],\n        [2.6835],\n        [2.6836],\n        [2.6836]], dtype=torch.float64, grad_fn=<ReluBackward0>)\ntensor([[ 9.3600e-02, -3.1623e+01],\n        [ 1.8140e-01, -2.1544e+01],\n        [ 8.9000e-02, -6.8129e+01],\n        [ 5.0400e-02, -3.1623e+01],\n        [ 8.0000e-04, -3.1623e+01],\n        [ 5.2660e-01, -3.1623e+01],\n        [ 5.0000e-03, -4.6416e+01],\n        [ 5.0000e-03, -6.8129e+01],\n        [ 9.3000e-02, -1.0000e+02],\n        [ 1.7220e-01, -6.8129e+01],\n        [ 2.0000e-03, -6.8129e+01],\n        [ 3.0000e-03, -6.8129e+01],\n        [ 9.8260e-01, -1.0000e+02],\n        [ 3.6000e-03, -1.0000e+02],\n        [ 2.4000e-03, -1.0000e+02],\n        [ 4.5620e-01, -3.1623e+01],\n        [ 1.0000e-03, -4.6416e+01],\n        [ 6.0000e-04, -1.0000e+02],\n        [ 2.4920e-01, -4.6416e+01],\n        [ 1.3400e-02, -1.0000e+02],\n        [ 4.6200e-02, -4.6416e+01],\n        [ 4.8000e-03, -3.1623e+01],\n        [ 3.5000e-02, -6.8129e+01],\n        [ 1.4200e-02, -4.6416e+01],\n        [ 9.0000e-03, -6.8129e+01],\n        [ 3.4960e-01, -6.8129e+01],\n        [ 1.6000e-03, -1.0000e+02],\n        [ 6.0000e-04, -2.1544e+01],\n        [ 1.4000e-03, -3.1623e+01],\n        [ 1.1000e-02, -1.0000e+02],\n        [ 1.4000e-03, -4.6416e+01],\n        [ 2.7420e-01, -6.8129e+01],\n        [ 3.1000e-02, -1.0000e+02],\n        [ 2.5000e-02, -6.8129e+01],\n        [ 3.8000e-03, -4.6416e+01],\n        [ 5.8000e-03, -3.1623e+01],\n        [ 1.6000e-02, -3.1623e+01],\n        [ 8.4800e-02, -3.1623e+01],\n        [ 5.3540e-01, -1.0000e+02],\n        [ 2.0000e-03, -1.0000e+02],\n        [ 1.4000e-03, -1.0000e+02],\n        [ 1.1600e-02, -6.8129e+01],\n        [ 8.8000e-03, -1.0000e+02],\n        [ 2.7540e-01, -1.0000e+02],\n        [ 1.2800e-02, -3.1623e+01],\n        [ 1.0200e-02, -1.0000e+02],\n        [ 1.4000e-03, -2.1544e+01],\n        [ 8.4000e-03, -3.1623e+01],\n        [ 4.2400e-02, -1.0000e+02],\n        [ 6.8000e-03, -4.6416e+01],\n        [ 3.2000e-03, -1.0000e+02],\n        [ 2.6000e-03, -2.1544e+01],\n        [ 1.6000e-02, -4.6416e+01],\n        [ 1.9040e-01, -4.6416e+01],\n        [ 1.2540e-01, -1.0000e+02],\n        [ 8.0000e-03, -1.0000e+02],\n        [ 2.6000e-03, -4.6416e+01],\n        [ 3.0000e-03, -4.6416e+01],\n        [ 1.6400e-02, -4.6416e+01],\n        [ 1.0000e-03, -1.0000e+02],\n        [ 7.2000e-03, -1.0000e+02],\n        [ 4.1500e-01, -6.8129e+01],\n        [ 5.9400e-02, -1.0000e+02],\n        [ 4.8260e-01, -2.1544e+01],\n        [ 3.2000e-03, -1.0000e+02],\n        [ 3.4680e-01, -1.0000e+02],\n        [ 3.6780e-01, -3.1623e+01],\n        [ 4.4000e-03, -1.0000e+02],\n        [ 8.0000e-03, -3.1623e+01],\n        [ 1.0000e-03, -4.6416e+01],\n        [ 1.5980e-01, -2.1544e+01],\n        [ 3.0400e-02, -4.6416e+01],\n        [ 7.5200e-02, -1.0000e+02],\n        [ 7.7400e-02, -4.6416e+01],\n        [ 2.2000e-03, -6.8129e+01],\n        [ 2.0000e-03, -6.8129e+01],\n        [ 2.0400e-01, -4.6416e+01],\n        [ 8.0000e-04, -3.1623e+01],\n        [ 8.7040e-01, -2.1544e+01],\n        [ 1.0000e-03, -4.6416e+01],\n        [ 6.0000e-04, -3.1623e+01],\n        [ 2.0080e-01, -4.6416e+01],\n        [ 2.8000e-03, -3.1623e+01],\n        [ 1.0000e-03, -1.0000e+02],\n        [ 1.0000e-03, -6.8129e+01],\n        [ 1.2000e-03, -2.1544e+01],\n        [ 8.0000e-04, -3.1623e+01],\n        [ 8.0000e-04, -4.6416e+01],\n        [ 7.6600e-02, -6.8129e+01],\n        [ 1.2000e-03, -4.6416e+01],\n        [ 8.0000e-04, -2.1544e+01],\n        [ 7.8000e-03, -6.8129e+01],\n        [ 1.7000e-02, -4.6416e+01],\n        [ 4.8600e-02, -4.6416e+01],\n        [ 1.0860e-01, -1.0000e+02],\n        [ 1.8000e-03, -4.6416e+01],\n        [ 2.2400e-02, -2.1544e+01],\n        [ 7.3960e-01, -1.0000e+02],\n        [ 4.0000e-02, -4.6416e+01],\n        [ 3.4440e-01, -2.1544e+01],\n        [ 2.6000e-03, -1.0000e+02],\n        [ 4.4000e-03, -3.1623e+01],\n        [ 6.0200e-02, -4.6416e+01],\n        [ 3.9200e-02, -1.0000e+02],\n        [ 9.7600e-02, -6.8129e+01],\n        [ 3.0000e-03, -4.6416e+01],\n        [ 2.0000e-03, -2.1544e+01],\n        [ 1.0200e-02, -2.1544e+01],\n        [ 1.2000e-03, -4.6416e+01],\n        [ 1.1400e-02, -1.0000e+02],\n        [ 4.0000e-03, -3.1623e+01],\n        [ 1.2000e-03, -3.1623e+01],\n        [ 1.0000e-03, -2.1544e+01],\n        [ 3.2640e-01, -2.1544e+01],\n        [ 2.8000e-03, -1.0000e+02],\n        [ 6.4800e-02, -2.1544e+01],\n        [ 9.5000e-01, -1.0000e+02],\n        [ 2.8000e-03, -3.1623e+01],\n        [ 2.1400e-02, -6.8129e+01],\n        [ 6.4000e-03, -4.6416e+01],\n        [ 2.0000e-03, -6.8129e+01],\n        [ 7.4000e-03, -1.0000e+02],\n        [ 2.7340e-01, -1.0000e+02],\n        [ 6.4000e-03, -2.1544e+01],\n        [ 4.0000e-04, -1.0000e+02],\n        [ 2.5400e-02, -6.8129e+01],\n        [ 4.9400e-02, -1.0000e+02],\n        [ 8.0000e-03, -4.6416e+01]], dtype=torch.float64)\ntensor([1850., 3160.,  937., 1629.,  395., 2771.,  682.,  480.,  611.,  989.,\n         375.,  418., 1240.,  298.,  230., 2515.,  321.,  110., 1550.,  409.,\n        1184.,  974.,  742.,  935.,  557., 1120.,  233.,  537.,  682.,  398.,\n         461., 1082.,  521.,  715.,  576.,  969., 1335., 1832.,  838.,  225.,\n         201.,  550.,  409.,  751., 1421.,  410.,  886., 1155.,  539.,  767.,\n         284., 1437.,  906., 1576.,  647.,  349.,  491.,  602.,  906.,  173.,\n         360., 1211.,  586., 3856.,  278.,  798., 2480.,  302., 1131.,  381.,\n        3247., 1072.,  589., 1305.,  321.,  331., 1509.,  437., 4939.,  308.,\n         285., 1444.,  889.,  163.,  299.,  885.,  529.,  287.,  855.,  414.,\n         603.,  530.,  981., 1176.,  616.,  422., 2273.,  948., 1188., 3646.,\n         253.,  949., 1248.,  539.,  918.,  593., 1165., 2150.,  388.,  427.,\n        1024.,  631.,  823., 3724.,  281., 2545., 1131.,  821.,  641.,  805.,\n         336.,  376.,  759., 1765.,   75.,  750.,  548.,  852.],\n       dtype=torch.float64)\ntensor([[2.6837],\n        [2.6835],\n        [2.6838],\n        [2.6836],\n        [2.6836],\n        [2.6837],\n        [2.6837],\n        [2.6838],\n        [2.6838],\n        [2.6838],\n        [2.6838],\n        [2.6838],\n        [2.6838],\n        [2.6838],\n        [2.6838],\n        [2.6837],\n        [2.6837],\n        [2.6838],\n        [2.6837],\n        [2.6838],\n        [2.6837],\n        [2.6836],\n        [2.6838],\n        [2.6837],\n        [2.6838],\n        [2.6838],\n        [2.6838],\n        [2.6835],\n        [2.6836],\n        [2.6838],\n        [2.6837],\n        [2.6838],\n        [2.6838],\n        [2.6838],\n        [2.6837],\n        [2.6836],\n        [2.6836],\n        [2.6837],\n        [2.6838],\n        [2.6838],\n        [2.6838],\n        [2.6838],\n        [2.6838],\n        [2.6838],\n        [2.6836],\n        [2.6838],\n        [2.6835],\n        [2.6836],\n        [2.6838],\n        [2.6837],\n        [2.6838],\n        [2.6835],\n        [2.6837],\n        [2.6837],\n        [2.6838],\n        [2.6838],\n        [2.6837],\n        [2.6837],\n        [2.6837],\n        [2.6838],\n        [2.6838],\n        [2.6838],\n        [2.6838],\n        [2.6836],\n        [2.6838],\n        [2.6838],\n        [2.6837],\n        [2.6838],\n        [2.6836],\n        [2.6837],\n        [2.6835],\n        [2.6837],\n        [2.6838],\n        [2.6837],\n        [2.6838],\n        [2.6838],\n        [2.6837],\n        [2.6836],\n        [2.6836],\n        [2.6837],\n        [2.6836],\n        [2.6837],\n        [2.6836],\n        [2.6838],\n        [2.6838],\n        [2.6835],\n        [2.6836],\n        [2.6837],\n        [2.6838],\n        [2.6837],\n        [2.6835],\n        [2.6838],\n        [2.6837],\n        [2.6837],\n        [2.6838],\n        [2.6837],\n        [2.6835],\n        [2.6838],\n        [2.6837],\n        [2.6836],\n        [2.6838],\n        [2.6836],\n        [2.6837],\n        [2.6838],\n        [2.6838],\n        [2.6837],\n        [2.6835],\n        [2.6835],\n        [2.6837],\n        [2.6838],\n        [2.6836],\n        [2.6836],\n        [2.6835],\n        [2.6836],\n        [2.6838],\n        [2.6835],\n        [2.6838],\n        [2.6836],\n        [2.6838],\n        [2.6837],\n        [2.6838],\n        [2.6838],\n        [2.6838],\n        [2.6835],\n        [2.6838],\n        [2.6838],\n        [2.6838],\n        [2.6837]], dtype=torch.float64, grad_fn=<ReluBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for i, (xg, a) in enumerate(tr_dl):\n",
    "    print(xg)\n",
    "    print(a)\n",
    "\n",
    "    print(bnet(xg))\n",
    "\n",
    "    if i==1:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 861,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train(dl, model, loss_fn, optim):\n",
    "    size = len(dl.dataset)\n",
    "    for batch, (xg, a) in enumerate(dl):\n",
    "        # Compute prediction error\n",
    "        pred = model(xg)\n",
    "        loss = loss_fn(pred, a)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # if batch % 10 == 0:\n",
    "        #     loss, current = loss.item(), batch * len(xg)\n",
    "        #     print(f\"loss: {loss:>.2f}  [{current:>2d}/{size:>2d}]\")\n",
    "\n",
    "        return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 862,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dl, model, loss_fn):\n",
    "    size = len(dl.dataset)\n",
    "    num_batches = len(dl)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for xg, a in dl:\n",
    "            pred = model(xg)\n",
    "            test_loss += loss_fn(pred, a).item()\n",
    "    test_loss /= num_batches\n",
    "    return test_loss\n",
    "    #print(f\"Test Error: Avg loss: {test_loss:>.2f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 902,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1 - \n",
      "\tTrain loss: 2700910.45\n",
      "\tTest loss: 2493463.21\n",
      "Epoch 101 - \n",
      "\tTrain loss: 2325001.17\n",
      "\tTest loss: 2491017.54\n",
      "Epoch 201 - \n",
      "\tTrain loss: 2245138.84\n",
      "\tTest loss: 2490569.44\n",
      "Epoch 301 - \n",
      "\tTrain loss: 2972198.84\n",
      "\tTest loss: 2490225.71\n",
      "Epoch 401 - \n",
      "\tTrain loss: 2499159.97\n",
      "\tTest loss: 2489936.42\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "epochs = 500\n",
    "for t in range(epochs):\n",
    "    trloss = train(tr_dl, bnet, mse_loss, optimizer)\n",
    "    teloss = test(te_dl, bnet, mse_loss)\n",
    "    if t%100 == 0:\n",
    "        print(f\"Epoch {t+1} - \")\n",
    "        print(f\"\\tTrain loss: {trloss:.2f}\")\n",
    "        print(f\"\\tTest loss: {teloss:.2f}\")\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 806,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[5.1502],\n",
       "        [5.1502],\n",
       "        [5.1502],\n",
       "        ...,\n",
       "        [5.1500],\n",
       "        [5.1500],\n",
       "        [5.1500]], dtype=torch.float64, grad_fn=<AddmmBackward>)"
      ]
     },
     "metadata": {},
     "execution_count": 806
    }
   ],
   "source": [
    "preds = bnet(torch.tensor(np.vstack((np.hstack((Xltrain[0:5])), np.repeat(gamma[0:5],len(Xltrain[0])))).T))\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_score(y_true=altrain[5], y_pred=pred.detach().numpy())\n",
    "#bnet(torch.unsqueeze(torch.tensor(Xltrain[5][0:1]),1), torch.unsqueeze(torch.tensor(np.repeat(gamma[5],1)),1))\n",
    "#torch.unsqueeze(torch.cat((torch.tensor(Xltrain[5]),torch.tensor(Xltrain[0])),0),1)\n",
    "torch.unsqueeze(torch.tensor(np.repeat(gamma[0:9],len(Xltrain[0]))),1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.unsqueeze(torch.tensor(Xltrain[0]),1)\n",
    "#bnet.forward(torch.tensor(Xltrain[0]), torch.tensor(np.repeat(gamma[0], len(Xltrain[0]))))\n",
    "torch.tensor(Xltrain[0]).view(-1, torch.tensor(Xltrain[0]).size(0)).shape\n",
    "torch.unsqueeze(torch.tensor(Xltrain[0]),1)\n",
    "#print(torch.unsqueeze(torch.linspace(-1,1,10),-1).shape)\n",
    "#bnet.forward(torch.tensor(Xltrain[0][1]),torch.tensor(gamma[0]))\n",
    "#torch.unsqueeze(torch.tensor(Xltrain[0]),0).shape"
   ]
  },
  {
   "source": [
    "## VAE implementation\n",
    "\n",
    "Following the procedure in https://github.com/AntixK/PyTorch-VAE/blob/master/models/vanilla_vae.py, seems pretty similar to the architecture of the BaselineNet..."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseVAE(nn.Module):\n",
    "    \n",
    "    def __init__(self) -> None:\n",
    "        super(BaseVAE, self).__init__()\n",
    "\n",
    "    def encode(self, input: torch.Tensor) -> List[torch.Tensor]:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def decode(self, input: torch.Tensor) -> Any:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def sample(self, batch_size:int, current_device: int, **kwargs) -> torch.Tensor:\n",
    "        raise RuntimeWarning()\n",
    "\n",
    "    def generate(self, x: torch.Tensor, **kwargs) -> torch.Tensor:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @abstractmethod\n",
    "    def forward(self, *inputs: torch.Tensor) -> torch.Tensor:\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def loss_function(self, *inputs: Any, **kwargs) -> torch.Tensor:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 881,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from https://debuggercafe.com/getting-started-with-variational-autoencoder-using-pytorch/\n",
    "# define a simple linear VAE\n",
    "class LinearVAE(nn.Module):\n",
    "    def __init__(self, features):\n",
    "        super(LinearVAE, self).__init__()\n",
    " \n",
    "        # encoder\n",
    "        self.enc1 = nn.Linear(2, 48)\n",
    "        self.enc2 = nn.Linear(48, features*2)\n",
    " \n",
    "        # decoder \n",
    "        self.dec1 = nn.Linear(features, 24)\n",
    "        self.dec2 = nn.Linear(24, 1)\n",
    "\n",
    "        self.double()\n",
    "\n",
    "    def reparameterize(self, mu, log_var):\n",
    "        \"\"\"\n",
    "        :param mu: mean from the encoder's latent space\n",
    "        :param log_var: log variance from the encoder's latent space\n",
    "        \"\"\"\n",
    "        std = torch.exp(0.5*log_var) # standard deviation\n",
    "        eps = torch.randn_like(std) # `randn_like` as we need the same size\n",
    "        sample = mu + (eps * std) # sampling as if coming from the input space\n",
    "        return sample\n",
    " \n",
    "    def forward(self, x):\n",
    "        # encoding\n",
    "        x = F.tanh(self.enc1(x))\n",
    "        x = self.enc2(x).view(-1, 2, features)\n",
    "\n",
    "        # get `mu` and `log_var`\n",
    "        mu = x[:, 0, :] # the first feature values as mean\n",
    "        log_var = x[:, 1, :] # the other feature values as variance\n",
    "\n",
    "        # get the latent vector through reparameterization\n",
    "        z = self.reparameterize(mu, log_var)\n",
    " \n",
    "        # decoding\n",
    "        x = F.tanh(self.dec1(z))\n",
    "        reconstruction = torch.sigmoid(self.dec2(x))\n",
    "        return reconstruction, mu, log_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_loss(mse_loss, mu, logvar):\n",
    "    \"\"\"\n",
    "    This function will add the reconstruction loss (MSELoss) and the \n",
    "    KL-Divergence.\n",
    "    KL-Divergence = 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
    "\n",
    "    :param mse_loss: recontruction loss\n",
    "    :param mu: the mean from the latent vector\n",
    "    :param logvar: log variance from the latent vector\n",
    "    \"\"\"\n",
    "    MSE = mse_loss \n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "\n",
    "    return MSE + KLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 896,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 512\n",
    "model = LinearVAE(features=16) # dimensionality of the latent variable\n",
    "optimizer = optim.SGD(model.parameters(), lr=2e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 888,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_vae(dl, model, loss_fn, optim):\n",
    "    size = len(dl.dataset)\n",
    "    for batch, (xg, a) in enumerate(dl):\n",
    "        # Compute prediction error\n",
    "        pred, mu, logvar = model(xg)\n",
    "        step_loss = loss_fn(pred, a)\n",
    "\n",
    "        loss = final_loss(step_loss, mu, logvar)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        step_loss.backward()\n",
    "        optimizer.step()\n",
    "        # if batch % 10 == 0:\n",
    "        #     loss, current = loss.item(), batch * len(xg)\n",
    "        #     print(f\"loss: {loss:>.2f}  [{current:>2d}/{size:>2d}]\")\n",
    "\n",
    "        return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 884,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_vae(dl, model, loss_fn):\n",
    "    size = len(dl.dataset)\n",
    "    num_batches = len(dl)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for xg, a in dl:\n",
    "            pred, _, _ = model(xg)\n",
    "            test_loss += loss_fn(pred, a).item()\n",
    "    test_loss /= num_batches\n",
    "    return test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 897,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1 - \n",
      "\tTrain loss: 2877075.60\n",
      "\tTest loss: 2493847.20\n",
      "Epoch 101 - \n",
      "\tTrain loss: 2341032.27\n",
      "\tTest loss: 2493783.15\n",
      "Epoch 201 - \n",
      "\tTrain loss: 2051467.14\n",
      "\tTest loss: 2493783.09\n",
      "Epoch 301 - \n",
      "\tTrain loss: 2134317.78\n",
      "\tTest loss: 2493783.07\n",
      "Epoch 401 - \n",
      "\tTrain loss: 2318922.50\n",
      "\tTest loss: 2493783.05\n",
      "Epoch 501 - \n",
      "\tTrain loss: 2566924.09\n",
      "\tTest loss: 2493783.04\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "epochs = 600\n",
    "for t in range(epochs):\n",
    "    trloss = train_vae(tr_dl, model, mse_loss, optimizer)\n",
    "    teloss = test_vae(te_dl, model, mse_loss)\n",
    "    if t%100 == 0:\n",
    "        print(f\"Epoch {t+1} - \")\n",
    "        print(f\"\\tTrain loss: {trloss:.2f}\")\n",
    "        print(f\"\\tTest loss: {teloss:.2f}\")\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 763,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "epoch [0], Loss: 295960.03\n",
      "epoch [100], Loss: nan\n",
      "epoch [200], Loss: nan\n",
      "epoch [300], Loss: nan\n",
      "epoch [400], Loss: nan\n",
      "epoch [500], Loss: nan\n",
      "epoch [600], Loss: nan\n",
      "epoch [700], Loss: nan\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "for e in range(800):\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    pred, mu, logvar = model(torch.tensor(np.vstack((Xltrain[0],np.repeat(gamma[0],len(Xltrain[0])))).T))\n",
    "    step_loss = mse_loss(pred, torch.unsqueeze(torch.tensor(np.hstack((altrain[0]))),1))\n",
    "\n",
    "    loss = final_loss(step_loss, mu, logvar)\n",
    "    loss.backward()\n",
    "\n",
    "    # update with current step regression parameters \n",
    "    optimizer.step()\n",
    "\n",
    "    if e % 100 == 0:\n",
    "        print ('epoch [{}], Loss: {:.2f}'.format(e, loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 747,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([800, 2])"
      ]
     },
     "metadata": {},
     "execution_count": 747
    }
   ],
   "source": [
    "torch.tensor(np.vstack((Xltrain[0],np.repeat(gamma[0],len(Xltrain[0])))).T).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 756,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan]], dtype=torch.float64, grad_fn=<SigmoidBackward>)"
      ]
     },
     "metadata": {},
     "execution_count": 756
    }
   ],
   "source": []
  }
 ]
}