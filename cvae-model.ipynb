{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Notebook for building and testing the model     \n",
    "\n",
    "In this notebook, I will build and run an initial test over the first 2,048 rows of the data to ensure proper functionality before deploying on midway2 to train on GPUs. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset, TensorDataset, DataLoader\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cuda setup\n",
    "device = torch.device(\"cuda\")\n",
    "kwargs = {'num_workers': 2, 'pin_memory': True} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyper params\n",
    "batch_size = 64\n",
    "latent_size = 10\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading in the data\n",
    "dat = np.genfromtxt('traindata/trip-2021-07-28.csv', delimiter=',')[1:,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to subset this in a conditional fashion (i.e., each gamma value, here class c, has 1000 data points)\n",
    "gamma = np.unique(dat[:,1])\n",
    "idx = [np.where(dat[:,1] == gamma[i]) for i in np.arange(len(gamma))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an 80/20 split in each data set\n",
    "temp = [train_test_split(dat[idx[i][0],0], dat[idx[i][0],2], test_size=0.2, random_state=42) for i in np.arange(len(gamma))]\n",
    "\n",
    "# for each gamma value...\n",
    "Xltrain = []\n",
    "Xltest = []\n",
    "altrain = []\n",
    "altest = []\n",
    "for t in np.arange(len(temp)):\n",
    "    Xltrain.append(temp[t][0])\n",
    "    Xltest.append(temp[t][1])\n",
    "    altrain.append(temp[t][2])\n",
    "    altest.append(temp[t][3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x7fa92450b8e0>"
      ]
     },
     "metadata": {},
     "execution_count": 91
    }
   ],
   "source": [
    "torch.utils.data.DataLoader(TensorDataset(torch.from_numpy(Xltrain[0]), torch.from_numpy(altrain[0])), batch_size=batch_size, shuffle=True, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 590,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaselineNet(nn.Module):\n",
    "    def __init__(self, hidden1):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Linear(1, hidden1),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden1, hidden1),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden1, 50),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(50, 10),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "        self.fc2 = nn.Sequential(\n",
    "            nn.Linear(1, 2),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "\n",
    "        self.predict = nn.Sequential(\n",
    "            nn.Linear(10 + 2, 1), \n",
    "        )\n",
    "        self.double()\n",
    "\n",
    "    def forward(self, x, g):\n",
    "        x = self.fc1(x)\n",
    "        g = self.fc2(g)\n",
    "        a = self.predict(torch.cat((x,g), 1))\n",
    "        return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 650,
   "metadata": {},
   "outputs": [],
   "source": [
    "bnet = BaselineNet(500)\n",
    "bnet.eval()\n",
    "mse_loss = nn.MSELoss()\n",
    "optimizer = optim.SGD(bnet.parameters(), lr=2e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 651,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "epoch [0], Loss: 25861327.75\n",
      "epoch [100], Loss: 18855595.18\n",
      "epoch [200], Loss: 16333009.39\n",
      "epoch [300], Loss: 15443805.50\n",
      "epoch [400], Loss: 15130363.79\n",
      "epoch [500], Loss: 15019876.52\n",
      "epoch [600], Loss: 14980930.06\n",
      "epoch [700], Loss: 14967201.54\n",
      "epoch [800], Loss: 14962362.26\n",
      "epoch [900], Loss: 14960656.42\n",
      "epoch [1000], Loss: 14960055.10\n",
      "epoch [1100], Loss: 14959843.13\n",
      "epoch [1200], Loss: 14959768.41\n",
      "epoch [1300], Loss: 14959742.06\n",
      "epoch [1400], Loss: 14959732.77\n",
      "epoch [1500], Loss: 14959729.50\n",
      "epoch [1600], Loss: 14959728.34\n",
      "epoch [1700], Loss: 14959727.92\n",
      "epoch [1800], Loss: 14959727.78\n",
      "epoch [1900], Loss: 14959727.72\n"
     ]
    }
   ],
   "source": [
    "for e in range(2000):\n",
    "    #pred = bnet(torch.from_numpy(Xltrain[0]).float(), torch.from_numpy(np.repeat(gamma[0], len(Xltrain[0]))).float())\n",
    "    #pred = bnet(torch.unsqueeze(torch.tensor(Xltrain[5]),1), torch.unsqueeze(torch.tensor(np.repeat(gamma[5], len(Xltrain[5]))),1))\n",
    "    pred = bnet(torch.unsqueeze(torch.tensor(np.hstack((Xltrain[0:2]))),1), torch.unsqueeze(torch.tensor(np.repeat(gamma[0:2],len(Xltrain[0]))),1))\n",
    "    step_loss = mse_loss(pred, torch.unsqueeze(torch.tensor(np.hstack((altrain[0:2]))),1))\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    step_loss.backward()\n",
    "    # update with current step regression parameters \n",
    "    optimizer.step()\n",
    "\n",
    "    if e % 100 == 0:\n",
    "        print ('epoch [{}], Loss: {:.2f}'.format(e, step_loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 652,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[3301.1692],\n",
       "        [3301.1692],\n",
       "        [3301.1692],\n",
       "        ...,\n",
       "        [3301.1692],\n",
       "        [3301.1692],\n",
       "        [3301.1692]], dtype=torch.float64, grad_fn=<AddmmBackward>)"
      ]
     },
     "metadata": {},
     "execution_count": 652
    }
   ],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 649,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([7200, 1])"
      ]
     },
     "metadata": {},
     "execution_count": 649
    }
   ],
   "source": [
    "#r2_score(y_true=altrain[5], y_pred=pred.detach().numpy())\n",
    "#bnet(torch.unsqueeze(torch.tensor(Xltrain[5][0:1]),1), torch.unsqueeze(torch.tensor(np.repeat(gamma[5],1)),1))\n",
    "#torch.unsqueeze(torch.cat((torch.tensor(Xltrain[5]),torch.tensor(Xltrain[0])),0),1)\n",
    "torch.unsqueeze(torch.tensor(np.repeat(gamma[0:9],len(Xltrain[0]))),1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "800"
      ]
     },
     "metadata": {},
     "execution_count": 481
    }
   ],
   "source": [
    "#torch.unsqueeze(torch.tensor(Xltrain[0]),1)\n",
    "#bnet.forward(torch.tensor(Xltrain[0]), torch.tensor(np.repeat(gamma[0], len(Xltrain[0]))))\n",
    "torch.tensor(Xltrain[0]).view(-1, torch.tensor(Xltrain[0]).size(0)).shape\n",
    "torch.unsqueeze(torch.tensor(Xltrain[0]),1)\n",
    "#print(torch.unsqueeze(torch.linspace(-1,1,10),-1).shape)\n",
    "#bnet.forward(torch.tensor(Xltrain[0][1]),torch.tensor(gamma[0]))\n",
    "#torch.unsqueeze(torch.tensor(Xltrain[0]),0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 514,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 71.3519, -73.1773, -42.9027,  71.6761,  72.8176, -74.0732, -73.8141]],\n",
       "       dtype=torch.float64, requires_grad=True)"
      ]
     },
     "metadata": {},
     "execution_count": 514
    }
   ],
   "source": [
    "bnet.predict[0].weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}