{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Notebook for building and testing the model     \n",
    "\n",
    "In this notebook, I will build and run an initial test over the first 2,048 rows of the data to ensure proper functionality before deploying on midway2 to train on GPUs. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset, TensorDataset, DataLoader\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cuda setup\n",
    "device = torch.device(\"cuda\")\n",
    "kwargs = {'num_workers': 2, 'pin_memory': True} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 831,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyper params\n",
    "batch_size = 128\n",
    "latent_size = 10\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading in the data\n",
    "dat = np.genfromtxt('traindata/trip-2021-07-28.csv', delimiter=',')[1:,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to subset this in a conditional fashion (i.e., each gamma value, here class c, has 1000 data points)\n",
    "gamma = np.unique(dat[:,1])\n",
    "idx = [np.where(dat[:,1] == gamma[i]) for i in np.arange(len(gamma))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an 80/20 split in each data set\n",
    "temp = [train_test_split(dat[idx[i][0],0], dat[idx[i][0],2], test_size=0.2, random_state=42) for i in np.arange(len(gamma))]\n",
    "\n",
    "# for each gamma value...\n",
    "Xltrain = []\n",
    "Xltest = []\n",
    "altrain = []\n",
    "altest = []\n",
    "for t in np.arange(len(temp)):\n",
    "    Xltrain.append(temp[t][0])\n",
    "    Xltest.append(temp[t][1])\n",
    "    altrain.append(temp[t][2])\n",
    "    altest.append(temp[t][3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 873,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a simple matrix of training data (...x2, ...x1) \n",
    "tr_dat = TensorDataset(torch.tensor(np.vstack((np.hstack((Xltrain[0])), np.repeat(gamma[0],len(Xltrain[0])))).T), torch.tensor(np.hstack((altrain[0]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 875,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_dl = DataLoader(tr_dat, batch_size=64, shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 876,
   "metadata": {},
   "outputs": [],
   "source": [
    "te_dat = TensorDataset(torch.tensor(np.vstack((np.hstack((Xltest[0])), np.repeat(gamma[0],len(Xltest[0])))).T), torch.tensor(np.hstack((altest[0]))))\n",
    "te_dl = DataLoader(te_dat, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 867,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaselineNet(nn.Module):\n",
    "    def __init__(self, hidden1):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Sequential(\n",
    "            # nn.Linear(2, hidden1),\n",
    "            # nn.Tanh(),\n",
    "            # nn.Linear(hidden1, 50), \n",
    "            # nn.Tanh(),\n",
    "            # nn.Linear(50, 1),\n",
    "            nn.Linear(2, 1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.double()\n",
    "\n",
    "    def forward(self, xg):\n",
    "        a = self.fc1(xg)\n",
    "        return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 877,
   "metadata": {},
   "outputs": [],
   "source": [
    "bnet = BaselineNet(500)\n",
    "bnet.eval()\n",
    "mse_loss = nn.MSELoss()\n",
    "optimizer = optim.Adagrad(bnet.parameters(), lr=2e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 878,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "800"
      ]
     },
     "metadata": {},
     "execution_count": 878
    }
   ],
   "source": [
    "len(tr_dl.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 866,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[ 1.6800e-02, -2.1544e+01],\n        [ 2.8000e-03, -4.6416e+01],\n        [ 1.0000e-03, -4.6416e+01],\n        [ 9.9340e-01, -6.8129e+01],\n        [ 6.0000e-04, -1.0000e+02],\n        [ 2.8000e-03, -2.1544e+01],\n        [ 8.5600e-02, -3.1623e+01],\n        [ 1.0640e-01, -6.8129e+01],\n        [ 6.0000e-04, -3.1623e+01],\n        [ 1.6000e-03, -4.6416e+01],\n        [ 4.0000e-04, -3.1623e+01],\n        [ 1.8500e-01, -2.1544e+01],\n        [ 9.6000e-03, -6.8129e+01],\n        [ 1.4000e-03, -1.0000e+02],\n        [ 1.6760e-01, -2.1544e+01],\n        [ 8.8000e-03, -2.1544e+01],\n        [ 5.3480e-01, -1.0000e+02],\n        [ 6.7940e-01, -4.6416e+01],\n        [ 7.4800e-01, -2.1544e+01],\n        [ 8.0000e-03, -2.1544e+01],\n        [ 4.0260e-01, -2.1544e+01],\n        [ 4.0000e-04, -1.0000e+02],\n        [ 2.8400e-02, -1.0000e+02],\n        [ 8.8000e-03, -3.1623e+01],\n        [ 2.2000e-03, -3.1623e+01],\n        [ 2.6000e-03, -3.1623e+01],\n        [ 1.0000e-03, -1.0000e+02],\n        [ 2.0000e-03, -1.0000e+02],\n        [ 9.2000e-03, -4.6416e+01],\n        [ 7.3120e-01, -1.0000e+02],\n        [ 6.0000e-04, -6.8129e+01],\n        [ 1.6000e-03, -6.8129e+01],\n        [ 1.1600e-02, -2.1544e+01],\n        [ 8.4000e-03, -6.8129e+01],\n        [ 6.0000e-04, -2.1544e+01],\n        [ 1.4000e-03, -3.1623e+01],\n        [ 1.3400e-02, -3.1623e+01],\n        [ 3.2200e-02, -2.1544e+01],\n        [ 9.1700e-01, -3.1623e+01],\n        [ 1.2800e-02, -4.6416e+01],\n        [ 2.9600e-01, -1.0000e+02],\n        [ 7.8000e-03, -6.8129e+01],\n        [ 9.8000e-03, -2.1544e+01],\n        [ 1.5720e-01, -3.1623e+01],\n        [ 7.1600e-02, -2.1544e+01],\n        [ 2.2600e-02, -1.0000e+02],\n        [ 3.0000e-03, -6.8129e+01],\n        [ 9.1620e-01, -3.1623e+01],\n        [ 1.9800e-01, -6.8129e+01],\n        [ 1.4200e-02, -6.8129e+01],\n        [ 2.1800e-01, -3.1623e+01],\n        [ 1.0000e-03, -3.1623e+01],\n        [ 6.0000e-04, -4.6416e+01],\n        [ 8.7600e-02, -2.1544e+01],\n        [ 6.9800e-02, -3.1623e+01],\n        [ 8.0000e-04, -1.0000e+02],\n        [ 3.1000e-02, -1.0000e+02],\n        [ 9.6000e-03, -3.1623e+01],\n        [ 3.2000e-03, -1.0000e+02],\n        [ 6.0000e-04, -2.1544e+01],\n        [ 6.9480e-01, -6.8129e+01],\n        [ 6.0000e-04, -2.1544e+01],\n        [ 9.6180e-01, -2.1544e+01],\n        [ 5.4000e-03, -3.1623e+01],\n        [ 1.6000e-03, -6.8129e+01],\n        [ 8.8000e-03, -2.1544e+01],\n        [ 6.0000e-04, -4.6416e+01],\n        [ 4.0000e-04, -3.1623e+01],\n        [ 2.1600e-02, -4.6416e+01],\n        [ 4.0000e-04, -6.8129e+01],\n        [ 1.0000e-02, -4.6416e+01],\n        [ 7.7200e-02, -1.0000e+02],\n        [ 5.8000e-03, -1.0000e+02],\n        [ 4.8000e-03, -3.1623e+01],\n        [ 1.3600e-02, -3.1623e+01],\n        [ 8.3940e-01, -1.0000e+02],\n        [ 6.4000e-01, -1.0000e+02],\n        [ 9.8620e-01, -3.1623e+01],\n        [ 5.5000e-02, -1.0000e+02],\n        [ 8.0000e-04, -3.1623e+01],\n        [ 4.1840e-01, -4.6416e+01],\n        [ 9.9580e-01, -6.8129e+01],\n        [ 1.3980e-01, -1.0000e+02],\n        [ 8.0000e-04, -3.1623e+01],\n        [ 5.3600e-01, -3.1623e+01],\n        [ 6.0000e-04, -1.0000e+02],\n        [ 6.0000e-03, -1.0000e+02],\n        [ 7.9600e-01, -2.1544e+01],\n        [ 3.7460e-01, -3.1623e+01],\n        [ 1.0000e-03, -2.1544e+01],\n        [ 4.4000e-03, -4.6416e+01],\n        [ 9.0400e-02, -4.6416e+01],\n        [ 7.8040e-01, -1.0000e+02],\n        [ 5.8200e-02, -3.1623e+01],\n        [ 6.0000e-04, -2.1544e+01],\n        [ 3.8560e-01, -2.1544e+01],\n        [ 1.4000e-03, -6.8129e+01],\n        [ 1.9940e-01, -4.6416e+01],\n        [ 1.9600e-01, -1.0000e+02],\n        [ 1.1400e-02, -4.6416e+01],\n        [ 9.7640e-01, -3.1623e+01],\n        [ 2.0800e-02, -4.6416e+01],\n        [ 2.4000e-03, -6.8129e+01],\n        [ 8.1500e-01, -4.6416e+01],\n        [ 4.3600e-02, -4.6416e+01],\n        [ 1.0000e-03, -1.0000e+02],\n        [ 2.4600e-01, -6.8129e+01],\n        [ 1.8000e-03, -6.8129e+01],\n        [ 1.1000e-02, -1.0000e+02],\n        [ 5.5500e-01, -4.6416e+01],\n        [ 1.2000e-03, -3.1623e+01],\n        [ 2.1780e-01, -4.6416e+01],\n        [ 1.8600e-02, -4.6416e+01],\n        [ 6.0000e-04, -6.8129e+01],\n        [ 2.7600e-02, -2.1544e+01],\n        [ 2.9600e-02, -1.0000e+02],\n        [ 8.9400e-02, -2.1544e+01],\n        [ 2.4200e-02, -1.0000e+02],\n        [ 1.4780e-01, -2.1544e+01],\n        [ 5.5200e-02, -2.1544e+01],\n        [ 2.0000e-03, -4.6416e+01],\n        [ 2.4000e-03, -4.6416e+01],\n        [ 4.3340e-01, -2.1544e+01],\n        [ 4.4000e-03, -6.8129e+01],\n        [ 1.2200e-01, -4.6416e+01],\n        [ 6.9780e-01, -3.1623e+01],\n        [ 1.0000e-03, -4.6416e+01],\n        [ 1.8000e-03, -4.6416e+01]], dtype=torch.float64)\ntensor([2201.,  547.,  324., 1854.,  121., 1324., 2050.,  975.,  315.,  430.,\n         231., 3112.,  583.,  192., 2999., 1642.,  875., 1943., 4782., 1617.,\n        3779.,   68.,  515., 1145.,  736.,  740.,  146.,  215.,  820.,  955.,\n         163.,  299., 2058.,  559.,  508.,  675., 1355., 2222., 3383.,  794.,\n         761.,  550., 1684., 2152., 2756.,  470.,  375., 3447., 1015.,  630.,\n        2243.,  489.,  205., 2893., 1877.,  148.,  521., 1123.,  284.,  452.,\n        1369.,  518., 5339., 1003.,  309., 1661.,  264.,  257.,  990.,  117.,\n         798.,  603.,  340., 1030., 1208., 1015.,  915., 3724.,  576.,  372.,\n        1755., 1845.,  669.,  467., 2660.,  120.,  352., 4645., 2557.,  787.,\n         652., 1373.,  982., 1867.,  574., 3691.,  311., 1510.,  713.,  907.,\n        3683.,  997.,  378., 2123., 1184.,  142., 1101.,  319.,  434., 1893.,\n         631., 1495., 1004.,  145., 2237.,  505., 2772.,  469., 3127., 2727.,\n         562.,  551., 3850.,  416., 1395., 2806.,  361.,  448.],\n       dtype=torch.float64)\ntensor([[3.7219],\n        [3.7239],\n        [3.7239],\n        [3.7241],\n        [3.7244],\n        [3.7219],\n        [3.7232],\n        [3.7242],\n        [3.7232],\n        [3.7239],\n        [3.7232],\n        [3.7219],\n        [3.7243],\n        [3.7244],\n        [3.7219],\n        [3.7219],\n        [3.7244],\n        [3.7237],\n        [3.7215],\n        [3.7219],\n        [3.7218],\n        [3.7244],\n        [3.7244],\n        [3.7232],\n        [3.7232],\n        [3.7232],\n        [3.7244],\n        [3.7244],\n        [3.7239],\n        [3.7244],\n        [3.7243],\n        [3.7243],\n        [3.7219],\n        [3.7243],\n        [3.7219],\n        [3.7232],\n        [3.7232],\n        [3.7219],\n        [3.7228],\n        [3.7239],\n        [3.7244],\n        [3.7243],\n        [3.7219],\n        [3.7231],\n        [3.7219],\n        [3.7244],\n        [3.7243],\n        [3.7228],\n        [3.7242],\n        [3.7243],\n        [3.7231],\n        [3.7232],\n        [3.7239],\n        [3.7219],\n        [3.7232],\n        [3.7244],\n        [3.7244],\n        [3.7232],\n        [3.7244],\n        [3.7219],\n        [3.7242],\n        [3.7219],\n        [3.7213],\n        [3.7232],\n        [3.7243],\n        [3.7219],\n        [3.7239],\n        [3.7232],\n        [3.7239],\n        [3.7243],\n        [3.7239],\n        [3.7244],\n        [3.7244],\n        [3.7232],\n        [3.7232],\n        [3.7244],\n        [3.7244],\n        [3.7227],\n        [3.7244],\n        [3.7232],\n        [3.7238],\n        [3.7241],\n        [3.7244],\n        [3.7232],\n        [3.7230],\n        [3.7244],\n        [3.7244],\n        [3.7215],\n        [3.7231],\n        [3.7219],\n        [3.7239],\n        [3.7239],\n        [3.7244],\n        [3.7232],\n        [3.7219],\n        [3.7218],\n        [3.7243],\n        [3.7239],\n        [3.7244],\n        [3.7239],\n        [3.7227],\n        [3.7239],\n        [3.7243],\n        [3.7237],\n        [3.7239],\n        [3.7244],\n        [3.7242],\n        [3.7243],\n        [3.7244],\n        [3.7238],\n        [3.7232],\n        [3.7239],\n        [3.7239],\n        [3.7243],\n        [3.7219],\n        [3.7244],\n        [3.7219],\n        [3.7244],\n        [3.7219],\n        [3.7219],\n        [3.7239],\n        [3.7239],\n        [3.7218],\n        [3.7243],\n        [3.7239],\n        [3.7229],\n        [3.7239],\n        [3.7239]], dtype=torch.float64, grad_fn=<AddmmBackward>)\ntensor([[ 1.3600e-02, -3.1623e+01],\n        [ 6.0000e-04, -3.1623e+01],\n        [ 2.9280e-01, -3.1623e+01],\n        [ 1.2060e-01, -6.8129e+01],\n        [ 1.0800e-02, -1.0000e+02],\n        [ 3.5400e-02, -3.1623e+01],\n        [ 1.2800e-02, -4.6416e+01],\n        [ 7.9340e-01, -6.8129e+01],\n        [ 1.0400e-02, -6.8129e+01],\n        [ 2.2800e-02, -2.1544e+01],\n        [ 3.0000e-01, -2.1544e+01],\n        [ 5.0800e-02, -2.1544e+01],\n        [ 8.0000e-04, -1.0000e+02],\n        [ 4.6000e-03, -4.6416e+01],\n        [ 8.0000e-03, -4.6416e+01],\n        [ 1.3000e-02, -2.1544e+01],\n        [ 1.3400e-02, -1.0000e+02],\n        [ 5.9100e-01, -6.8129e+01],\n        [ 9.3440e-01, -6.8129e+01],\n        [ 3.3880e-01, -6.8129e+01],\n        [ 8.5920e-01, -2.1544e+01],\n        [ 4.2000e-03, -3.1623e+01],\n        [ 3.1000e-02, -1.0000e+02],\n        [ 8.0000e-04, -1.0000e+02],\n        [ 2.0400e-01, -4.6416e+01],\n        [ 3.4440e-01, -2.1544e+01],\n        [ 4.2000e-03, -4.6416e+01],\n        [ 2.7400e-02, -3.1623e+01],\n        [ 7.3960e-01, -1.0000e+02],\n        [ 2.0000e-03, -4.6416e+01],\n        [ 1.8000e-03, -3.1623e+01],\n        [ 2.6120e-01, -6.8129e+01],\n        [ 4.4000e-03, -3.1623e+01],\n        [ 3.2000e-03, -2.1544e+01],\n        [ 1.2000e-03, -1.0000e+02],\n        [ 3.4000e-03, -3.1623e+01],\n        [ 4.8000e-03, -3.1623e+01],\n        [ 7.7400e-02, -4.6416e+01],\n        [ 1.3220e-01, -2.1544e+01],\n        [ 8.0000e-04, -1.0000e+02],\n        [ 1.2000e-03, -6.8129e+01],\n        [ 6.0000e-04, -2.1544e+01],\n        [ 1.9000e-02, -4.6416e+01],\n        [ 2.0000e-03, -2.1544e+01],\n        [ 4.9000e-02, -4.6416e+01],\n        [ 8.6460e-01, -4.6416e+01],\n        [ 3.0880e-01, -6.8129e+01],\n        [ 8.6700e-01, -6.8129e+01],\n        [ 8.2000e-03, -6.8129e+01],\n        [ 9.2800e-02, -3.1623e+01],\n        [ 9.8440e-01, -2.1544e+01],\n        [ 1.0000e-03, -4.6416e+01],\n        [ 8.4000e-03, -3.1623e+01],\n        [ 6.0000e-04, -2.1544e+01],\n        [ 7.4000e-03, -1.0000e+02],\n        [ 2.2000e-03, -4.6416e+01],\n        [ 2.8000e-03, -4.6416e+01],\n        [ 8.2000e-03, -4.6416e+01],\n        [ 3.3200e-02, -3.1623e+01],\n        [ 3.4000e-03, -1.0000e+02],\n        [ 2.0600e-02, -4.6416e+01],\n        [ 6.0000e-04, -4.6416e+01],\n        [ 1.0000e-03, -4.6416e+01],\n        [ 7.6000e-03, -1.0000e+02],\n        [ 8.1800e-02, -2.1544e+01],\n        [ 1.2000e-02, -3.1623e+01],\n        [ 1.3000e-02, -2.1544e+01],\n        [ 2.0860e-01, -3.1623e+01],\n        [ 1.9940e-01, -2.1544e+01],\n        [ 3.8000e-02, -1.0000e+02],\n        [ 7.2400e-02, -3.1623e+01],\n        [ 8.9340e-01, -2.1544e+01],\n        [ 3.5000e-02, -3.1623e+01],\n        [ 4.0200e-02, -1.0000e+02],\n        [ 2.1700e-01, -1.0000e+02],\n        [ 6.6000e-03, -2.1544e+01],\n        [ 4.4000e-03, -2.1544e+01],\n        [ 1.0000e-03, -4.6416e+01],\n        [ 1.2000e-03, -4.6416e+01],\n        [ 6.0000e-04, -6.8129e+01],\n        [ 8.4000e-03, -1.0000e+02],\n        [ 3.8020e-01, -6.8129e+01],\n        [ 7.4660e-01, -2.1544e+01],\n        [ 4.0000e-03, -1.0000e+02],\n        [ 5.1400e-01, -6.8129e+01],\n        [ 6.0000e-04, -2.1544e+01],\n        [ 2.8000e-03, -4.6416e+01],\n        [ 7.6800e-02, -4.6416e+01],\n        [ 2.2000e-03, -3.1623e+01],\n        [ 6.0000e-04, -3.1623e+01],\n        [ 1.5980e-01, -4.6416e+01],\n        [ 2.8000e-03, -6.8129e+01],\n        [ 1.8000e-03, -3.1623e+01],\n        [ 6.2000e-03, -6.8129e+01],\n        [ 2.0400e-02, -4.6416e+01],\n        [ 4.4000e-03, -1.0000e+02],\n        [ 6.0000e-04, -1.0000e+02],\n        [ 6.2000e-03, -1.0000e+02],\n        [ 6.5200e-02, -4.6416e+01],\n        [ 9.1000e-02, -2.1544e+01],\n        [ 8.0000e-04, -6.8129e+01],\n        [ 6.8000e-03, -1.0000e+02],\n        [ 5.6000e-03, -6.8129e+01],\n        [ 1.6220e-01, -4.6416e+01],\n        [ 9.1300e-01, -3.1623e+01],\n        [ 5.7400e-02, -4.6416e+01],\n        [ 6.0000e-04, -4.6416e+01],\n        [ 1.0000e-03, -1.0000e+02],\n        [ 5.2000e-03, -6.8129e+01],\n        [ 2.7640e-01, -4.6416e+01],\n        [ 7.5200e-02, -1.0000e+02],\n        [ 1.4540e-01, -4.6416e+01],\n        [ 1.2000e-03, -2.1544e+01],\n        [ 2.2860e-01, -3.1623e+01],\n        [ 1.0000e-03, -1.0000e+02],\n        [ 8.0000e-04, -6.8129e+01],\n        [ 1.2000e-03, -1.0000e+02],\n        [ 7.5640e-01, -6.8129e+01],\n        [ 6.0000e-04, -6.8129e+01],\n        [ 1.8200e-02, -3.1623e+01],\n        [ 3.4000e-03, -4.6416e+01],\n        [ 8.5200e-02, -3.1623e+01],\n        [ 3.8500e-01, -6.8129e+01],\n        [ 2.0000e-02, -1.0000e+02],\n        [ 5.8800e-02, -2.1544e+01],\n        [ 4.0000e-04, -4.6416e+01],\n        [ 9.2200e-02, -4.6416e+01],\n        [ 6.8820e-01, -3.1623e+01]], dtype=torch.float64)\ntensor([1316.,  304., 2386.,  983.,  390., 1841., 1004., 1425.,  561., 2535.,\n        3736., 2672.,  149.,  706.,  852., 1902.,  408., 1261., 1594., 1148.,\n        4861.,  947.,  509.,  157., 1509., 3646.,  656., 1506.,  948.,  562.,\n         630., 1127., 1054., 1382.,  182.,  836., 1094., 1305., 2906.,  127.,\n         321.,  727.,  961., 1025., 1278., 2228., 1125., 1476.,  557., 1998.,\n        5116.,  286., 1155.,  539.,  376.,  462.,  524.,  829., 1645.,  298.,\n         978.,  201.,  385.,  347., 2852., 1222., 1906., 2241., 3262.,  523.,\n        1884., 4837., 1748.,  522.,  751., 1600., 1457.,  308.,  364.,  184.,\n         372., 1200., 4401.,  289., 1209.,  692.,  613., 1263.,  770.,  242.,\n        1511.,  384.,  719.,  506.,  943.,  323.,   95.,  345., 1303., 2814.,\n         214.,  332.,  482., 1472., 3390., 1205.,  207.,  146.,  487., 1683.,\n         589., 1530., 1045., 2263.,  159.,  196.,  193., 1413.,  154., 1295.,\n         561., 1913., 1207.,  482., 2496.,  119., 1344., 2953.],\n       dtype=torch.float64)\ntensor([[3.7232],\n        [3.7232],\n        [3.7231],\n        [3.7242],\n        [3.7244],\n        [3.7232],\n        [3.7239],\n        [3.7241],\n        [3.7243],\n        [3.7219],\n        [3.7218],\n        [3.7219],\n        [3.7244],\n        [3.7239],\n        [3.7239],\n        [3.7219],\n        [3.7244],\n        [3.7242],\n        [3.7241],\n        [3.7242],\n        [3.7214],\n        [3.7232],\n        [3.7244],\n        [3.7244],\n        [3.7239],\n        [3.7218],\n        [3.7239],\n        [3.7232],\n        [3.7244],\n        [3.7239],\n        [3.7232],\n        [3.7242],\n        [3.7232],\n        [3.7219],\n        [3.7244],\n        [3.7232],\n        [3.7232],\n        [3.7239],\n        [3.7219],\n        [3.7244],\n        [3.7243],\n        [3.7219],\n        [3.7239],\n        [3.7219],\n        [3.7239],\n        [3.7237],\n        [3.7242],\n        [3.7241],\n        [3.7243],\n        [3.7232],\n        [3.7213],\n        [3.7239],\n        [3.7232],\n        [3.7219],\n        [3.7244],\n        [3.7239],\n        [3.7239],\n        [3.7239],\n        [3.7232],\n        [3.7244],\n        [3.7239],\n        [3.7239],\n        [3.7239],\n        [3.7244],\n        [3.7219],\n        [3.7232],\n        [3.7219],\n        [3.7231],\n        [3.7219],\n        [3.7244],\n        [3.7232],\n        [3.7214],\n        [3.7232],\n        [3.7244],\n        [3.7244],\n        [3.7219],\n        [3.7219],\n        [3.7239],\n        [3.7239],\n        [3.7243],\n        [3.7244],\n        [3.7242],\n        [3.7215],\n        [3.7244],\n        [3.7242],\n        [3.7219],\n        [3.7239],\n        [3.7239],\n        [3.7232],\n        [3.7232],\n        [3.7239],\n        [3.7243],\n        [3.7232],\n        [3.7243],\n        [3.7239],\n        [3.7244],\n        [3.7244],\n        [3.7244],\n        [3.7239],\n        [3.7219],\n        [3.7243],\n        [3.7244],\n        [3.7243],\n        [3.7239],\n        [3.7228],\n        [3.7239],\n        [3.7239],\n        [3.7244],\n        [3.7243],\n        [3.7238],\n        [3.7244],\n        [3.7239],\n        [3.7219],\n        [3.7231],\n        [3.7244],\n        [3.7243],\n        [3.7244],\n        [3.7242],\n        [3.7243],\n        [3.7232],\n        [3.7239],\n        [3.7232],\n        [3.7242],\n        [3.7244],\n        [3.7219],\n        [3.7239],\n        [3.7239],\n        [3.7229]], dtype=torch.float64, grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "for i, (xg, a) in enumerate(tr_dl):\n",
    "    print(xg)\n",
    "    print(a)\n",
    "\n",
    "    print(bnet(xg))\n",
    "\n",
    "    if i==1:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 861,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train(dl, model, loss_fn, optim):\n",
    "    size = len(dl.dataset)\n",
    "    for batch, (xg, a) in enumerate(dl):\n",
    "        # Compute prediction error\n",
    "        pred = model(xg)\n",
    "        loss = loss_fn(pred, a)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # if batch % 10 == 0:\n",
    "        #     loss, current = loss.item(), batch * len(xg)\n",
    "        #     print(f\"loss: {loss:>.2f}  [{current:>2d}/{size:>2d}]\")\n",
    "\n",
    "        return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 862,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dl, model, loss_fn):\n",
    "    size = len(dl.dataset)\n",
    "    num_batches = len(dl)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for xg, a in dl:\n",
    "            pred = model(xg)\n",
    "            test_loss += loss_fn(pred, a).item()\n",
    "    test_loss /= num_batches\n",
    "    return test_loss\n",
    "    #print(f\"Test Error: Avg loss: {test_loss:>.2f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 872,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1 - \n",
      "\tTrain loss: 2302621.37\n",
      "\tTest loss: 2490284.89\n",
      "Epoch 101 - \n",
      "\tTrain loss: 2298347.18\n",
      "\tTest loss: 2486747.29\n",
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/anaconda3/envs/cvae/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3437, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-872-95088cac1c2a>\", line 4, in <module>\n",
      "    teloss = test(te_dl, bnet, mse_loss)\n",
      "  File \"<ipython-input-862-9119e7659a5f>\", line 7, in test\n",
      "    for xg, a in dl:\n",
      "  File \"/usr/local/anaconda3/envs/cvae/lib/python3.9/site-packages/torch/utils/data/dataloader.py\", line 435, in __next__\n",
      "    data = self._next_data()\n",
      "  File \"/usr/local/anaconda3/envs/cvae/lib/python3.9/site-packages/torch/utils/data/dataloader.py\", line 1068, in _next_data\n",
      "    idx, data = self._get_data()\n",
      "  File \"/usr/local/anaconda3/envs/cvae/lib/python3.9/site-packages/torch/utils/data/dataloader.py\", line 1034, in _get_data\n",
      "    success, data = self._try_get_data()\n",
      "  File \"/usr/local/anaconda3/envs/cvae/lib/python3.9/site-packages/torch/utils/data/dataloader.py\", line 872, in _try_get_data\n",
      "    data = self._data_queue.get(timeout=timeout)\n",
      "  File \"/usr/local/anaconda3/envs/cvae/lib/python3.9/multiprocessing/queues.py\", line 113, in get\n",
      "    if not self._poll(timeout):\n",
      "  File \"/usr/local/anaconda3/envs/cvae/lib/python3.9/multiprocessing/connection.py\", line 262, in poll\n",
      "    return self._poll(timeout)\n",
      "  File \"/usr/local/anaconda3/envs/cvae/lib/python3.9/multiprocessing/connection.py\", line 429, in _poll\n",
      "    r = wait([self], timeout)\n",
      "  File \"/usr/local/anaconda3/envs/cvae/lib/python3.9/multiprocessing/connection.py\", line 936, in wait\n",
      "    ready = selector.select(timeout)\n",
      "  File \"/usr/local/anaconda3/envs/cvae/lib/python3.9/selectors.py\", line 416, in select\n",
      "    fd_event_list = self._selector.poll(timeout)\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/anaconda3/envs/cvae/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 2061, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/anaconda3/envs/cvae/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 1101, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/usr/local/anaconda3/envs/cvae/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 248, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/usr/local/anaconda3/envs/cvae/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 281, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/usr/local/anaconda3/envs/cvae/lib/python3.9/inspect.py\", line 1541, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"/usr/local/anaconda3/envs/cvae/lib/python3.9/inspect.py\", line 1499, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"/usr/local/anaconda3/envs/cvae/lib/python3.9/inspect.py\", line 709, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"/usr/local/anaconda3/envs/cvae/lib/python3.9/inspect.py\", line 746, in getmodule\n",
      "    if ismodule(module) and hasattr(module, '__file__'):\n",
      "  File \"/usr/local/anaconda3/envs/cvae/lib/python3.9/site-packages/torch/utils/data/_utils/signal_handling.py\", line 66, in handler\n",
      "    _error_if_any_worker_fails()\n",
      "RuntimeError: DataLoader worker (pid 57366) is killed by signal: Unknown signal: 0. \n"
     ]
    },
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "object of type 'NoneType' has no len()",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "\u001b[0;32m<ipython-input-872-95088cac1c2a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mtrloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtr_dl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmse_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mteloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mte_dl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmse_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;36m100\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-862-9119e7659a5f>\u001b[0m in \u001b[0;36mtest\u001b[0;34m(dl, model, loss_fn)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mxg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdl\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m             \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/envs/cvae/lib/python3.9/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    434\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 435\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    436\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/envs/cvae/lib/python3.9/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1067\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1068\u001b[0;31m             \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1069\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/envs/cvae/lib/python3.9/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1033\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1034\u001b[0;31m                 \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1035\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/envs/cvae/lib/python3.9/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 872\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    873\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/envs/cvae/lib/python3.9/multiprocessing/queues.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    112\u001b[0m                     \u001b[0mtimeout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeadline\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmonotonic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mEmpty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/envs/cvae/lib/python3.9/multiprocessing/connection.py\u001b[0m in \u001b[0;36mpoll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_readable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/envs/cvae/lib/python3.9/multiprocessing/connection.py\u001b[0m in \u001b[0;36m_poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    428\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 429\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    430\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/envs/cvae/lib/python3.9/multiprocessing/connection.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(object_list, timeout)\u001b[0m\n\u001b[1;32m    935\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 936\u001b[0;31m                 \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    937\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/envs/cvae/lib/python3.9/selectors.py\u001b[0m in \u001b[0;36mselect\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    415\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 416\u001b[0;31m             \u001b[0mfd_event_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_selector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    417\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mInterruptedError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/anaconda3/envs/cvae/lib/python3.9/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[1;32m   2060\u001b[0m                         \u001b[0;31m# in the engines. This should return a list of strings.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2061\u001b[0;31m                         \u001b[0mstb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_render_traceback_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2062\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'KeyboardInterrupt' object has no attribute '_render_traceback_'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/envs/cvae/lib/python3.9/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[1;32m   2061\u001b[0m                         \u001b[0mstb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_render_traceback_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2062\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2063\u001b[0;31m                         stb = self.InteractiveTB.structured_traceback(etype,\n\u001b[0m\u001b[1;32m   2064\u001b[0m                                             value, tb, tb_offset=tb_offset)\n\u001b[1;32m   2065\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/envs/cvae/lib/python3.9/site-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1365\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1366\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1367\u001b[0;31m         return FormattedTB.structured_traceback(\n\u001b[0m\u001b[1;32m   1368\u001b[0m             self, etype, value, tb, tb_offset, number_of_lines_of_context)\n\u001b[1;32m   1369\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/envs/cvae/lib/python3.9/site-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1265\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose_modes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1266\u001b[0m             \u001b[0;31m# Verbose modes need a full traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1267\u001b[0;31m             return VerboseTB.structured_traceback(\n\u001b[0m\u001b[1;32m   1268\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb_offset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumber_of_lines_of_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1269\u001b[0m             )\n",
      "\u001b[0;32m/usr/local/anaconda3/envs/cvae/lib/python3.9/site-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, evalue, etb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1122\u001b[0m         \u001b[0;34m\"\"\"Return a nice text document describing the traceback.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1124\u001b[0;31m         formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n\u001b[0m\u001b[1;32m   1125\u001b[0m                                                                tb_offset)\n\u001b[1;32m   1126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/envs/cvae/lib/python3.9/site-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mformat_exception_as_a_whole\u001b[0;34m(self, etype, evalue, etb, number_of_lines_of_context, tb_offset)\u001b[0m\n\u001b[1;32m   1080\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1081\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1082\u001b[0;31m         \u001b[0mlast_unique\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecursion_repeat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_recursion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morig_etype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1083\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1084\u001b[0m         \u001b[0mframes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat_records\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlast_unique\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecursion_repeat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/envs/cvae/lib/python3.9/site-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mfind_recursion\u001b[0;34m(etype, value, records)\u001b[0m\n\u001b[1;32m    380\u001b[0m     \u001b[0;31m# first frame (from in to out) that looks different.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_recursion_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 382\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    383\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m     \u001b[0;31m# Select filename, lineno, func_name to track frames with\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'NoneType' has no len()"
     ]
    }
   ],
   "source": [
    "epochs = 500\n",
    "for t in range(epochs):\n",
    "    trloss = train(tr_dl, bnet, mse_loss, optimizer)\n",
    "    teloss = test(te_dl, bnet, mse_loss)\n",
    "    if t%100 == 0:\n",
    "        print(f\"Epoch {t+1} - \")\n",
    "        print(f\"\\tTrain loss: {trloss:.2f}\")\n",
    "        print(f\"\\tTest loss: {teloss:.2f}\")\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 806,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[5.1502],\n",
       "        [5.1502],\n",
       "        [5.1502],\n",
       "        ...,\n",
       "        [5.1500],\n",
       "        [5.1500],\n",
       "        [5.1500]], dtype=torch.float64, grad_fn=<AddmmBackward>)"
      ]
     },
     "metadata": {},
     "execution_count": 806
    }
   ],
   "source": [
    "preds = bnet(torch.tensor(np.vstack((np.hstack((Xltrain[0:5])), np.repeat(gamma[0:5],len(Xltrain[0])))).T))\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_score(y_true=altrain[5], y_pred=pred.detach().numpy())\n",
    "#bnet(torch.unsqueeze(torch.tensor(Xltrain[5][0:1]),1), torch.unsqueeze(torch.tensor(np.repeat(gamma[5],1)),1))\n",
    "#torch.unsqueeze(torch.cat((torch.tensor(Xltrain[5]),torch.tensor(Xltrain[0])),0),1)\n",
    "torch.unsqueeze(torch.tensor(np.repeat(gamma[0:9],len(Xltrain[0]))),1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.unsqueeze(torch.tensor(Xltrain[0]),1)\n",
    "#bnet.forward(torch.tensor(Xltrain[0]), torch.tensor(np.repeat(gamma[0], len(Xltrain[0]))))\n",
    "torch.tensor(Xltrain[0]).view(-1, torch.tensor(Xltrain[0]).size(0)).shape\n",
    "torch.unsqueeze(torch.tensor(Xltrain[0]),1)\n",
    "#print(torch.unsqueeze(torch.linspace(-1,1,10),-1).shape)\n",
    "#bnet.forward(torch.tensor(Xltrain[0][1]),torch.tensor(gamma[0]))\n",
    "#torch.unsqueeze(torch.tensor(Xltrain[0]),0).shape"
   ]
  },
  {
   "source": [
    "## VAE implementation\n",
    "\n",
    "Following the procedure in https://github.com/AntixK/PyTorch-VAE/blob/master/models/vanilla_vae.py, seems pretty similar to the architecture of the BaselineNet..."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseVAE(nn.Module):\n",
    "    \n",
    "    def __init__(self) -> None:\n",
    "        super(BaseVAE, self).__init__()\n",
    "\n",
    "    def encode(self, input: torch.Tensor) -> List[torch.Tensor]:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def decode(self, input: torch.Tensor) -> Any:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def sample(self, batch_size:int, current_device: int, **kwargs) -> torch.Tensor:\n",
    "        raise RuntimeWarning()\n",
    "\n",
    "    def generate(self, x: torch.Tensor, **kwargs) -> torch.Tensor:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @abstractmethod\n",
    "    def forward(self, *inputs: torch.Tensor) -> torch.Tensor:\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def loss_function(self, *inputs: Any, **kwargs) -> torch.Tensor:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 761,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from https://debuggercafe.com/getting-started-with-variational-autoencoder-using-pytorch/\n",
    "# define a simple linear VAE\n",
    "class LinearVAE(nn.Module):\n",
    "    def __init__(self, features):\n",
    "        super(LinearVAE, self).__init__()\n",
    " \n",
    "        # encoder\n",
    "        self.enc1 = nn.Linear(2, 48)\n",
    "        self.enc2 = nn.Linear(48, features*2)\n",
    " \n",
    "        # decoder \n",
    "        self.dec1 = nn.Linear(features, 24)\n",
    "        self.dec2 = nn.Linear(24, 1)\n",
    "\n",
    "        self.double()\n",
    "\n",
    "    def reparameterize(self, mu, log_var):\n",
    "        \"\"\"\n",
    "        :param mu: mean from the encoder's latent space\n",
    "        :param log_var: log variance from the encoder's latent space\n",
    "        \"\"\"\n",
    "        std = torch.exp(0.5*log_var) # standard deviation\n",
    "        eps = torch.randn_like(std) # `randn_like` as we need the same size\n",
    "        sample = mu + (eps * std) # sampling as if coming from the input space\n",
    "        return sample\n",
    " \n",
    "    def forward(self, x):\n",
    "        # encoding\n",
    "        x = F.tanh(self.enc1(x))\n",
    "        x = self.enc2(x).view(-1, 2, features)\n",
    "\n",
    "        # get `mu` and `log_var`\n",
    "        mu = x[:, 0, :] # the first feature values as mean\n",
    "        log_var = x[:, 1, :] # the other feature values as variance\n",
    "\n",
    "        # get the latent vector through reparameterization\n",
    "        z = self.reparameterize(mu, log_var)\n",
    " \n",
    "        # decoding\n",
    "        x = F.tanh(self.dec1(z))\n",
    "        reconstruction = torch.sigmoid(self.dec2(x))\n",
    "        return reconstruction, mu, log_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 762,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 512\n",
    "model = LinearVAE(features=16) # dimensionality of the latent variable\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_loss(mse_loss, mu, logvar):\n",
    "    \"\"\"\n",
    "    This function will add the reconstruction loss (MSELoss) and the \n",
    "    KL-Divergence.\n",
    "    KL-Divergence = 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
    "\n",
    "    :param mse_loss: recontruction loss\n",
    "    :param mu: the mean from the latent vector\n",
    "    :param logvar: log variance from the latent vector\n",
    "    \"\"\"\n",
    "    MSE = mse_loss \n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "\n",
    "    return MSE + KLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_vae(dl, model, loss_fn, optim):\n",
    "    size = len(dl.dataset)\n",
    "    for batch, (xg, a) in enumerate(dl):\n",
    "        # Compute prediction error\n",
    "        pred, mu, logvar = model(xg)\n",
    "        step_loss = loss_fn(pred, a)\n",
    "\n",
    "        loss = final_loss(step_loss, mu, logvar)\n",
    "        \n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # if batch % 10 == 0:\n",
    "        #     loss, current = loss.item(), batch * len(xg)\n",
    "        #     print(f\"loss: {loss:>.2f}  [{current:>2d}/{size:>2d}]\")\n",
    "\n",
    "        return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_vae(dl, model, loss_fn):\n",
    "    size = len(dl.dataset)\n",
    "    num_batches = len(dl)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for xg, a in dl:\n",
    "            pred, _, _ = model(xg)\n",
    "            test_loss += loss_fn(pred, a).item()\n",
    "    test_loss /= num_batches\n",
    "    return test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 500\n",
    "for t in range(epochs):\n",
    "    trloss = train(tr_dl, bnet, mse_loss, optimizer)\n",
    "    teloss = test(te_dl, bnet, mse_loss)\n",
    "    if t%100 == 0:\n",
    "        print(f\"Epoch {t+1} - \")\n",
    "        print(f\"\\tTrain loss: {trloss:.2f}\")\n",
    "        print(f\"\\tTest loss: {teloss:.2f}\")\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 763,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "epoch [0], Loss: 295960.03\n",
      "epoch [100], Loss: nan\n",
      "epoch [200], Loss: nan\n",
      "epoch [300], Loss: nan\n",
      "epoch [400], Loss: nan\n",
      "epoch [500], Loss: nan\n",
      "epoch [600], Loss: nan\n",
      "epoch [700], Loss: nan\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "for e in range(800):\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    pred, mu, logvar = model(torch.tensor(np.vstack((Xltrain[0],np.repeat(gamma[0],len(Xltrain[0])))).T))\n",
    "    step_loss = mse_loss(pred, torch.unsqueeze(torch.tensor(np.hstack((altrain[0]))),1))\n",
    "\n",
    "    loss = final_loss(step_loss, mu, logvar)\n",
    "    loss.backward()\n",
    "\n",
    "    # update with current step regression parameters \n",
    "    optimizer.step()\n",
    "\n",
    "    if e % 100 == 0:\n",
    "        print ('epoch [{}], Loss: {:.2f}'.format(e, loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 747,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([800, 2])"
      ]
     },
     "metadata": {},
     "execution_count": 747
    }
   ],
   "source": [
    "torch.tensor(np.vstack((Xltrain[0],np.repeat(gamma[0],len(Xltrain[0])))).T).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 756,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan]], dtype=torch.float64, grad_fn=<SigmoidBackward>)"
      ]
     },
     "metadata": {},
     "execution_count": 756
    }
   ],
   "source": []
  }
 ]
}